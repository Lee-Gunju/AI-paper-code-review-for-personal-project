{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "End-to-end Masked Language Modeling with BERT.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPott7dg0KSvNj1yDBh7alb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Lee-Gunju/AI-paper-code-review-for-personal-project/blob/master/End_to_end_Masked_Language_Modeling_with_BERT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lD2eOhkt7Sgb"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
        "from dataclasses import dataclass\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import glob\n",
        "import re\n",
        "from pprint import pprint"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ypWlmG368Rzu"
      },
      "source": [
        "@dataclass\n",
        "class Config:\n",
        "    MAX_LEN = 256\n",
        "    BATCH_SIZE = 32\n",
        "    LR = 0.001\n",
        "    VOCAB_SIZE = 30000\n",
        "    EMBED_DIM = 128\n",
        "    NUM_HEAD = 8  # used in bert model\n",
        "    FF_DIM = 128  # used in bert model\n",
        "    NUM_LAYERS = 1\n",
        "\n",
        "\n",
        "config = Config()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hcsg2vFi8mVV",
        "outputId": "1a63cfeb-9c46-4aa5-e712-7da5035eb67b"
      },
      "source": [
        "!curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "!tar -xf aclImdb_v1.tar.gz"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 80.2M  100 80.2M    0     0  26.4M      0  0:00:03  0:00:03 --:--:-- 26.4M\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oURwa2E-8oI3"
      },
      "source": [
        "def get_text_list_from_files(files):\n",
        "  text_list = []\n",
        "  for name in files:\n",
        "    with open(name) as f:\n",
        "      for line in f:\n",
        "        text_list.append(line)\n",
        "  return text_list \n",
        "\n",
        "\n",
        "def get_data_from_text_files(folder_name):\n",
        "\n",
        "  pos_files = glob.glob('aclImdb/' + folder_name + '/pos/*.txt')\n",
        "  pos_texts = get_text_list_from_files(pos_files)\n",
        "  neg_files = glob.glob('aclImdb/' + folder_name + '/neg/*.txt')\n",
        "  neg_texts = get_text_list_from_files(neg_files)\n",
        "  df = pd.DataFrame({\"review\" : pos_texts + neg_texts, 'sentiment': [0] * len(pos_texts) + [1] * len(neg_texts)})\n",
        "\n",
        "  df = df.sample(len(df)).reset_index(drop = True)\n",
        "  return df \n",
        "\n",
        "\n",
        "train_df = get_data_from_text_files('train')\n",
        "test_df = get_data_from_text_files('test')\n",
        "\n",
        "all_data = train_df.append(test_df)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wuwyjqzC_kvJ",
        "outputId": "8f14116f-ac40-453f-c545-1ac0e2ac4eb8"
      },
      "source": [
        "all_data[0:10].head(1)['review'].values"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['This an free adaptation of the novels of Clarence Mulford; fans of the Willaim Boyd films will probably feel a little at sea here (and the reviews here so far reflect that). But I knew of Hopalong from the novels first, and never cared much for the Boyd films once I got around to them.<br /><br />Christopher Coppola has made a wise choice - he has not made a nostalgic \"Western\"; instead, he has approached the Cassidy story as a slice of what we used to call \\'Americana\\'; or what older critics once called \\'homespun\\'. As the film unraveled, I found myself more and more reminded of the great \"Hallmark Theater\" version of Mark Twain\\'s \"Roughing It\", with James Garner narrating.<br /><br />Both these films remind us that, although films about the \\'old west\\' are probably always to be mythic for Americans, they need not be \\'westerns\\'; they can very well be just films about what it meant to be American in that time, in that place.<br /><br />I never feel pandered to, watching this film; there\\'s no effort to shove the Boyd-Cassidy legacy down our throats, no irony, no camp. Consequently, I get a sense of these characters as having walked - or ridden horseback - across some real western America I too could have walked a hundred years ago.<br /><br />Given that, the plainness of the film - it positively avoids anything we have come to call \"style\" - is all to its favor; and the plain acting of the performers fits neatly in with this; gosh, it really does feel like some story told around a campfire on a cattle drive - no visual dressing, just the quirks and good humor - and sudden violence - that we expect from the good narration of an adventure yarn. I was very pleasantly surprised by this film, and if the viewer sets aside encultured expectations, he or she will find considerable pleasure in it.<br /><br />I would have given this film 9-stars, but I\\'ll give it a ten just because most reviewers here have missed the point completely; and I urge them to set their memories of Boyd aside and give this film another chance.<br /><br />Note 1: A reviewer complained that Hopalong shoots people dead in this film, rather than shooting the guns out of their hands (ala Boyd\\'s Cassidy); first, Cassidy DOES shoot people dead in the novels; second, if Cassidy were a real cowboy he would have shot people dead - the problem with shooting guns out of people\\'s hands is that they can always get another gun - which happens to be part of the subtext of this very film.<br /><br />Note 2: I admit that I am jealous of the Coppola family, that they have the Director of \"The Godfather\" among them who can get them all opportunities to make movies that I can\\'t; but a good movie is a good movie; and this is a good movie. If it\\'s by somebody by the name \"Coppola\", well, that\\'s just is as it is. America is the land of opportunity (or was, until Bush got into office) - that\\'s what the great American novels are all about.'],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kH9cjhOv_lRV"
      },
      "source": [
        "def custom_standardization(input_data):\n",
        "    lowercase = tf.strings.lower(input_data)\n",
        "    stripped_html = tf.strings.regex_replace(lowercase, \"<br />\", \" \")\n",
        "    return tf.strings.regex_replace(\n",
        "        stripped_html, \"[%s]\" % re.escape(\"!#$%&'()*+,-./:;<=>?@\\^_`{|}~\"), \"\"\n",
        "    )\n",
        "\n",
        "\n",
        "def get_vectorize_layer(texts, vocab_size, max_seq, special_tokens=[\"[MASK]\"]):\n",
        "    \"\"\"Build Text vectorization layer\n",
        "\n",
        "    Args:\n",
        "      texts (list): List of string i.e input texts\n",
        "      vocab_size (int): vocab size\n",
        "      max_seq (int): Maximum sequence lenght.\n",
        "      special_tokens (list, optional): List of special tokens. Defaults to ['[MASK]'].\n",
        "\n",
        "    Returns:\n",
        "        layers.Layer: Return TextVectorization Keras Layer\n",
        "    \"\"\"\n",
        "    vectorize_layer = TextVectorization(\n",
        "        max_tokens=vocab_size,\n",
        "        output_mode=\"int\",\n",
        "        standardize=custom_standardization,\n",
        "        output_sequence_length=max_seq,\n",
        "    )\n",
        "    vectorize_layer.adapt(texts)\n",
        "\n",
        "    # Insert mask token in vocabulary\n",
        "    vocab = vectorize_layer.get_vocabulary()\n",
        "    vocab = vocab[2 : vocab_size - len(special_tokens)] + [\"[mask]\"]\n",
        "    vectorize_layer.set_vocabulary(vocab)\n",
        "    return vectorize_layer\n",
        "\n",
        "\n",
        "vectorize_layer = get_vectorize_layer(\n",
        "    all_data.review.values.tolist(),\n",
        "    config.VOCAB_SIZE,\n",
        "    config.MAX_LEN,\n",
        "    special_tokens=[\"[mask]\"],\n",
        ")\n",
        "\n",
        "# Get mask token id for masked language model\n",
        "mask_token_id = vectorize_layer([\"[mask]\"]).numpy()[0][0]\n",
        "\n",
        "\n",
        "def encode(texts):\n",
        "    encoded_texts = vectorize_layer(texts)\n",
        "    return encoded_texts.numpy()\n",
        "\n",
        "\n",
        "def get_masked_input_and_labels(encoded_texts):\n",
        "    # 15% BERT masking\n",
        "    inp_mask = np.random.rand(*encoded_texts.shape) < 0.15\n",
        "    # Do not mask special tokens\n",
        "    inp_mask[encoded_texts <= 2] = False\n",
        "    # Set targets to -1 by default, it means ignore\n",
        "    labels = -1 * np.ones(encoded_texts.shape, dtype=int)\n",
        "    # Set labels for masked tokens\n",
        "    labels[inp_mask] = encoded_texts[inp_mask]\n",
        "\n",
        "    # Prepare input\n",
        "    encoded_texts_masked = np.copy(encoded_texts)\n",
        "    # Set input to [MASK] which is the last token for the 90% of tokens\n",
        "    # This means leaving 10% unchanged\n",
        "    inp_mask_2mask = inp_mask & (np.random.rand(*encoded_texts.shape) < 0.90)\n",
        "    encoded_texts_masked[\n",
        "        inp_mask_2mask\n",
        "    ] = mask_token_id  # mask token is the last in the dict\n",
        "\n",
        "    # Set 10% to a random token\n",
        "    inp_mask_2random = inp_mask_2mask & (np.random.rand(*encoded_texts.shape) < 1 / 9)\n",
        "    encoded_texts_masked[inp_mask_2random] = np.random.randint(\n",
        "        3, mask_token_id, inp_mask_2random.sum()\n",
        "    )\n",
        "\n",
        "    # Prepare sample_weights to pass to .fit() method\n",
        "    sample_weights = np.ones(labels.shape)\n",
        "    sample_weights[labels == -1] = 0\n",
        "\n",
        "    # y_labels would be same as encoded_texts i.e input tokens\n",
        "    y_labels = np.copy(encoded_texts)\n",
        "\n",
        "    return encoded_texts_masked, y_labels, sample_weights\n",
        "\n",
        "\n",
        "# We have 25000 examples for training\n",
        "x_train = encode(train_df.review.values)  # encode reviews with vectorizer\n",
        "y_train = train_df.sentiment.values\n",
        "train_classifier_ds = (\n",
        "    tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
        "    .shuffle(1000)\n",
        "    .batch(config.BATCH_SIZE)\n",
        ")\n",
        "\n",
        "# We have 25000 examples for testing\n",
        "x_test = encode(test_df.review.values)\n",
        "y_test = test_df.sentiment.values\n",
        "test_classifier_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(\n",
        "    config.BATCH_SIZE\n",
        ")\n",
        "\n",
        "# Build dataset for end to end model input (will be used at the end)\n",
        "test_raw_classifier_ds = tf.data.Dataset.from_tensor_slices(\n",
        "    (test_df.review.values, y_test)\n",
        ").batch(config.BATCH_SIZE)\n",
        "\n",
        "# Prepare data for masked language model\n",
        "x_all_review = encode(all_data.review.values)\n",
        "x_masked_train, y_masked_labels, sample_weights = get_masked_input_and_labels(\n",
        "    x_all_review\n",
        ")\n",
        "\n",
        "mlm_ds = tf.data.Dataset.from_tensor_slices(\n",
        "    (x_masked_train, y_masked_labels, sample_weights)\n",
        ")\n",
        "mlm_ds = mlm_ds.shuffle(1000).batch(config.BATCH_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IjCBSIqwLLWo",
        "outputId": "dc656f23-2213-4e89-9330-c0a929d0c7c2"
      },
      "source": [
        "x_masked_train[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([   11, 29999,   958,  1269,     5,     2,  2704, 29999,  9263,\n",
              "           1,   445,     5,     2,     1, 14271, 29999, 29999, 29999,\n",
              "         233, 29999,   117,    30,  1729,   132,     3,     2,   806,\n",
              "         132, 29999, 29999,  4327,    12,    18,    10,   649,     5,\n",
              "       22209,    35,     2,  2704,    88, 29999,   109,  3665,    71,\n",
              "          16,     2, 14271,    94,   282,    10,   183,   181,     6,\n",
              "          90, 29999,  7220,    44,    93,     4,  2186,  1089,    27,\n",
              "          44,    21,    93,     4,  4568, 27658,   296,    27,    44,\n",
              "        6308,     2, 29999,    65,    14, 29999,  5471, 29999,    48,\n",
              "          73, 29999,     6,   651, 15183,    39,    48,   940,  1343,\n",
              "         282,   468, 24956, 29999,     2,    19, 29999,    10,   248,\n",
              "       29999,    51,     3,    51,  1502,     5,     2, 29999,     1,\n",
              "       29999,   310,     5,   950,     1,     1,  2303,    15,   587,\n",
              "        6783, 12741, 29999,   129,    94,  3064,   169,    12,   253,\n",
              "          94,    42,     2,   170,  1272, 29999,   235,   203, 29684,\n",
              "          26, 17536,    16,  1525,    34,   345,    21,    26,  2447,\n",
              "          34,    68,    52,    72,  2894,    40,    94,    42, 29999,\n",
              "           9,   977,     6, 29999,   313,     8,    12,    59,     8,\n",
              "          12,   273, 14902,   109,   233,     1,     6, 29999, 29999,\n",
              "          19,   218,    56,   756, 29999, 13569,     2,     1,  5604,\n",
              "         182,   251, 29999,    56,  3132,    56,  1228,  7162,    10,\n",
              "          75,     4,   278,     5, 29999,   100,    14,   254,  2185,\n",
              "          39,  8978, 11623, 29999,    46, 29999,   927, 29999,    10,\n",
              "          99,    97,    25, 29999,     4,  2997,   151,   588,   340,\n",
              "          12,     2,     1,     5,     2, 29999,     9,  5174, 29999,\n",
              "         231,    73, 29999,   212,     6,   651, 26409, 29999,    31,\n",
              "           6,    29,  1931,     3,     2,  1010, 29999,     5,     2,\n",
              "        3128,  2267,   305, 29999,    15,    11,  9240, 29999,    62,\n",
              "         121,   233, 29999,    46])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wVnksBAGLQ1n",
        "outputId": "9256365a-33e4-443d-faaa-eac8d4766223"
      },
      "source": [
        "y_masked_labels[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([   11,    33,   958,  1269,     5,     2,  2704,     5,  9263,\n",
              "           1,   445,     5,     2,     1, 14271,    94,    77,   235,\n",
              "         233,     4,   117,    30,  1729,   132,     3,     2,   806,\n",
              "         132,    37,   227,  4327,    12,    18,    10,   649,     5,\n",
              "       22209,    35,     2,  2704,    88,     3,   109,  3665,    71,\n",
              "          16,     2, 14271,    94,   282,    10,   183,   181,     6,\n",
              "          90,  1411,  7220,    44,    93,     4,  2186,  1089,    27,\n",
              "          44,    21,    93,     4,  4568, 27658,   296,    27,    44,\n",
              "        6308,     2,  7047,    65,    14,     4,  5471,     5,    48,\n",
              "          73,   321,     6,   651, 15183,    39,    48,   940,  1343,\n",
              "         282,   468, 24956,    14,     2,    19, 22679,    10,   248,\n",
              "         522,    51,     3,    51,  1502,     5,     2,    79,     1,\n",
              "       26582,   310,     5,   950,     1,     1,  2303,    15,   587,\n",
              "        6783, 12741,   190,   129,    94,  3064,   169,    12,   253,\n",
              "          94,    42,     2,   170,  1272,    23,   235,   203,     6,\n",
              "          26, 17536,    16,  1525,    34,   345,    21,    26,  2447,\n",
              "          34,    68,    52,    72,    26,    40,    94,    42,    48,\n",
              "           9,   977,     6,    26,   313,     8,    12,    59,     8,\n",
              "          12,   273,    10,   109,   233,     1,     6,   144,    11,\n",
              "          19,   218,    56,   756,     6, 13569,     2,     1,  5604,\n",
              "         182,   251,  9085,    56,  3132,    56,  1228,  7162,    10,\n",
              "          75,     4,   278,     5,   129,   100,    14,   254,  2185,\n",
              "          39,  8978, 11623,   590,    46,   147,   927,   947,    10,\n",
              "          99,    97,    25,  2185,     4,  2997,   151,   588,   340,\n",
              "          12,     2,     1,     5,     2,    19,     9,  5174,  8154,\n",
              "         231,    73,    25,   212,     6,   651, 26409,     7,    31,\n",
              "           6,    29,  1931,     3,     2,  1010,   110,     5,     2,\n",
              "        3128,  2267,  6467,     8,    15,    11,  9240,     9,    62,\n",
              "         121,   233,    38,    46])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Sbjp1MwSxyY",
        "outputId": "b05a7f65-f80e-4833-cddc-efb4ecbe4d9f"
      },
      "source": [
        "sample_weights[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
              "       1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0., 1., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 1., 0., 0.,\n",
              "       0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1.,\n",
              "       0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1.,\n",
              "       1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1.,\n",
              "       0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       1., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 1.,\n",
              "       0.])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4y5vFrsFS1zt",
        "outputId": "0030aabc-563b-4e45-8d8e-d6d23ddfe8d7"
      },
      "source": [
        "next(iter(test_raw_classifier_ds))[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(32,), dtype=string, numpy=\n",
              "array([b\"What is this!! its so bad. The animation looks so terrible , it looks like a ps1 type game. The actors are awful, they just cannot act to save their lives. I sat through all of this film an then at the end I was annoyed when I realised I had wasted 3 hours of my life. I've not heard of this film, did it ever actually come out in the cinema or did it go straight to DVD? A girl got shot?! What is up with that, it was just a stupid film. They totally copied 'The Day After Tomorrow'. Its got to be one of the worst films i have ever seen. I would definitely recommend to people to not waste their time with this. You could spend your time watching 'The Day After Tomorrow', its a lot better. Well thats what I think of the film. Actually why have I wasted my time writing about it, ah dam!! Its really annoying me, its wasted 3 hours and 10 minutes now.\",\n",
              "       b'To call this film a complete waste of celluloid would be an understatement.<br /><br />The acting was unconvincing to say the least, especially from actor Craig Fong, who couldn\\'t have acted stiffer. As far as story goes...well...what story?! The \"film\" is nominally about Harry Lee, a Malaysian of Chinese descent who comes back to his home country after flunking out of every course he took and tries to start a band.<br /><br />The film has ever cliche you can think of -- sex, tension among band members and a little bit of racial tension thrown in.<br /><br />The problem is that even with a subject that\\'s been covered adequately by even the most amateurish directors, this movie is all over the place and the whole thing just feels contrived with parts that would make even the most hardened reviewers\\' hairs stand on end.<br /><br />',\n",
              "       b'I was looking for ATTACK on Precinct 13. There, the film is THAT memorable. Who is the star of this? Ethan Hawke or Matt Dillon (I can\\'t tell who the lead actor is, that\\'s a pretty big point against the movie right there) Gabriel Byrne (who could\\'t have needed the money this badly could he?) Drea De Matteo is stunning but only because of her amazing body. It took me ages to finally work out she\\'s Joey\\'s sister off \"Friends\". I agree that the so called SWAT people attacking the station are pretty crap, as far as tactics go. We were even taught better basic skills than this in RAF basic training.<br /><br />Avoid this, even the snow doesn\\'t want to fall on a bus full of prisoners! Very bad continuity indeed.<br /><br />Avoid like the plague!',\n",
              "       b'There are several ways to misunderstand this movie and a couple of them have been shown in some of the past comments. This is a movie to be analyzed as a free recreation of a known subject and therefore not to be compared with the opera, the book or other Carmen movies seen before. It just stands for itself and I must say that this Carmen does it very well. It is a mistake to compare because that is the first step to deny movies the chance to be autonomous creative works of art. Vicente Aranda is a master of atmosphere and the art direction, the costumes and the photography are extremely well put together to achieve a pleasing aesthetic experience. Let\\'s take it as it is.<br /><br />And that brings us to the next misunderstanding. Someone complains about the typical Spanish clich\\xc3\\xa9s in the movie. Well, historically the movie is extremely well researched and you can see the results of that very serious work in every scene. It is not only an accurate portrait of the \"black Spain\" of knife and espadrille that Goya portrayed so vividly, but it\\'s also of that part of history as seen by a foreigner fascinated with the folkloric side of that society. Honestly, anyone who doesn\\'t want to see any clich\\xc3\\xa9 about Spain shouldn\\'t buy a ticket to see Carmen, but in this case those clich\\xc3\\xa9s are presented before they became one and the way to see them is getting rid of our own prejudices.<br /><br />Another important requirement to understand this movie properly is to speak the language. It is not acceptable to criticize any actor performance for not having understood his or her lines. If all the rest of the audience did, the problem most likely lies somewhere else. Paz Vega has an immaculate diction with her Andalusian accent and all she says is understandable and credible. Her Argentinian partner, Leonardo Sbaraglia, gives also a convincing portrait of the Basque officer that became a \"bandolero\", and her accent is very well learned.<br /><br />No less important is to have a minimally open approach to the material. To say that Paz Vega is \"horrible\" suggests that the author of the phrase entered the theater for the wrong reasons. We already had in Spain a critic in one of the most prestigious papers that used to recommend us pictures he found homosexually arousing, without mentioning it explicitly. And that was not totally fair for the rest of us, especially for the ones that hadn\\'t detected that the man was writing with parts of his anatomy that many readers didn\\'t necessarily had to care for. I\\'m not suggesting at all that the reviewer had the same motivation, but the expectations must have been different as the ones of those among us that went to see a talented and beautiful actress play an almost classic role, because that\\'s what we got. Paz Vega IS Carmen, and an excellent one, in Vicente Aranda\\'s movie.',\n",
              "       b'This young filmmaker has a talent for capturing his audience quickly with unusual camera work and sparse but intense scripts. The concept here of combining animation with live footage is remarkably well-executed and the soundtrack is very good.<br /><br />The decision to release the movie in twelve parts online puts the onus on the director to make each episode fascinating enough for the viewer to invest in buying each upcoming episode. I only wish all motion pictures had this kind of commitment to keeping their audiences entertained throughout their stories.<br /><br />Highly recommended.',\n",
              "       b\"Yeah, I know his character was supposed to be a drunk, and he may have been just acting goofy. But something tells this critic that Mr. Pleasence really was drinking a lot and was intoxicated during his scenes in the film. Basically everything he says is slurred and often unintelligible. Or maybe it was just the poor productions values... hard to say.<br /><br />Anyway, The Race for the Yankee Zephyr is a film that just doesn't work. That's a shame, too, since the film has a terrific opening and a generally interesting plot. Ultimitely the production values are just too low and the action just too sparse for this New Zealand adventure to deliver the goods. The story deals with a US war plane which is filled with gold, money, and medals, which crashes into a lake in New Zealand during WWII. The plane remains lost for about forty years or so until it somehow washes ashore and a drunk (Pleasence) literally stumbles onto it. At first he gathers up all the purple heart medals and tries to sell them in town, actually getting $75 apiece for them! Little does he know that once he sells them, the local jeweler gets on the phone and starts trying to track down info about the plane. Before you can blink, all of the attention brings a wealthy scumbag (Peppard) and his henchmen into town and they quickly try to force the old guy to give up the location of the plane since they know there is much more on it than just medals. The old drunk's business partner (Wahl) and his daughter (Warren) then race out to try and claim the fortune before the bad guys can get to it. The resulting action just isn't as fun as you'd hope it would be.<br /><br />The acting is rather awful, save for Pleasence. George Peppard tries to do some kind of (I guess) Austrailian accent, but it is hardly convincing. Lesley Ann Warren isn't too bad, but Ken Wahl is really bad. He's basically doing his best impression of Michael Pare on his worst day. And that's saying something. Hopefully he made enough money on this film to fix his front teeth which looked a bit crooked. I don't recall if he'd had them straightened by the time he was in Wiseguy. The rest of the cast are pretty untalented. Probably mostly locals who never did much else. I guess the biggest problems for me were the lack of action for much of the film, and the lack of danger. The villains are just too nice and goofy to be taken seriously. And honestly, there are NO helicopters in the film that look like the ones on the DVD cover. And none of the boats in the film have teeth painted on them, either.<br /><br />The film does have its strengths, though. The beginning which starts off as a newsreel and then becomes part of the story was a nice touch. Brian May's score sounds a little too much like the one in Mad Max 2, but he included a nice little march they play for Pleasence in some scenes. Sounds just like the one in the Great Escape! There are some neat helicopter stunts and a great boat chase that apparently killed three stunt men during filming. The scenery, despite the grainy look of the picture, is still quite beautiful. The thing you'll remember most is the drunken antics of Donald Pleasence, though. He was almost enough to save this film. Almost. 4 of 10 stars.<br /><br />The Hound.\",\n",
              "       b\"I was wondering what possessed the organizers of the Victoria Film Festival to include this film in their program. I guess they must have agreed with the others who have reviewed this film. I, on the other hand, consider it the worst film I have ever seen. It starts with a bad script, full of holes, and dialog so unlikely it's embarrassing. Ideas are introduced, then dropped with no development. The acting left me totally cold and uninvolved. The set decoration was appropriate for the time, but a decorator's nightmare. The only way the characters could love this house is if their previous homes had been ghastly. The attic looked as if the items had been thrown in for the scene with no attempt to create the look of a real attic that has been filling with junk over the years. The photography was leaden and lacking in variety. Save your money for something worthwhile.\",\n",
              "       b\"I remember watching this film as a kid and I was in complete awe of it, I couldn't take my eyes of the television. This movie has it all for horror fans! This movie had no funny moments expect a couple of one liners by stooge(who was my favorite character in the film) kevin tenney directed this jewel and did a wonderful job with a low budget, I thought the end was awesome the only thing that could stop them was by surviving the night they were unstoppable killing machines! the effects done by steve johnson we're excellent I would recommend this movie to anyone who has a love for low budget horror movies because as the old saying goes they don't make them like this anymore. The sequels were pretty good too, but not as good as good as the original. This is a must have in any horror collection, buy it if you can find it you won't be disappointed\",\n",
              "       b'The impossibly sexy Rosie Holotik plays Charlotte Beale, a new nurse at one of those movie-type asylums where the doctor in charge has his own, unorthodox ways of treating the inmates. Reluctantly bringing her on board is the officious Dr. Masters (Anne MacAdams, a.k.a. Annabelle Weenick). The inmates don\\'t take too kindly to Charlotte, either.<br /><br />Part mystery, part horror, this initial effort from B-movie director S.F. Brownrigg has an oppressive feel to it. These colorful psychos, including a nymphomaniac with no self-esteem, a gentle giant who\\'s already had a lobotomy, and a supposed \"judge\" who speaks in legalese jargon, dominate the screen with their unnerving presence.<br /><br />It\\'s a picture that works fairly well, establishing the gritty, grim atmosphere right from the get go. Some brief bursts of graphic violence also help to give it that good old fashioned exploitative quality.<br /><br />It builds in intensity as the imperiled heroine struggles to maintain her own sanity, offering one surreal encounter after another. Particularly effective is the poetry-quoting, dotty old Mrs. Callingham.<br /><br />For cheap 1970\\'s era cheese, it\\'s pretty effective, with a particularly nasty climax. I had a pretty good time watching it.<br /><br />7/10',\n",
              "       b'In Moscow, the priest Owen (Vincent Gallo) hires a team to guide him in the underworld to find his friend Sergei (Rade Serbedzija) that is missing while researching the legend about the existence of demons and an entrance to hell beneath the city. <br /><br />I bought this DVD based on the name of Val Kilmer and the interesting pictures on the cover. I am totally disappointed since this film is one of the worst movies I have ever seen. I do not understand how Val Kilmer accepted to participate in this production. There are two shameful reviews in IMDb promoting this movie and they are typically fake, written by users with only one review in this site. There are two possible ways to see this boring and awful film: my wife and I napped many times because of the monotony of this pointless story, and we used the rewind button of the DVD to repeat each lost scene. However, the correct way should have been the use of the fast forward or the stop button, to end this crap faster. My vote is one.<br /><br />Title (Brazil): \"Cidade Sombria\" (\"Dark City\")',\n",
              "       b\"this movie wasn't good. i thought it'd be a cute disney movie just like the original. wrong. it was awkward for christina ricci, whom i expect so much more than this, you could just tell by watching. i think doug e. doug did the best he could. sit 5 year olds in front of the this, any older, and they might start to fall asleep.\",\n",
              "       b\"1 thing. this movie sucks BIG TIME..i was into singaporean comedy when Chiken Rice war came along. But, this time, even Gurmit Singh (well-done) acting cant pull this one of. A total failure of following HK's Shaolin Soccer. Next time: do ur own thing!\",\n",
              "       b'\"Cherry\" tells of a naive, unmarried virgin who decides to have a baby but isn\\'t quite sure how to go about it. This easy going little sleeper is full of quirky characters and tongue-in-cheek situational humor. Fresh, fun, mold breaking stuff, I happened to really enjoy this flick...for whatever that\\'s worth. Recommended for lovers of romantic comedy who want something different.',\n",
              "       b'Certainly this film has the ring of truth about it, as it purports to be based on actual occurrences at a Marine base. It deals with the attempted cover-up by the local Marine commander of unacceptable conduct by a Marine major which resulted in his being shot to death by his former girl friend, a Marine captain. The man and woman had been lovers, but the captain attempted to break off the relation when she discovered her boy friend was married. He continued to stalk her, going so far as to fire his side arm in her direction at one time. Finally he broke into her home, attacked her with a knife, and was shot twice with her service pistol and killed. The civilian prosecutor ruled the killing self defense, but the Marines decided to charge the captain with murder. The major, you see, was a decorated hero from Vietnam, and an old friend of the commanding colonel at the Marine base. The captain, too, had made some enemies in her motor pool command, rejecting some male advances in a very butch style.<br /><br />There is considerable psychological freight motivating and controlling the actions of the principal participants in this drama, which the very capable cast gets across nicely. The director and editor, however, seem determined to obscure the happenings as much as possible with frustrating flashbacks and shifting points of view. You\\'re lucky if you know where you\\'re at most of the time. Bear with them, though; it\\'s a worthwhile story as the captain\\'s court martial trial unfolds, and it seems every man\\'s hand is against her, even her attorney at times. <br /><br />The verdict? Well, after all, this is rather a suspense story, so you\\'ll have to see for yourself. There is a kind of \"pacifist\" message folded into the film, but forget about that. Sure, \"war is hell\", but sometimes it can\\'t be avoided. We\\'ll need those Marines then, even if they aren\\'t always the best champions of fair play internally. As Kipling says in his poem \"Tommy Atkins\":<br /><br />\"It\\'s Tommy this and Tommy that, And Tommy wait outside. But, it\\'s room for Mr. Atkins, When the troopship\\'s on the tide.\"',\n",
              "       b\"Hell to Pay was a disappointment. It did not have anywhere near the substance of a B Western movie, and should in no way be compared to a fantastic movie like Silverado. The dialog was dull, the plot was torpid, the soundtrack was overbearingly unnecessary, and the acting was awful. Even the professionals could've taken some lessons from the Sunset Carson School of Acting. The only positive thing about this movie is that it showcased some of the top Cowboy shooters in the nation, but you can see them in a better light in any SASS video. The packaging of this feature makes it very enticing, and the preview is decent, but it's all over after that.\",\n",
              "       b\"This would have to rate as one of the worst films of all time. The film screened at the Italian Film Festival in Melbourne, Australia. After the screening, not only did I want my money refunded, I wanted the 1.5 wasted hours of my life back too. I have a very broad tolerance level when it comes to the indulgences of some European film-making, but this is one of those films that is selected for festivals based on the reputation of the filmmaker alone. This film is proof that while such selections may satisfy the egos of the film-maker and the selection panel, there is absolutely no joy for the audience. There is no character development whatsoever, the plot is a garbled mess, the style is nonsensical, the shot selection is appalling, and the editing is worse. By the end of the first reel, you'll wonder if you walked into the wrong cinema, and by the end of the third reel, you'll be begging to be put out of your misery. This film is an abomination.\",\n",
              "       b'\"Boy Next Door\" is a hilarious romp through male neurosis. In just over fifteen minutes, the film takes us on a journey that most full-length features can\\'t even match. Great performances, excellent camera work and editing---this short is a classic from start to finish. Kudos to Travis Davis for pulling double duty as both director and star. He\\'s the funniest nebbish since Woody Allen. And what a treat to see Richard Moll back again. If you thought all he could do was play \"Bull\" the bailiff on \"Night Court\", think again. This gem of a film showcases the brilliantly funny writing of Stephen Garvey. Remember that name. Forget Charlie Kaufman, Steve Garvey is the true current king of quirky comedy.',\n",
              "       b'Great movie. Post-apocalyptic films kick ass. This one is no exception. Kept up the pace and interest without a speck of dialogue (mainly through some good character development). The fight between Reno and the Hero was tight. I also liked the use of cave paintings and medieval-like weapons to show how primitive and savage mankind had become without their technology and guzzaline. The connection between the beginning and end was a little spacey, that is, I had a hard time understanding the distances between the hotel and the opening sequence. In sum, kick ass character progression, design, story without the cushion of dialogue, and most importantly, the always appreciated desolate scenery of a post-apocalyptic wasteland.',\n",
              "       b\"Whoever filled this stupid idea of acting and producing a movie in Himesh's head, which is always hidden under a cap, covering almost half of his face all the time ? Only hope this is first and the last as well, for God's sake ! From Assalam Valekum to Gayatri Mantra, Himesh has tried every thing, to create an aura of his so-called singing talent, which is nothing but atrocious pronunciations of words like Tanhaiyya, which completely kill the beauty of the terms, so commonly used for love songs. Why does Himesh not smile ? Simple, because he does not use close Up toothpaste ! Now there this friend of his, tailing him around every where, and this number one lawyer in the town, who has to herself sexily wiggle and try to seduce Himesh, of all the handsome German people she might have met earlier, perhaps the male lawyers on this part of the world might be cursing their fate, for destined to deal with the stiff, unattractive lot, every day ! The action scenes are so funnily shot, like the event planner attacked by the thief, autos riding over the cars, so on and so forth. The father of the bride seems to be in a great hurry to get rid of his daughter by marrying her off, that he flies to and fro around. Most hypocrite, he praises HR for distributing love among people of the world, as if they were sweets , and on his back, coolly gives a lecture to his daughter on these show business men. when Himesh is proved innocent, he again unceremoniously dumps the other guy, as if it's a game of musical chair ! i didn't get to se the poor guy's face even, did you ? Hansika in the role of Ria, looks as if she is in need of an eye check up, for strain in her eyes ! The fellow in the role of a friend is good, who has acted quite naturally, and should be in better movies, like Sharman Joshi, for example. Child artist in Trishu's brief appearance is sweet, but wasted. It seems today's young generation has gone nuts , since they prefer this kind of mockery of lyrics and musical scores , and associate gossip with it, for example, if you sing ek bar aaja, the ghost would come. this is a weird taste in music, and rather funny. i am surprised, how such classic lyricists like Gulzar , have opted for Himesh of all the people, to give music. There is story in Panchtantra , that a crow attach\\xc3\\xa9s so many feathers of a peacock, to look beautiful, and appeal to the birds; but the feathers fall off ultimately, and the real dark crow is revealed ! Hope Himesh takes a hint, and refrains from manufacturing such meaningless stuff, and wasting precious money, which he has earned by taxing his short nose so much ! His friend does tell him, if your nose is cut, how will you sing ? Thanks for showing us Germany, Himesh, at a reasonable cost of renting the DVD ! and correct those spelling mistakes, will you ? an extra e in movie, and no e in love ! There is also a famous number from the old film Sholey, Mehbooba, on which Mallika Sherawat wiggles, once again, but this time with Himesh, winking at her, and conveniently, Ria , his so-called real love, and his new bride is not around ! Now that was very clever, Himesh ! At least one thing in this movie which you have done smartly, to justify Sherawat's presence . But doesn't she look a bit washed down ?\",\n",
              "       b\"Jean Dujardin gets Connery's mannerisms down pat: the adjusting the cuff links when entering a club as all the women turn to admire him, the nonchalant straightening and smoothing down of the tie, the swaggering, steely gait. It's uncanny, and you come to realise just how much of Bond in the Sixties was Connery's creation and not really Ian Fleming's character. <br /><br />The cinematography is a nod to those early films, the movie takes off From Russia With Love and Thunderball mainly. The main joke is how chauvinistic the hero is, not just in terms of sexism but nationalism and colonialism, and how he puts noses out of joint when he is sent to Egypt. <br /><br />It's not perfect - about 20 mins in it seems a one-joke movie and bits of it remind one of spoofs of the day, of which there were plenty. Morcecambe and Wise's The Intelligence Men had suspect-looking men in fez's following their heroes around too, and that's going back a bit. Unlike Sellers' Clouseau or Baron Cohen's Borat, Dujardin doesn't give his character that layer of realness or genuine pathos - he is too busy perfecting his Connery mannerisms. It doesn't do enough with the credits or a big song, and there's no funny or serious villain, like Mike Myers' Dr Evil or Ricardo Montalban's Naked Gun nemesis, for the hero to go up against.<br /><br />But the scene where OSS117 wakes up in Cairo one morning had me laughing out loud in the three-quarters empty cinema, and the whole thing looks wonderful, plus you'll never get a chance to see Operation Kid Brother on the screen, and the women are ace crumpet, really hot. It's a Bond spoof without falling into the mad scientist/Ken Adam sets or funny gadgets routine. Throughly recommended.\",\n",
              "       b'This is just a long advertisement for the movie \"The Death Tunnel\". Although it is an interesting history of the Waverly Hills Sanatorium, the whole ghost theory is up to your interpretation. More \"Ghost Orbs\" which you can duplicate with dust in front of a flash camera, and the \"Ectoplasmic Mist\" is just someone\\'s cigarette smoke lit up with the camera flash. I couldn\\'t see any \"Shadow People\" until they drew an outline around the blurry distortion of the image. No scientific explanations were suggested for the phenomena, only paranormal. Perhaps these are only events one would have to see to believe, so be sure to make plans to visit the Sanatorium on your next vacation in Louisville, Kentucky.',\n",
              "       b'If you\\'re like me and you occasionally enjoy watching terrible movies (I guess it\\'s kind of like slowing down at a car crash), you can\\'t do better than this! The plot is inane, the special effects are hilarious and the acting is some of the worst you\\'ll ever see! 4 THUMBS DOWN! WOOOHOOOOOOOO!!! Seriously, I have no idea how the director and the \"actors\" can sleep at night! It\\'s painful, and yet hysterically funny, to watch and I highly recommend it for those who want to punish themselves for something. If you can watch this crap without wincing, you\\'re a better man than I\\'ll ever be! I wonder if the producer of this garbage had any idea what he was getting himself (and his money) into!',\n",
              "       b\"My gosh, this movie was nothing more than filmmaking by numbers. Struggling salesman can't make a go of it in New York, mentor with a heart of gold takes him under his wing, struggling salesman moves to California and makes it big, then loses it big, then bounces back with the simple life, then hits rock bottom trying to get back to the top. I don't think I can remember any part of the plot that took more than five seconds to develop. Case in point (spoiler?): When the John Kapelos character calls to say he and his girlfriend were coming to Santa Cruz to visit, and James Woods says there's practically no chance he would come, you knew with 100% certainty they were coming in the next scene or two.<br /><br />On the other hand, Sean Young sure looked good.\",\n",
              "       b\"Apparently this was an award winner. Apparently someone had a gun against his/her head and was force to nominate Maize: the Movie.<br /><br />Or this must have been a mistake.<br /><br />This is the most unwatchable movie ever made. The screening and the editing is the biggest horror of this movie. Two little girls get lost in a cornfield and get stalked by someone who can be heard laughing under his rubber mask. The little girls run into their hero dad, and then runs away from him, W.T.F.? The hero dad in the movie keeps losing track of them in the few minutes of watching this.<br /><br />The girls obviously weren't trained actors, and had no common sense to them. They were so annoying and so infantile in the movie, it not even remotely comedic. Hearing them scream over and over again like a broken record was the reason why I got up and left. You can't even listen to this movie without nearly going into convulsions.<br /><br />I can puke a better award winner than this garbage.\",\n",
              "       b\"Keys to the VIP is one of the most entertaining, informative and hilarious shows that is on television right now. The idea is original, and well executed, as it manages to preserve the reality aspect, but still remain entertaining. All of the judges have a razor wit. They're not the nicest at all times, but if you're looking for comfort, go watch a chick flick. Say what you like about validity of the show, but it is absolutely real. I know people who have competed on the show, and they have confirmed this. <br /><br />If you want to laugh, watch this show. It is on of the best comedy shows ever made.\",\n",
              "       b\"This is one of Cassavetes' best performances. The entire cast is outstanding, as is Martin Ritt's sublimely understated direction. The anger, angst, and desparation of urban labor battles is magnificently told in a fashion that is neither obtrusive nor patronizing. In a way it is dated with its era, but in many ways, it is gloriously timeless.\",\n",
              "       b\"The Merchant of Four Seasons isn't what I would call a happy movie, at all, or even one that impressed me to the point of praising it to the sky (there are other Fassbinder flicks for that, like Veronika Voss and the underrated Satan's Brew). But it's certainly no less than a fascinating experiment in taking a look at those in a society that you and me and others we know might possibly know, or not really want to know. I imagine in the early 70s in Germany a generation, coming out of WW2, had a stigma to live with but tried their best just to get by. This is a stigma that floats all over this film, and in many instances in Fassbinder's work in general, but especially because with Four Seasons he takes his eye on the middle class, and a particular married couple- the distanced, depressed, angry Hans the fruit seller and his long-suffered wife- that is nothing short than trying for realism in the guise of melodrama. If Cassavetes were a crazy German he might make this film, maybe even as just a lark.<br /><br />The story sounds simple enough, where Hans' drinking gets out of control, he beats his wife (this scene is one of the toughest to take, maybe in just any movie, the way Fassbinder's camera lingers without a cut as his wife is left helpless and their daughter trying to stop him in his frenzy) and then she's ready to leave him. As he stands in the room, her family holding him back, she makes the call for divorce and he gets a heart attack right there. He recovers, his business suddenly starts booming again with some help from some good (or not so good) employees - and yet this only continues his longing, for another woman, and his despair in general.<br /><br />And yet it's in this simplicity that Fassbinder tries, and succeeds for the most part, in attaining a mood of dread, of a tense vibe in a kitchen or in the bedroom or out on the street that you can cut with a knife and bleed out. The weakest part of this all may be the acting... at least that was my initial impression. Hans, played by Hirschmuller, can be a stilted presence, with only the slightest movements in his face and eyes, and for a while it doesn't look like he's much of a good actor. The actress playing his wife, Irm Hermann, and her sister (Fassbinder Hanna Schygulla) fare better, but only cause they're given more to do conventionally, like cry or look concerned. It takes some time to adjust to what is, essentially, a void in his guy Hans, of something from his own psychological self-torment or self-pity that pervades himself and those around him who just want to get on with some sense of normalcy, especially once Hans gets successful.<br /><br />Not everything clicks together in The Merchant of Four Seasons, but enough did to make me recommend it to those looking for a different slice-of-life than you might be used to with more modern American movies. Fassbinder's world here is a combat between the melodrama he loves in cinema and the harsh, crushing sense of humanism that he feels personally and puts into characters that, for better or worse, we somehow identify with. Are the Epps a family you know of? Or could you even be them? Who's to say. It's a methodical study of tragic emptiness in the human spirit, and its goals are all attained.\",\n",
              "       b\"I'm a collector of films starring Ms. Weaver, so I bought this only because of her being in it. I find it really odd that her early career is filled with so many awful movies. She started with incredible promise in Alien but then had a slew of bombs. These bombs include this movie, Deal of the Century, One Woman or Two, and Half Moon Street. She also appeared in The Year Of Living Dangerously, which was not a bomb, but her performance was less than notable. In the time between Alien and it's 1986 sequel, Aliens, the only movie she did that was worth anything was Ghostbusters. before the release of Aliens, I'm sure everyone thought this woman was on her way out. Luckily she wasn't.<br /><br />Back to Eyewitness though, the film is boring. It doesn't create any suspense. William Hurt seems like a cardboard stand in, and the atmosphere is just to dry. Sigourney is decent but nothing worth remembering.<br /><br />Watch this movie if you must but don't go in with any expectations of a decent movie. Watch better movies with these two stars like Accidental Tourist and Working Girl.\",\n",
              "       b'The big names in film tried to do their part for the war effort, and Charlie Chaplin was no exception. This patriotic and propagandist picture is part of his contribution, although the war was nearly over by the time of its release. The tramp goes to war, humorously accomplishes acts of heroism and kicks the Kaiser in the bum. It\\'s a very funny film, although I don\\'t think it nearly one of his best. It\\'s with \\'A Dog\\'s Life\\' as his better output for First National before he made his early masterpiece \\'The Kid.\\' They are his first three-reelers, which contain sustained, more elaborate gags than he could usually orchestrate in his two-reel shorts at Mutual.<br /><br />It can be difficult to balance a pro-war message with slapstick antics and scenes of burlesque on the front, but one wouldn\\'t think so watching \\'Shoulder Arms.\\' It\\'s also preferable in many respects to a \"more serious,\" dramatic work with a similar message, such as Griffith\\'s \\'Hearts of the World.\\' Chaplin had become a true virtuoso of screen comedy by this time; he makes it look effortless. He knew very well by now that a film with fewer gags--with more elaboration, refinement and careful timing--could be better than any knockabout, Keystone-type farce with a dozen pratfalls a minute. The sequence where Chaplin is disguised as a tree is a pertinent example. Even with wars raging, Chaplin can lift the spirits of millions.',\n",
              "       b'I haven\\'t seen this film since it came out in the mid 70s, but I do recall it as being a very realistic portrayal of the music business ( right up there with Paul Simons \"One Trick Pony \" ..another vastly underrated film IMO )<br /><br />Harvey Keitel does an excellent job as a producer caught between the music he believes in , and the commercial \"tripe\" the record company \"suits\" want him to work with.<br /><br />Since I spent my entire career in the music business as a composer /arranger /producer, I can really vouch for the verisimilitude this film possesses. <br /><br />If it should ever come out on DVD uncut, I\\'d buy it!',\n",
              "       b\"Considering all of the comedies with a military situation that have been done in history, someone had to be the first. One could make a case that in Shoulder Arms, Charlie Chaplin invented the genre.<br /><br />Hard to believe that back then this was a daring move. When you consider that some of the best films involving such people as Bob Hope, Abbott&Costello, Laurel&Hardy involved military service and made during war time, it's just something you accept and laugh at.<br /><br />In the First World War Chaplin along with fellow stars Douglas Fairbanks and Mary Pickford went out on bond tours. He was a great supporter of the Allied cause, unusual for someone of his left wing views. It would seem only natural that the Tramp would be drafted and unfortunately would flummox around and wreak havoc on all.<br /><br />A lot of things you'd see in the service comedies of World War II got their start in Shoulder Arms. Chaplin had no more imitators because within a few weeks of the film's release, the war was over.<br /><br />But a comedy art form had been established by one of comedy's greatest geniuses.\",\n",
              "       b'I haven\\'t seen every single movie that Burt Reynolds has ever made, but this one (which I\\'ve just finished watching, for the third time) may very well be his best! It suffers only from some slow stretches; Burt perhaps tried to make it more \"arty\" than it should have been. On the other hand, he managed to avoid many of the usual cliches in the presentation of the \"tough cop\" role he plays (notice, for example, the scene in which he attempts to kiss Rachel Ward for the first time, or the fear he expresses just before the final showdown with the indestructible Henry Silva). In fact, Silva and those two ninja assassins are three of the most memorable villains of cop thrillers of the 80s. The film also has some offbeat touches, a surprising amount of humor, a brutal and gripping fistfight and many well-directed shots. (***)'],\n",
              "      dtype=object)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IN5T6MfmUSGy",
        "outputId": "92187f74-3094-4c22-9ce7-9aad9d8f2bfe"
      },
      "source": [
        "next(iter(test_classifier_ds))[0][0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(256,), dtype=int64, numpy=\n",
              "array([   48,     7,    11,    29,    37,    82,     2,   818,   280,\n",
              "          37,   373,     9,   280,    38,     4, 24672,   570,   475,\n",
              "           2,   150,    23,   369,    34,    40,   556,   497,     6,\n",
              "         571,    63,   465,    10,  1703,   138,    31,     5,    11,\n",
              "          19,    33,    91,    30,     2,   125,    10,    13,  3225,\n",
              "          50,    10,  5173,    10,    67,   974,   443,   616,     5,\n",
              "          54,   119,   193,    21,   536,     5,    11,    19,   111,\n",
              "           9,   120,   155,   212,    45,     8,     2,   439,    39,\n",
              "         111,     9,   139,   805,     6,   266,     4,   255,   183,\n",
              "         315,    48,     7,    55,    15,    12,     9,    13,    40,\n",
              "           4,   359,    19,    34,   442,  6246,     2,   267,   101,\n",
              "        5566,    29,   183,     6,    26,    28,     5,     2,   243,\n",
              "          94,    10,    25,   120,   105,    10,    57,   391,   358,\n",
              "           6,    84,     6,    21,   436,    63,    59,    15,    11,\n",
              "          22,    97,  1110,   122,    59,   144,     2,   267,   101,\n",
              "        5566,    29,     4,   163,   123,    72,   176,    48,    10,\n",
              "         102,     5,     2,    19,   155,   134,    25,    10,   974,\n",
              "          54,    59,   487,    42,     9,  4592,  8397,    29,    62,\n",
              "         623,    69,    29,   974,   443,   616,     3,   297,   225,\n",
              "         148,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0])>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MX-PLnfhUgKo",
        "outputId": "ed160f0e-6760-4349-9033-181eb2665c33"
      },
      "source": [
        "def bert_module(query, key, value, i):\n",
        "    # Multi headed self-attention\n",
        "    attention_output = layers.MultiHeadAttention(\n",
        "        num_heads=config.NUM_HEAD,\n",
        "        key_dim=config.EMBED_DIM // config.NUM_HEAD,\n",
        "        name=\"encoder_{}/multiheadattention\".format(i),\n",
        "    )(query, key, value)\n",
        "    attention_output = layers.Dropout(0.1, name=\"encoder_{}/att_dropout\".format(i))(\n",
        "        attention_output\n",
        "    )\n",
        "    attention_output = layers.LayerNormalization(\n",
        "        epsilon=1e-6, name=\"encoder_{}/att_layernormalization\".format(i)\n",
        "    )(query + attention_output)\n",
        "\n",
        "    # Feed-forward layer\n",
        "    ffn = keras.Sequential(\n",
        "        [\n",
        "            layers.Dense(config.FF_DIM, activation=\"relu\"),\n",
        "            layers.Dense(config.EMBED_DIM),\n",
        "        ],\n",
        "        name=\"encoder_{}/ffn\".format(i),\n",
        "    )\n",
        "    ffn_output = ffn(attention_output)\n",
        "    ffn_output = layers.Dropout(0.1, name=\"encoder_{}/ffn_dropout\".format(i))(\n",
        "        ffn_output\n",
        "    )\n",
        "    sequence_output = layers.LayerNormalization(\n",
        "        epsilon=1e-6, name=\"encoder_{}/ffn_layernormalization\".format(i)\n",
        "    )(attention_output + ffn_output)\n",
        "    return sequence_output\n",
        "\n",
        "\n",
        "def get_pos_encoding_matrix(max_len, d_emb):\n",
        "    pos_enc = np.array(\n",
        "        [\n",
        "            [pos / np.power(10000, 2 * (j // 2) / d_emb) for j in range(d_emb)]\n",
        "            if pos != 0\n",
        "            else np.zeros(d_emb)\n",
        "            for pos in range(max_len)\n",
        "        ]\n",
        "    )\n",
        "    pos_enc[1:, 0::2] = np.sin(pos_enc[1:, 0::2])  # dim 2i\n",
        "    pos_enc[1:, 1::2] = np.cos(pos_enc[1:, 1::2])  # dim 2i+1\n",
        "    return pos_enc\n",
        "\n",
        "\n",
        "loss_fn = keras.losses.SparseCategoricalCrossentropy(\n",
        "    reduction=tf.keras.losses.Reduction.NONE\n",
        ")\n",
        "loss_tracker = tf.keras.metrics.Mean(name=\"loss\")\n",
        "\n",
        "\n",
        "class MaskedLanguageModel(tf.keras.Model):\n",
        "    def train_step(self, inputs):\n",
        "        if len(inputs) == 3:\n",
        "            features, labels, sample_weight = inputs\n",
        "        else:\n",
        "            features, labels = inputs\n",
        "            sample_weight = None\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            predictions = self(features, training=True)\n",
        "            loss = loss_fn(labels, predictions, sample_weight=sample_weight)\n",
        "\n",
        "        # Compute gradients\n",
        "        trainable_vars = self.trainable_variables\n",
        "        gradients = tape.gradient(loss, trainable_vars)\n",
        "\n",
        "        # Update weights\n",
        "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
        "\n",
        "        # Compute our own metrics\n",
        "        loss_tracker.update_state(loss, sample_weight=sample_weight)\n",
        "\n",
        "        # Return a dict mapping metric names to current value\n",
        "        return {\"loss\": loss_tracker.result()}\n",
        "\n",
        "    @property\n",
        "    def metrics(self):\n",
        "        # We list our `Metric` objects here so that `reset_states()` can be\n",
        "        # called automatically at the start of each epoch\n",
        "        # or at the start of `evaluate()`.\n",
        "        # If you don't implement this property, you have to call\n",
        "        # `reset_states()` yourself at the time of your choosing.\n",
        "        return [loss_tracker]\n",
        "\n",
        "\n",
        "def create_masked_language_bert_model():\n",
        "    inputs = layers.Input((config.MAX_LEN,), dtype=tf.int64)\n",
        "\n",
        "    word_embeddings = layers.Embedding(\n",
        "        config.VOCAB_SIZE, config.EMBED_DIM, name=\"word_embedding\"\n",
        "    )(inputs)\n",
        "    position_embeddings = layers.Embedding(\n",
        "        input_dim=config.MAX_LEN,\n",
        "        output_dim=config.EMBED_DIM,\n",
        "        weights=[get_pos_encoding_matrix(config.MAX_LEN, config.EMBED_DIM)],\n",
        "        name=\"position_embedding\",\n",
        "    )(tf.range(start=0, limit=config.MAX_LEN, delta=1))\n",
        "    embeddings = word_embeddings + position_embeddings\n",
        "\n",
        "    encoder_output = embeddings\n",
        "    for i in range(config.NUM_LAYERS):\n",
        "        encoder_output = bert_module(encoder_output, encoder_output, encoder_output, i)\n",
        "\n",
        "    mlm_output = layers.Dense(config.VOCAB_SIZE, name=\"mlm_cls\", activation=\"softmax\")(\n",
        "        encoder_output\n",
        "    )\n",
        "    mlm_model = MaskedLanguageModel(inputs, mlm_output, name=\"masked_bert_model\")\n",
        "\n",
        "    optimizer = keras.optimizers.Adam(learning_rate=config.LR)\n",
        "    mlm_model.compile(optimizer=optimizer)\n",
        "    return mlm_model\n",
        "\n",
        "\n",
        "id2token = dict(enumerate(vectorize_layer.get_vocabulary()))\n",
        "token2id = {y: x for x, y in id2token.items()}\n",
        "\n",
        "\n",
        "class MaskedTextGenerator(keras.callbacks.Callback):\n",
        "    def __init__(self, sample_tokens, top_k=5):\n",
        "        self.sample_tokens = sample_tokens\n",
        "        self.k = top_k\n",
        "\n",
        "    def decode(self, tokens):\n",
        "        return \" \".join([id2token[t] for t in tokens if t != 0])\n",
        "\n",
        "    def convert_ids_to_tokens(self, id):\n",
        "        return id2token[id]\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        prediction = self.model.predict(self.sample_tokens)\n",
        "\n",
        "        masked_index = np.where(self.sample_tokens == mask_token_id)\n",
        "        masked_index = masked_index[1]\n",
        "        mask_prediction = prediction[0][masked_index]\n",
        "\n",
        "        top_indices = mask_prediction[0].argsort()[-self.k :][::-1]\n",
        "        values = mask_prediction[0][top_indices]\n",
        "\n",
        "        for i in range(len(top_indices)):\n",
        "            p = top_indices[i]\n",
        "            v = values[i]\n",
        "            tokens = np.copy(sample_tokens[0])\n",
        "            tokens[masked_index[0]] = p\n",
        "            result = {\n",
        "                \"input_text\": self.decode(sample_tokens[0].numpy()),\n",
        "                \"prediction\": self.decode(tokens),\n",
        "                \"probability\": v,\n",
        "                \"predicted mask token\": self.convert_ids_to_tokens(p),\n",
        "            }\n",
        "            pprint(result)\n",
        "\n",
        "\n",
        "sample_tokens = vectorize_layer([\"I have watched this [mask] and it was awesome\"])\n",
        "generator_callback = MaskedTextGenerator(sample_tokens.numpy())\n",
        "\n",
        "bert_masked_model = create_masked_language_bert_model()\n",
        "bert_masked_model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"masked_bert_model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 256)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "word_embedding (Embedding)      (None, 256, 128)     3840000     input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "tf.__operators__.add (TFOpLambd (None, 256, 128)     0           word_embedding[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "encoder_0/multiheadattention (M (None, 256, 128)     66048       tf.__operators__.add[0][0]       \n",
            "                                                                 tf.__operators__.add[0][0]       \n",
            "                                                                 tf.__operators__.add[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "encoder_0/att_dropout (Dropout) (None, 256, 128)     0           encoder_0/multiheadattention[0][0\n",
            "__________________________________________________________________________________________________\n",
            "tf.__operators__.add_1 (TFOpLam (None, 256, 128)     0           tf.__operators__.add[0][0]       \n",
            "                                                                 encoder_0/att_dropout[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "encoder_0/att_layernormalizatio (None, 256, 128)     256         tf.__operators__.add_1[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "encoder_0/ffn (Sequential)      (None, 256, 128)     33024       encoder_0/att_layernormalization[\n",
            "__________________________________________________________________________________________________\n",
            "encoder_0/ffn_dropout (Dropout) (None, 256, 128)     0           encoder_0/ffn[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "tf.__operators__.add_2 (TFOpLam (None, 256, 128)     0           encoder_0/att_layernormalization[\n",
            "                                                                 encoder_0/ffn_dropout[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "encoder_0/ffn_layernormalizatio (None, 256, 128)     256         tf.__operators__.add_2[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "mlm_cls (Dense)                 (None, 256, 30000)   3870000     encoder_0/ffn_layernormalization[\n",
            "==================================================================================================\n",
            "Total params: 7,809,584\n",
            "Trainable params: 7,809,584\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g2iL0_TRbBOq",
        "outputId": "1c720947-2051-496a-c49f-17176d2b05ac"
      },
      "source": [
        "mlm_ds"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<BatchDataset shapes: ((None, 256), (None, 256), (None, 256)), types: (tf.int64, tf.int64, tf.float64)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A3cEToPdbiqJ",
        "outputId": "f3fd1373-26fd-48c5-fa40-ceb6471182a8"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<bound method MaskedTextGenerator.on_epoch_end of <__main__.MaskedTextGenerator object at 0x7f4ba4939690>>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qzppc4KfbkaK",
        "outputId": "186c63b3-a811-462a-a327-75ef74213f45"
      },
      "source": [
        "bert_masked_model.fit(mlm_ds, epochs=5, callbacks=[generator_callback])\n",
        "bert_masked_model.save(\"bert_mlm_imdb.h5\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "1563/1563 [==============================] - 348s 220ms/step - loss: 6.9792\n",
            "{'input_text': 'i have watched this [mask] and it was awesome',\n",
            " 'predicted mask token': 'this',\n",
            " 'prediction': 'i have watched this this and it was awesome',\n",
            " 'probability': 0.06871921}\n",
            "{'input_text': 'i have watched this [mask] and it was awesome',\n",
            " 'predicted mask token': 'a',\n",
            " 'prediction': 'i have watched this a and it was awesome',\n",
            " 'probability': 0.05148873}\n",
            "{'input_text': 'i have watched this [mask] and it was awesome',\n",
            " 'predicted mask token': 'movie',\n",
            " 'prediction': 'i have watched this movie and it was awesome',\n",
            " 'probability': 0.03854019}\n",
            "{'input_text': 'i have watched this [mask] and it was awesome',\n",
            " 'predicted mask token': 'of',\n",
            " 'prediction': 'i have watched this of and it was awesome',\n",
            " 'probability': 0.034013562}\n",
            "{'input_text': 'i have watched this [mask] and it was awesome',\n",
            " 'predicted mask token': 'i',\n",
            " 'prediction': 'i have watched this i and it was awesome',\n",
            " 'probability': 0.02866348}\n",
            "Epoch 2/5\n",
            "1563/1563 [==============================] - 345s 221ms/step - loss: 6.4692\n",
            "{'input_text': 'i have watched this [mask] and it was awesome',\n",
            " 'predicted mask token': 'movie',\n",
            " 'prediction': 'i have watched this movie and it was awesome',\n",
            " 'probability': 0.6522561}\n",
            "{'input_text': 'i have watched this [mask] and it was awesome',\n",
            " 'predicted mask token': 'is',\n",
            " 'prediction': 'i have watched this is and it was awesome',\n",
            " 'probability': 0.037504043}\n",
            "{'input_text': 'i have watched this [mask] and it was awesome',\n",
            " 'predicted mask token': 'film',\n",
            " 'prediction': 'i have watched this film and it was awesome',\n",
            " 'probability': 0.030748975}\n",
            "{'input_text': 'i have watched this [mask] and it was awesome',\n",
            " 'predicted mask token': 'first',\n",
            " 'prediction': 'i have watched this first and it was awesome',\n",
            " 'probability': 0.022252262}\n",
            "{'input_text': 'i have watched this [mask] and it was awesome',\n",
            " 'predicted mask token': 'one',\n",
            " 'prediction': 'i have watched this one and it was awesome',\n",
            " 'probability': 0.021578012}\n",
            "Epoch 3/5\n",
            "1563/1563 [==============================] - 347s 222ms/step - loss: 5.8842\n",
            "{'input_text': 'i have watched this [mask] and it was awesome',\n",
            " 'predicted mask token': 'movie',\n",
            " 'prediction': 'i have watched this movie and it was awesome',\n",
            " 'probability': 0.53892654}\n",
            "{'input_text': 'i have watched this [mask] and it was awesome',\n",
            " 'predicted mask token': 'film',\n",
            " 'prediction': 'i have watched this film and it was awesome',\n",
            " 'probability': 0.12117333}\n",
            "{'input_text': 'i have watched this [mask] and it was awesome',\n",
            " 'predicted mask token': 'one',\n",
            " 'prediction': 'i have watched this one and it was awesome',\n",
            " 'probability': 0.06364077}\n",
            "{'input_text': 'i have watched this [mask] and it was awesome',\n",
            " 'predicted mask token': 'is',\n",
            " 'prediction': 'i have watched this is and it was awesome',\n",
            " 'probability': 0.016052099}\n",
            "{'input_text': 'i have watched this [mask] and it was awesome',\n",
            " 'predicted mask token': 'show',\n",
            " 'prediction': 'i have watched this show and it was awesome',\n",
            " 'probability': 0.014673562}\n",
            "Epoch 4/5\n",
            "1563/1563 [==============================] - 348s 223ms/step - loss: 5.4058\n",
            "{'input_text': 'i have watched this [mask] and it was awesome',\n",
            " 'predicted mask token': 'movie',\n",
            " 'prediction': 'i have watched this movie and it was awesome',\n",
            " 'probability': 0.42979586}\n",
            "{'input_text': 'i have watched this [mask] and it was awesome',\n",
            " 'predicted mask token': 'one',\n",
            " 'prediction': 'i have watched this one and it was awesome',\n",
            " 'probability': 0.15530944}\n",
            "{'input_text': 'i have watched this [mask] and it was awesome',\n",
            " 'predicted mask token': 'film',\n",
            " 'prediction': 'i have watched this film and it was awesome',\n",
            " 'probability': 0.10320356}\n",
            "{'input_text': 'i have watched this [mask] and it was awesome',\n",
            " 'predicted mask token': 'recommended',\n",
            " 'prediction': 'i have watched this recommended and it was awesome',\n",
            " 'probability': 0.015434429}\n",
            "{'input_text': 'i have watched this [mask] and it was awesome',\n",
            " 'predicted mask token': 'show',\n",
            " 'prediction': 'i have watched this show and it was awesome',\n",
            " 'probability': 0.011281365}\n",
            "Epoch 5/5\n",
            "1563/1563 [==============================] - 345s 221ms/step - loss: 4.8956\n",
            "{'input_text': 'i have watched this [mask] and it was awesome',\n",
            " 'predicted mask token': 'movie',\n",
            " 'prediction': 'i have watched this movie and it was awesome',\n",
            " 'probability': 0.5867157}\n",
            "{'input_text': 'i have watched this [mask] and it was awesome',\n",
            " 'predicted mask token': 'film',\n",
            " 'prediction': 'i have watched this film and it was awesome',\n",
            " 'probability': 0.09918898}\n",
            "{'input_text': 'i have watched this [mask] and it was awesome',\n",
            " 'predicted mask token': 'one',\n",
            " 'prediction': 'i have watched this one and it was awesome',\n",
            " 'probability': 0.03807246}\n",
            "{'input_text': 'i have watched this [mask] and it was awesome',\n",
            " 'predicted mask token': 'stinker',\n",
            " 'prediction': 'i have watched this stinker and it was awesome',\n",
            " 'probability': 0.016193205}\n",
            "{'input_text': 'i have watched this [mask] and it was awesome',\n",
            " 'predicted mask token': 'show',\n",
            " 'prediction': 'i have watched this show and it was awesome',\n",
            " 'probability': 0.013052396}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vWdj6vNLcCVY"
      },
      "source": [
        "# Load pretrained bert model\n",
        "mlm_model = keras.models.load_model(\n",
        "    \"bert_mlm_imdb.h5\", custom_objects={\"MaskedLanguageModel\": MaskedLanguageModel})\n",
        "\n",
        "pretrained_bert_model = tf.keras.Model(mlm_model.input, mlm_model.get_layer(\"encoder_0/ffn_layernormalization\").output)\n",
        "\n",
        "# Freeze it\n",
        "pretrained_bert_model.trainable = False\n",
        "\n",
        "def create_classifier_bert_model():\n",
        "  inputs = layers.Input((config.MAX_LEN,), dtype = tf.int64)\n",
        "  sequence_output = pretrained_bert_model(inputs)\n",
        "  pooled_output = layers.GlobalMaxPooling1D()(sequence_output)\n",
        "  hidden_layer = layers.Dense(64, activation=\"relu\")(pooled_output)\n",
        "  outputs = layers.Dense(1, activation=\"sigmoid\")(hidden_layer)\n",
        "\n",
        "  classifer_model = keras.Model(inputs, outputs, name=\"classification\")\n",
        "  optimizer = keras.optimizers.Adam()\n",
        "  classifer_model.compile(\n",
        "        optimizer=optimizer, loss=\"binary_crossentropy\", metrics=[\"accuracy\"]\n",
        "    )\n",
        "  return classifer_model\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gcqhxAdpfa-K",
        "outputId": "fe75afc7-68b4-4390-fed2-5589d171802e"
      },
      "source": [
        "classifer_model = create_classifier_bert_model()\n",
        "classifer_model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"classification\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_2 (InputLayer)         [(None, 256)]             0         \n",
            "_________________________________________________________________\n",
            "model (Functional)           (None, 256, 128)          3939584   \n",
            "_________________________________________________________________\n",
            "global_max_pooling1d (Global (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 64)                8256      \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 1)                 65        \n",
            "=================================================================\n",
            "Total params: 3,947,905\n",
            "Trainable params: 8,321\n",
            "Non-trainable params: 3,939,584\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bk6C8eRBfbGm",
        "outputId": "3347b3ed-c454-40a8-aa4d-69283b239256"
      },
      "source": [
        "# Train the classifier with frozen BERT stage\n",
        "classifer_model.fit(\n",
        "    train_classifier_ds,\n",
        "    epochs=5,\n",
        "    validation_data=test_classifier_ds,\n",
        ")\n",
        "\n",
        "# Unfreeze the BERT model for fine-tuning\n",
        "pretrained_bert_model.trainable = True\n",
        "optimizer = keras.optimizers.Adam()\n",
        "classifer_model.compile(\n",
        "    optimizer=optimizer, loss=\"binary_crossentropy\", metrics=[\"accuracy\"]\n",
        ")\n",
        "classifer_model.fit(\n",
        "    train_classifier_ds,\n",
        "    epochs=5,\n",
        "    validation_data=test_classifier_ds,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "782/782 [==============================] - 23s 27ms/step - loss: 0.7818 - accuracy: 0.5341 - val_loss: 0.6885 - val_accuracy: 0.5749\n",
            "Epoch 2/5\n",
            "782/782 [==============================] - 21s 27ms/step - loss: 0.6729 - accuracy: 0.6006 - val_loss: 0.6462 - val_accuracy: 0.6243\n",
            "Epoch 3/5\n",
            "782/782 [==============================] - 21s 27ms/step - loss: 0.6614 - accuracy: 0.6127 - val_loss: 0.6330 - val_accuracy: 0.6417\n",
            "Epoch 4/5\n",
            "782/782 [==============================] - 21s 27ms/step - loss: 0.6489 - accuracy: 0.6250 - val_loss: 0.6302 - val_accuracy: 0.6452\n",
            "Epoch 5/5\n",
            "782/782 [==============================] - 21s 26ms/step - loss: 0.6438 - accuracy: 0.6308 - val_loss: 0.6295 - val_accuracy: 0.6450\n",
            "Epoch 1/5\n",
            "782/782 [==============================] - 61s 77ms/step - loss: 0.5447 - accuracy: 0.7247 - val_loss: 0.3355 - val_accuracy: 0.8537\n",
            "Epoch 2/5\n",
            "782/782 [==============================] - 61s 78ms/step - loss: 0.2940 - accuracy: 0.8730 - val_loss: 0.3402 - val_accuracy: 0.8608\n",
            "Epoch 3/5\n",
            "782/782 [==============================] - 60s 77ms/step - loss: 0.1659 - accuracy: 0.9377 - val_loss: 0.4634 - val_accuracy: 0.8432\n",
            "Epoch 4/5\n",
            "782/782 [==============================] - 59s 76ms/step - loss: 0.0709 - accuracy: 0.9755 - val_loss: 0.5575 - val_accuracy: 0.8482\n",
            "Epoch 5/5\n",
            "782/782 [==============================] - 60s 77ms/step - loss: 0.0373 - accuracy: 0.9868 - val_loss: 0.5624 - val_accuracy: 0.8510\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f4bc6448090>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CWdZkT9gjB7E",
        "outputId": "a3eb4ae4-99ff-49eb-dd3d-89df0994e7d5"
      },
      "source": [
        "def get_end_to_end(model):\n",
        "    inputs_string = keras.Input(shape=(1,), dtype=\"string\")\n",
        "    indices = vectorize_layer(inputs_string)\n",
        "    outputs = model(indices)\n",
        "    end_to_end_model = keras.Model(inputs_string, outputs, name=\"end_to_end_model\")\n",
        "    optimizer = keras.optimizers.Adam(learning_rate=config.LR)\n",
        "    end_to_end_model.compile(\n",
        "        optimizer=optimizer, loss=\"binary_crossentropy\", metrics=[\"accuracy\"]\n",
        "    )\n",
        "    return end_to_end_model\n",
        "\n",
        "\n",
        "end_to_end_classification_model = get_end_to_end(classifer_model)\n",
        "end_to_end_classification_model.evaluate(test_raw_classifier_ds)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "782/782 [==============================] - 12s 15ms/step - loss: 0.5676 - accuracy: 0.8522\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.5624432563781738, 0.8510000109672546]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F1oP3BkRkCVy"
      },
      "source": [
        "  "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}