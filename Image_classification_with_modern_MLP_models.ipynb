{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Image classification with modern MLP models.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyNilZH5GHCVSDPlBHbS6ETO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Lee-Gunju/AI-paper-code-review-for-personal-project/blob/master/Image_classification_with_modern_MLP_models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VbC-WWYtBqY4",
        "outputId": "e73678c9-2406-4cec-a521-97193272dcc4"
      },
      "source": [
        "pip install -U tensorflow-addons"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow-addons\n",
            "  Downloading tensorflow_addons-0.14.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
            "\u001b[?25l\r\u001b[K     |▎                               | 10 kB 33.5 MB/s eta 0:00:01\r\u001b[K     |▋                               | 20 kB 32.6 MB/s eta 0:00:01\r\u001b[K     |▉                               | 30 kB 19.0 MB/s eta 0:00:01\r\u001b[K     |█▏                              | 40 kB 16.0 MB/s eta 0:00:01\r\u001b[K     |█▌                              | 51 kB 6.9 MB/s eta 0:00:01\r\u001b[K     |█▊                              | 61 kB 7.1 MB/s eta 0:00:01\r\u001b[K     |██                              | 71 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |██▍                             | 81 kB 8.7 MB/s eta 0:00:01\r\u001b[K     |██▋                             | 92 kB 8.7 MB/s eta 0:00:01\r\u001b[K     |███                             | 102 kB 6.6 MB/s eta 0:00:01\r\u001b[K     |███▎                            | 112 kB 6.6 MB/s eta 0:00:01\r\u001b[K     |███▌                            | 122 kB 6.6 MB/s eta 0:00:01\r\u001b[K     |███▉                            | 133 kB 6.6 MB/s eta 0:00:01\r\u001b[K     |████▏                           | 143 kB 6.6 MB/s eta 0:00:01\r\u001b[K     |████▍                           | 153 kB 6.6 MB/s eta 0:00:01\r\u001b[K     |████▊                           | 163 kB 6.6 MB/s eta 0:00:01\r\u001b[K     |█████                           | 174 kB 6.6 MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 184 kB 6.6 MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 194 kB 6.6 MB/s eta 0:00:01\r\u001b[K     |██████                          | 204 kB 6.6 MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 215 kB 6.6 MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 225 kB 6.6 MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 235 kB 6.6 MB/s eta 0:00:01\r\u001b[K     |███████                         | 245 kB 6.6 MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 256 kB 6.6 MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 266 kB 6.6 MB/s eta 0:00:01\r\u001b[K     |████████                        | 276 kB 6.6 MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 286 kB 6.6 MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 296 kB 6.6 MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 307 kB 6.6 MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 317 kB 6.6 MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 327 kB 6.6 MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 337 kB 6.6 MB/s eta 0:00:01\r\u001b[K     |██████████                      | 348 kB 6.6 MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 358 kB 6.6 MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 368 kB 6.6 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 378 kB 6.6 MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 389 kB 6.6 MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 399 kB 6.6 MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 409 kB 6.6 MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 419 kB 6.6 MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 430 kB 6.6 MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 440 kB 6.6 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 450 kB 6.6 MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 460 kB 6.6 MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 471 kB 6.6 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 481 kB 6.6 MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 491 kB 6.6 MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 501 kB 6.6 MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 512 kB 6.6 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 522 kB 6.6 MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 532 kB 6.6 MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 542 kB 6.6 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 552 kB 6.6 MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 563 kB 6.6 MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 573 kB 6.6 MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 583 kB 6.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 593 kB 6.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 604 kB 6.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 614 kB 6.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 624 kB 6.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 634 kB 6.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 645 kB 6.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 655 kB 6.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 665 kB 6.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 675 kB 6.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 686 kB 6.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 696 kB 6.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 706 kB 6.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 716 kB 6.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 727 kB 6.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 737 kB 6.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 747 kB 6.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 757 kB 6.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 768 kB 6.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 778 kB 6.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 788 kB 6.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 798 kB 6.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 808 kB 6.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 819 kB 6.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 829 kB 6.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 839 kB 6.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 849 kB 6.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 860 kB 6.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 870 kB 6.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 880 kB 6.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 890 kB 6.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 901 kB 6.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 911 kB 6.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 921 kB 6.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 931 kB 6.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 942 kB 6.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 952 kB 6.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 962 kB 6.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 972 kB 6.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 983 kB 6.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 993 kB 6.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 1.0 MB 6.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▎  | 1.0 MB 6.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 1.0 MB 6.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 1.0 MB 6.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 1.0 MB 6.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 1.1 MB 6.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 1.1 MB 6.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 1.1 MB 6.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 1.1 MB 6.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 1.1 MB 6.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.1 MB 6.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.1 MB 6.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow-addons) (2.7.1)\n",
            "Installing collected packages: tensorflow-addons\n",
            "Successfully installed tensorflow-addons-0.14.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zcHsM9ZVCI7C"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import tensorflow_addons as tfa"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oGQnm2h9CL2p",
        "outputId": "a2aa3b37-4cd9-4053-a206-cf5371739d6e"
      },
      "source": [
        "num_classes = 100\n",
        "input_shape = (32, 32, 3)\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = keras.datasets.cifar100.load_data()\n",
        "\n",
        "print(f\"x_train shape: {x_train.shape} - y_train shape: {y_train.shape}\")\n",
        "print(f\"x_test shape: {x_test.shape} - y_test shape: {y_test.shape}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz\n",
            "169009152/169001437 [==============================] - 4s 0us/step\n",
            "169017344/169001437 [==============================] - 4s 0us/step\n",
            "x_train shape: (50000, 32, 32, 3) - y_train shape: (50000, 1)\n",
            "x_test shape: (10000, 32, 32, 3) - y_test shape: (10000, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VKbw-FV8CN_B",
        "outputId": "feb1c3f7-31e9-422e-ca6f-3a61df3869bf"
      },
      "source": [
        "weight_decay = 0.0001\n",
        "batch_size = 128\n",
        "num_epochs = 50\n",
        "dropout_rate = 0.2\n",
        "image_size = 64  # We'll resize input images to this size.\n",
        "patch_size = 8  # Size of the patches to be extracted from the input images.\n",
        "num_patches = (image_size // patch_size) ** 2  # Size of the data array.\n",
        "embedding_dim = 256  # Number of hidden units.\n",
        "num_blocks = 4  # Number of blocks.\n",
        "\n",
        "print(f\"Image size: {image_size} X {image_size} = {image_size ** 2}\")\n",
        "print(f\"Patch size: {patch_size} X {patch_size} = {patch_size ** 2} \")\n",
        "print(f\"Patches per image: {num_patches}\")\n",
        "print(f\"Elements per patch (3 channels): {(patch_size ** 2) * 3}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Image size: 64 X 64 = 4096\n",
            "Patch size: 8 X 8 = 64 \n",
            "Patches per image: 64\n",
            "Elements per patch (3 channels): 192\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cFQC_QbOCQCj"
      },
      "source": [
        "def build_classifier(blocks, positional_encoding=False):\n",
        "    inputs = layers.Input(shape=input_shape)\n",
        "    # Augment data.\n",
        "    augmented = data_augmentation(inputs)\n",
        "    # Create patches.\n",
        "    patches = Patches(patch_size, num_patches)(augmented)\n",
        "    # Encode patches to generate a [batch_size, num_patches, embedding_dim] tensor.\n",
        "    x = layers.Dense(units=embedding_dim)(patches)\n",
        "    if positional_encoding:\n",
        "        positions = tf.range(start=0, limit=num_patches, delta=1)\n",
        "        position_embedding = layers.Embedding(\n",
        "            input_dim=num_patches, output_dim=embedding_dim\n",
        "        )(positions)\n",
        "        x = x + position_embedding\n",
        "    # Process x using the module blocks.\n",
        "    x = blocks(x)\n",
        "    # Apply global average pooling to generate a [batch_size, embedding_dim] representation tensor.\n",
        "    representation = layers.GlobalAveragePooling1D()(x)\n",
        "    # Apply dropout.\n",
        "    representation = layers.Dropout(rate=dropout_rate)(representation)\n",
        "    # Compute logits outputs.\n",
        "    logits = layers.Dense(num_classes)(representation)\n",
        "    # Create the Keras model.\n",
        "    return keras.Model(inputs=inputs, outputs=logits)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Juf4QvnbCfQB"
      },
      "source": [
        "def run_experiment(model):\n",
        "    # Create Adam optimizer with weight decay.\n",
        "    optimizer = tfa.optimizers.AdamW(\n",
        "        learning_rate=learning_rate, weight_decay=weight_decay,\n",
        "    )\n",
        "    # Compile the model.\n",
        "    model.compile(\n",
        "        optimizer=optimizer,\n",
        "        loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "        metrics=[\n",
        "            keras.metrics.SparseCategoricalAccuracy(name=\"acc\"),\n",
        "            keras.metrics.SparseTopKCategoricalAccuracy(5, name=\"top5-acc\"),\n",
        "        ],\n",
        "    )\n",
        "    # Create a learning rate scheduler callback.\n",
        "    reduce_lr = keras.callbacks.ReduceLROnPlateau(\n",
        "        monitor=\"val_loss\", factor=0.5, patience=5\n",
        "    )\n",
        "    # Create an early stopping callback.\n",
        "    early_stopping = tf.keras.callbacks.EarlyStopping(\n",
        "        monitor=\"val_loss\", patience=10, restore_best_weights=True\n",
        "    )\n",
        "    # Fit the model.\n",
        "    history = model.fit(\n",
        "        x=x_train,\n",
        "        y=y_train,\n",
        "        batch_size=batch_size,\n",
        "        epochs=num_epochs,\n",
        "        validation_split=0.1,\n",
        "        callbacks=[early_stopping, reduce_lr],\n",
        "    )\n",
        "\n",
        "    _, accuracy, top_5_accuracy = model.evaluate(x_test, y_test)\n",
        "    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n",
        "    print(f\"Test top 5 accuracy: {round(top_5_accuracy * 100, 2)}%\")\n",
        "\n",
        "    # Return history to plot learning curves.\n",
        "    return history"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FDaD9lXeCks8"
      },
      "source": [
        "data_augmentation = keras.Sequential(\n",
        "    [\n",
        "        layers.Normalization(),\n",
        "        layers.Resizing(image_size, image_size),\n",
        "        layers.RandomFlip(\"horizontal\"),\n",
        "        layers.RandomZoom(\n",
        "            height_factor=0.2, width_factor=0.2\n",
        "        ),\n",
        "    ],\n",
        "    name=\"data_augmentation\",\n",
        ")\n",
        "# Compute the mean and the variance of the training data for normalization.\n",
        "data_augmentation.layers[0].adapt(x_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9zPi19SpCl1B"
      },
      "source": [
        "class Patches(layers.Layer):\n",
        "    def __init__(self, patch_size, num_patches):\n",
        "        super(Patches, self).__init__()\n",
        "        self.patch_size = patch_size\n",
        "        self.num_patches = num_patches\n",
        "\n",
        "    def call(self, images):\n",
        "        batch_size = tf.shape(images)[0]\n",
        "        patches = tf.image.extract_patches(\n",
        "            images=images,\n",
        "            sizes=[1, self.patch_size, self.patch_size, 1],\n",
        "            strides=[1, self.patch_size, self.patch_size, 1],\n",
        "            rates=[1, 1, 1, 1],\n",
        "            padding=\"VALID\",\n",
        "        )\n",
        "        patch_dims = patches.shape[-1]\n",
        "        patches = tf.reshape(patches, [batch_size, self.num_patches, patch_dims])\n",
        "        return patches"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pY_vmb3DCx_h"
      },
      "source": [
        "# The MLP-Mixer model\n",
        "class MLPMixerLayer(layers.Layer):\n",
        "    def __init__(self, num_patches, hidden_units, dropout_rate, *args, **kwargs):\n",
        "        super(MLPMixerLayer, self).__init__(*args, **kwargs)\n",
        "\n",
        "        self.mlp1 = keras.Sequential(\n",
        "            [\n",
        "                layers.Dense(units=num_patches),\n",
        "                tfa.layers.GELU(),\n",
        "                layers.Dense(units=num_patches),\n",
        "                layers.Dropout(rate=dropout_rate),\n",
        "            ]\n",
        "        )\n",
        "        self.mlp2 = keras.Sequential(\n",
        "            [\n",
        "                layers.Dense(units=num_patches),\n",
        "                tfa.layers.GELU(),\n",
        "                layers.Dense(units=embedding_dim),\n",
        "                layers.Dropout(rate=dropout_rate),\n",
        "            ]\n",
        "        )\n",
        "        self.normalize = layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # Apply layer normalization.\n",
        "        x = self.normalize(inputs)\n",
        "        # Transpose inputs from [num_batches, num_patches, hidden_units] to [num_batches, hidden_units, num_patches].\n",
        "        x_channels = tf.linalg.matrix_transpose(x)\n",
        "        # Apply mlp1 on each channel independently.\n",
        "        mlp1_outputs = self.mlp1(x_channels)\n",
        "        # Transpose mlp1_outputs from [num_batches, hidden_dim, num_patches] to [num_batches, num_patches, hidden_units].\n",
        "        mlp1_outputs = tf.linalg.matrix_transpose(mlp1_outputs)\n",
        "        # Add skip connection.\n",
        "        x = mlp1_outputs + inputs\n",
        "        # Apply layer normalization.\n",
        "        x_patches = self.normalize(x)\n",
        "        # Apply mlp2 on each patch independtenly.\n",
        "        mlp2_outputs = self.mlp2(x_patches)\n",
        "        # Add skip connection.\n",
        "        x = x + mlp2_outputs\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bhWqsz5QDBcg",
        "outputId": "bfe3f985-76d8-42aa-b67a-7b7b21980daf"
      },
      "source": [
        "mlpmixer_blocks = keras.Sequential(\n",
        "    [MLPMixerLayer(num_patches, embedding_dim, dropout_rate) for _ in range(num_blocks)]\n",
        ")\n",
        "learning_rate = 0.005\n",
        "mlpmixer_classifier = build_classifier(mlpmixer_blocks)\n",
        "history = run_experiment(mlpmixer_classifier)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "352/352 [==============================] - 23s 50ms/step - loss: 3.8560 - acc: 0.1153 - top5-acc: 0.3220 - val_loss: 3.4379 - val_acc: 0.1780 - val_top5-acc: 0.4544\n",
            "Epoch 2/50\n",
            "352/352 [==============================] - 17s 48ms/step - loss: 3.3501 - acc: 0.1946 - top5-acc: 0.4661 - val_loss: 3.3250 - val_acc: 0.2128 - val_top5-acc: 0.4898\n",
            "Epoch 3/50\n",
            "352/352 [==============================] - 17s 48ms/step - loss: 3.1591 - acc: 0.2274 - top5-acc: 0.5178 - val_loss: 3.1246 - val_acc: 0.2536 - val_top5-acc: 0.5564\n",
            "Epoch 4/50\n",
            "352/352 [==============================] - 17s 48ms/step - loss: 2.9920 - acc: 0.2636 - top5-acc: 0.5614 - val_loss: 3.0327 - val_acc: 0.2644 - val_top5-acc: 0.5748\n",
            "Epoch 5/50\n",
            "352/352 [==============================] - 17s 48ms/step - loss: 2.8982 - acc: 0.2801 - top5-acc: 0.5811 - val_loss: 2.8722 - val_acc: 0.2926 - val_top5-acc: 0.5974\n",
            "Epoch 6/50\n",
            "352/352 [==============================] - 17s 48ms/step - loss: 2.8088 - acc: 0.2960 - top5-acc: 0.6029 - val_loss: 2.8303 - val_acc: 0.3144 - val_top5-acc: 0.6190\n",
            "Epoch 7/50\n",
            "352/352 [==============================] - 17s 48ms/step - loss: 2.7342 - acc: 0.3118 - top5-acc: 0.6227 - val_loss: 2.6468 - val_acc: 0.3360 - val_top5-acc: 0.6428\n",
            "Epoch 8/50\n",
            "352/352 [==============================] - 17s 48ms/step - loss: 2.6796 - acc: 0.3210 - top5-acc: 0.6356 - val_loss: 2.7219 - val_acc: 0.3350 - val_top5-acc: 0.6458\n",
            "Epoch 9/50\n",
            "352/352 [==============================] - 17s 48ms/step - loss: 2.6440 - acc: 0.3285 - top5-acc: 0.6432 - val_loss: 2.6305 - val_acc: 0.3422 - val_top5-acc: 0.6592\n",
            "Epoch 10/50\n",
            "352/352 [==============================] - 17s 48ms/step - loss: 2.6024 - acc: 0.3341 - top5-acc: 0.6542 - val_loss: 2.5968 - val_acc: 0.3458 - val_top5-acc: 0.6606\n",
            "Epoch 11/50\n",
            "352/352 [==============================] - 17s 48ms/step - loss: 2.5599 - acc: 0.3436 - top5-acc: 0.6626 - val_loss: 2.7193 - val_acc: 0.3320 - val_top5-acc: 0.6442\n",
            "Epoch 12/50\n",
            "352/352 [==============================] - 17s 48ms/step - loss: 2.5259 - acc: 0.3512 - top5-acc: 0.6694 - val_loss: 2.5945 - val_acc: 0.3602 - val_top5-acc: 0.6732\n",
            "Epoch 13/50\n",
            "352/352 [==============================] - 17s 48ms/step - loss: 2.5039 - acc: 0.3585 - top5-acc: 0.6738 - val_loss: 2.5406 - val_acc: 0.3662 - val_top5-acc: 0.6810\n",
            "Epoch 14/50\n",
            "352/352 [==============================] - 17s 48ms/step - loss: 2.4775 - acc: 0.3604 - top5-acc: 0.6792 - val_loss: 2.5924 - val_acc: 0.3570 - val_top5-acc: 0.6802\n",
            "Epoch 15/50\n",
            "352/352 [==============================] - 17s 48ms/step - loss: 2.4407 - acc: 0.3700 - top5-acc: 0.6867 - val_loss: 2.4338 - val_acc: 0.3838 - val_top5-acc: 0.7022\n",
            "Epoch 16/50\n",
            "352/352 [==============================] - 17s 48ms/step - loss: 2.4346 - acc: 0.3696 - top5-acc: 0.6898 - val_loss: 2.5749 - val_acc: 0.3644 - val_top5-acc: 0.6854\n",
            "Epoch 17/50\n",
            "352/352 [==============================] - 17s 48ms/step - loss: 2.3904 - acc: 0.3774 - top5-acc: 0.7005 - val_loss: 2.5414 - val_acc: 0.3720 - val_top5-acc: 0.6848\n",
            "Epoch 18/50\n",
            "352/352 [==============================] - 17s 48ms/step - loss: 2.3757 - acc: 0.3800 - top5-acc: 0.7043 - val_loss: 2.4807 - val_acc: 0.3774 - val_top5-acc: 0.6940\n",
            "Epoch 19/50\n",
            "352/352 [==============================] - 17s 48ms/step - loss: 2.3602 - acc: 0.3862 - top5-acc: 0.7027 - val_loss: 2.4023 - val_acc: 0.3816 - val_top5-acc: 0.7086\n",
            "Epoch 20/50\n",
            "352/352 [==============================] - 17s 48ms/step - loss: 2.3333 - acc: 0.3895 - top5-acc: 0.7131 - val_loss: 2.3824 - val_acc: 0.3938 - val_top5-acc: 0.7090\n",
            "Epoch 21/50\n",
            "352/352 [==============================] - 17s 48ms/step - loss: 2.3125 - acc: 0.3930 - top5-acc: 0.7165 - val_loss: 2.3763 - val_acc: 0.4008 - val_top5-acc: 0.7162\n",
            "Epoch 22/50\n",
            "352/352 [==============================] - 17s 48ms/step - loss: 2.2937 - acc: 0.3999 - top5-acc: 0.7182 - val_loss: 2.4144 - val_acc: 0.3824 - val_top5-acc: 0.7126\n",
            "Epoch 23/50\n",
            "352/352 [==============================] - 17s 48ms/step - loss: 2.2777 - acc: 0.4005 - top5-acc: 0.7247 - val_loss: 2.3850 - val_acc: 0.3958 - val_top5-acc: 0.7274\n",
            "Epoch 24/50\n",
            "352/352 [==============================] - 17s 48ms/step - loss: 2.2625 - acc: 0.4051 - top5-acc: 0.7270 - val_loss: 2.3590 - val_acc: 0.4108 - val_top5-acc: 0.7224\n",
            "Epoch 25/50\n",
            "352/352 [==============================] - 17s 48ms/step - loss: 2.2473 - acc: 0.4094 - top5-acc: 0.7305 - val_loss: 2.3097 - val_acc: 0.4174 - val_top5-acc: 0.7322\n",
            "Epoch 26/50\n",
            "352/352 [==============================] - 17s 48ms/step - loss: 2.2310 - acc: 0.4119 - top5-acc: 0.7310 - val_loss: 2.3335 - val_acc: 0.4062 - val_top5-acc: 0.7222\n",
            "Epoch 27/50\n",
            "352/352 [==============================] - 17s 48ms/step - loss: 2.2171 - acc: 0.4176 - top5-acc: 0.7366 - val_loss: 2.3321 - val_acc: 0.4154 - val_top5-acc: 0.7316\n",
            "Epoch 28/50\n",
            "352/352 [==============================] - 17s 48ms/step - loss: 2.2124 - acc: 0.4168 - top5-acc: 0.7362 - val_loss: 2.3279 - val_acc: 0.4172 - val_top5-acc: 0.7338\n",
            "Epoch 29/50\n",
            "352/352 [==============================] - 17s 49ms/step - loss: 2.2067 - acc: 0.4192 - top5-acc: 0.7372 - val_loss: 2.2914 - val_acc: 0.4216 - val_top5-acc: 0.7338\n",
            "Epoch 30/50\n",
            "352/352 [==============================] - 17s 48ms/step - loss: 2.1900 - acc: 0.4189 - top5-acc: 0.7410 - val_loss: 2.3323 - val_acc: 0.4156 - val_top5-acc: 0.7264\n",
            "Epoch 31/50\n",
            "352/352 [==============================] - 17s 48ms/step - loss: 2.1760 - acc: 0.4255 - top5-acc: 0.7432 - val_loss: 2.2453 - val_acc: 0.4276 - val_top5-acc: 0.7386\n",
            "Epoch 32/50\n",
            "352/352 [==============================] - 17s 48ms/step - loss: 2.1673 - acc: 0.4267 - top5-acc: 0.7448 - val_loss: 2.3013 - val_acc: 0.4144 - val_top5-acc: 0.7328\n",
            "Epoch 33/50\n",
            "352/352 [==============================] - 17s 48ms/step - loss: 2.1481 - acc: 0.4290 - top5-acc: 0.7494 - val_loss: 2.2683 - val_acc: 0.4180 - val_top5-acc: 0.7386\n",
            "Epoch 34/50\n",
            "352/352 [==============================] - 17s 48ms/step - loss: 2.1533 - acc: 0.4304 - top5-acc: 0.7491 - val_loss: 2.2683 - val_acc: 0.4208 - val_top5-acc: 0.7380\n",
            "Epoch 35/50\n",
            "352/352 [==============================] - 17s 48ms/step - loss: 2.1356 - acc: 0.4332 - top5-acc: 0.7505 - val_loss: 2.2808 - val_acc: 0.4286 - val_top5-acc: 0.7422\n",
            "Epoch 36/50\n",
            "352/352 [==============================] - 17s 48ms/step - loss: 2.1347 - acc: 0.4331 - top5-acc: 0.7532 - val_loss: 2.2219 - val_acc: 0.4278 - val_top5-acc: 0.7476\n",
            "Epoch 37/50\n",
            "352/352 [==============================] - 17s 48ms/step - loss: 2.1257 - acc: 0.4376 - top5-acc: 0.7527 - val_loss: 2.2247 - val_acc: 0.4390 - val_top5-acc: 0.7480\n",
            "Epoch 38/50\n",
            "352/352 [==============================] - 17s 48ms/step - loss: 2.1192 - acc: 0.4364 - top5-acc: 0.7538 - val_loss: 2.2902 - val_acc: 0.4300 - val_top5-acc: 0.7402\n",
            "Epoch 39/50\n",
            "352/352 [==============================] - 17s 48ms/step - loss: 2.1154 - acc: 0.4390 - top5-acc: 0.7554 - val_loss: 2.2631 - val_acc: 0.4260 - val_top5-acc: 0.7446\n",
            "Epoch 40/50\n",
            "352/352 [==============================] - 17s 48ms/step - loss: 2.1049 - acc: 0.4393 - top5-acc: 0.7571 - val_loss: 2.3459 - val_acc: 0.4204 - val_top5-acc: 0.7346\n",
            "Epoch 41/50\n",
            "352/352 [==============================] - 17s 48ms/step - loss: 2.0959 - acc: 0.4430 - top5-acc: 0.7588 - val_loss: 2.2409 - val_acc: 0.4352 - val_top5-acc: 0.7534\n",
            "Epoch 42/50\n",
            "352/352 [==============================] - 17s 48ms/step - loss: 1.8720 - acc: 0.4907 - top5-acc: 0.7988 - val_loss: 2.0713 - val_acc: 0.4712 - val_top5-acc: 0.7768\n",
            "Epoch 43/50\n",
            "352/352 [==============================] - 17s 48ms/step - loss: 1.8265 - acc: 0.5013 - top5-acc: 0.8060 - val_loss: 2.0572 - val_acc: 0.4702 - val_top5-acc: 0.7762\n",
            "Epoch 44/50\n",
            "352/352 [==============================] - 17s 48ms/step - loss: 1.8065 - acc: 0.5057 - top5-acc: 0.8097 - val_loss: 2.0099 - val_acc: 0.4834 - val_top5-acc: 0.7840\n",
            "Epoch 45/50\n",
            "352/352 [==============================] - 17s 48ms/step - loss: 1.7949 - acc: 0.5098 - top5-acc: 0.8112 - val_loss: 2.0741 - val_acc: 0.4660 - val_top5-acc: 0.7802\n",
            "Epoch 46/50\n",
            "352/352 [==============================] - 17s 48ms/step - loss: 1.7853 - acc: 0.5119 - top5-acc: 0.8146 - val_loss: 2.0232 - val_acc: 0.4778 - val_top5-acc: 0.7832\n",
            "Epoch 47/50\n",
            "352/352 [==============================] - 17s 48ms/step - loss: 1.7899 - acc: 0.5071 - top5-acc: 0.8138 - val_loss: 2.0601 - val_acc: 0.4796 - val_top5-acc: 0.7798\n",
            "Epoch 48/50\n",
            "352/352 [==============================] - 17s 48ms/step - loss: 1.7756 - acc: 0.5134 - top5-acc: 0.8155 - val_loss: 2.0404 - val_acc: 0.4748 - val_top5-acc: 0.7802\n",
            "Epoch 49/50\n",
            "352/352 [==============================] - 17s 48ms/step - loss: 1.7683 - acc: 0.5138 - top5-acc: 0.8184 - val_loss: 1.9929 - val_acc: 0.4834 - val_top5-acc: 0.7890\n",
            "Epoch 50/50\n",
            "352/352 [==============================] - 17s 48ms/step - loss: 1.7653 - acc: 0.5186 - top5-acc: 0.8189 - val_loss: 2.0090 - val_acc: 0.4782 - val_top5-acc: 0.7876\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 1.9907 - acc: 0.4849 - top5-acc: 0.7804\n",
            "Test accuracy: 48.49%\n",
            "Test top 5 accuracy: 78.04%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xFKE1-N0DC1g"
      },
      "source": [
        "# The FNet model\n",
        "class FNetLayer(layers.Layer):\n",
        "    def __init__(self, num_patches, embedding_dim, dropout_rate, *args, **kwargs):\n",
        "        super(FNetLayer, self).__init__(*args, **kwargs)\n",
        "\n",
        "        self.ffn = keras.Sequential(\n",
        "            [\n",
        "                layers.Dense(units=embedding_dim),\n",
        "                tfa.layers.GELU(),\n",
        "                layers.Dropout(rate=dropout_rate),\n",
        "                layers.Dense(units=embedding_dim),\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        self.normalize1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.normalize2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # Apply fourier transformations.\n",
        "        x = tf.cast(\n",
        "            tf.signal.fft2d(tf.cast(inputs, dtype=tf.dtypes.complex64)),\n",
        "            dtype=tf.dtypes.float32,\n",
        "        )\n",
        "        # Add skip connection.\n",
        "        x = x + inputs\n",
        "        # Apply layer normalization.\n",
        "        x = self.normalize1(x)\n",
        "        # Apply Feedfowrad network.\n",
        "        x_ffn = self.ffn(x)\n",
        "        # Add skip connection.\n",
        "        x = x + x_ffn\n",
        "        # Apply layer normalization.\n",
        "        return self.normalize2(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8BODp_QuDR3I",
        "outputId": "fdf85bc8-f811-41d0-eb66-d94a0900fe09"
      },
      "source": [
        "fnet_blocks = keras.Sequential(\n",
        "    [FNetLayer(num_patches, embedding_dim, dropout_rate) for _ in range(num_blocks)]\n",
        ")\n",
        "learning_rate = 0.001\n",
        "fnet_classifier = build_classifier(fnet_blocks, positional_encoding=True)\n",
        "history = run_experiment(fnet_classifier)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "352/352 [==============================] - 21s 51ms/step - loss: 4.1316 - acc: 0.0722 - top5-acc: 0.2298 - val_loss: 3.8144 - val_acc: 0.1210 - val_top5-acc: 0.3260\n",
            "Epoch 2/50\n",
            "352/352 [==============================] - 17s 50ms/step - loss: 3.6952 - acc: 0.1332 - top5-acc: 0.3616 - val_loss: 3.4867 - val_acc: 0.1694 - val_top5-acc: 0.4210\n",
            "Epoch 3/50\n",
            "352/352 [==============================] - 17s 49ms/step - loss: 3.4322 - acc: 0.1771 - top5-acc: 0.4383 - val_loss: 3.2588 - val_acc: 0.2144 - val_top5-acc: 0.4806\n",
            "Epoch 4/50\n",
            "352/352 [==============================] - 17s 50ms/step - loss: 3.2206 - acc: 0.2140 - top5-acc: 0.4943 - val_loss: 3.1250 - val_acc: 0.2342 - val_top5-acc: 0.5174\n",
            "Epoch 5/50\n",
            "352/352 [==============================] - 17s 49ms/step - loss: 3.0824 - acc: 0.2371 - top5-acc: 0.5340 - val_loss: 2.9492 - val_acc: 0.2702 - val_top5-acc: 0.5662\n",
            "Epoch 6/50\n",
            "352/352 [==============================] - 17s 49ms/step - loss: 2.9693 - acc: 0.2583 - top5-acc: 0.5606 - val_loss: 2.8938 - val_acc: 0.2742 - val_top5-acc: 0.5792\n",
            "Epoch 7/50\n",
            "352/352 [==============================] - 17s 49ms/step - loss: 2.8822 - acc: 0.2762 - top5-acc: 0.5834 - val_loss: 2.7961 - val_acc: 0.2962 - val_top5-acc: 0.6006\n",
            "Epoch 8/50\n",
            "352/352 [==============================] - 17s 49ms/step - loss: 2.8105 - acc: 0.2909 - top5-acc: 0.6010 - val_loss: 2.7196 - val_acc: 0.3050 - val_top5-acc: 0.6210\n",
            "Epoch 9/50\n",
            "352/352 [==============================] - 17s 49ms/step - loss: 2.7455 - acc: 0.3035 - top5-acc: 0.6150 - val_loss: 2.6537 - val_acc: 0.3208 - val_top5-acc: 0.6312\n",
            "Epoch 10/50\n",
            "352/352 [==============================] - 17s 49ms/step - loss: 2.6786 - acc: 0.3179 - top5-acc: 0.6326 - val_loss: 2.6464 - val_acc: 0.3238 - val_top5-acc: 0.6414\n",
            "Epoch 11/50\n",
            "352/352 [==============================] - 17s 50ms/step - loss: 2.6252 - acc: 0.3276 - top5-acc: 0.6455 - val_loss: 2.5938 - val_acc: 0.3450 - val_top5-acc: 0.6414\n",
            "Epoch 12/50\n",
            "352/352 [==============================] - 17s 49ms/step - loss: 2.5729 - acc: 0.3385 - top5-acc: 0.6559 - val_loss: 2.5106 - val_acc: 0.3506 - val_top5-acc: 0.6636\n",
            "Epoch 13/50\n",
            "352/352 [==============================] - 17s 49ms/step - loss: 2.5326 - acc: 0.3470 - top5-acc: 0.6672 - val_loss: 2.5006 - val_acc: 0.3612 - val_top5-acc: 0.6670\n",
            "Epoch 14/50\n",
            "352/352 [==============================] - 17s 49ms/step - loss: 2.4904 - acc: 0.3574 - top5-acc: 0.6719 - val_loss: 2.4563 - val_acc: 0.3720 - val_top5-acc: 0.6788\n",
            "Epoch 15/50\n",
            "352/352 [==============================] - 18s 50ms/step - loss: 2.4411 - acc: 0.3683 - top5-acc: 0.6835 - val_loss: 2.4115 - val_acc: 0.3714 - val_top5-acc: 0.6858\n",
            "Epoch 16/50\n",
            "352/352 [==============================] - 17s 49ms/step - loss: 2.4044 - acc: 0.3716 - top5-acc: 0.6950 - val_loss: 2.4353 - val_acc: 0.3748 - val_top5-acc: 0.6850\n",
            "Epoch 17/50\n",
            "352/352 [==============================] - 17s 50ms/step - loss: 2.3755 - acc: 0.3806 - top5-acc: 0.6977 - val_loss: 2.3878 - val_acc: 0.3858 - val_top5-acc: 0.6962\n",
            "Epoch 18/50\n",
            "352/352 [==============================] - 17s 49ms/step - loss: 2.3442 - acc: 0.3835 - top5-acc: 0.7041 - val_loss: 2.3364 - val_acc: 0.3856 - val_top5-acc: 0.7036\n",
            "Epoch 19/50\n",
            "352/352 [==============================] - 17s 49ms/step - loss: 2.3061 - acc: 0.3952 - top5-acc: 0.7131 - val_loss: 2.3257 - val_acc: 0.3904 - val_top5-acc: 0.7044\n",
            "Epoch 20/50\n",
            "352/352 [==============================] - 17s 49ms/step - loss: 2.2854 - acc: 0.4024 - top5-acc: 0.7186 - val_loss: 2.3248 - val_acc: 0.3934 - val_top5-acc: 0.7018\n",
            "Epoch 21/50\n",
            "352/352 [==============================] - 17s 50ms/step - loss: 2.2573 - acc: 0.4054 - top5-acc: 0.7249 - val_loss: 2.2974 - val_acc: 0.4038 - val_top5-acc: 0.7198\n",
            "Epoch 22/50\n",
            "352/352 [==============================] - 17s 49ms/step - loss: 2.2369 - acc: 0.4083 - top5-acc: 0.7282 - val_loss: 2.2735 - val_acc: 0.4082 - val_top5-acc: 0.7194\n",
            "Epoch 23/50\n",
            "352/352 [==============================] - 17s 49ms/step - loss: 2.2144 - acc: 0.4129 - top5-acc: 0.7340 - val_loss: 2.2712 - val_acc: 0.4072 - val_top5-acc: 0.7190\n",
            "Epoch 24/50\n",
            "352/352 [==============================] - 17s 49ms/step - loss: 2.1968 - acc: 0.4179 - top5-acc: 0.7376 - val_loss: 2.2908 - val_acc: 0.4010 - val_top5-acc: 0.7140\n",
            "Epoch 25/50\n",
            "352/352 [==============================] - 17s 50ms/step - loss: 2.1769 - acc: 0.4231 - top5-acc: 0.7411 - val_loss: 2.2789 - val_acc: 0.4072 - val_top5-acc: 0.7248\n",
            "Epoch 26/50\n",
            "352/352 [==============================] - 17s 49ms/step - loss: 2.1617 - acc: 0.4288 - top5-acc: 0.7442 - val_loss: 2.2491 - val_acc: 0.4074 - val_top5-acc: 0.7222\n",
            "Epoch 27/50\n",
            "352/352 [==============================] - 17s 49ms/step - loss: 2.1434 - acc: 0.4263 - top5-acc: 0.7471 - val_loss: 2.2580 - val_acc: 0.4122 - val_top5-acc: 0.7230\n",
            "Epoch 28/50\n",
            "352/352 [==============================] - 17s 49ms/step - loss: 2.1249 - acc: 0.4308 - top5-acc: 0.7514 - val_loss: 2.2498 - val_acc: 0.4132 - val_top5-acc: 0.7278\n",
            "Epoch 29/50\n",
            "352/352 [==============================] - 17s 50ms/step - loss: 2.1143 - acc: 0.4357 - top5-acc: 0.7534 - val_loss: 2.2615 - val_acc: 0.4150 - val_top5-acc: 0.7216\n",
            "Epoch 30/50\n",
            "352/352 [==============================] - 17s 49ms/step - loss: 2.0894 - acc: 0.4444 - top5-acc: 0.7582 - val_loss: 2.2710 - val_acc: 0.4082 - val_top5-acc: 0.7202\n",
            "Epoch 31/50\n",
            "352/352 [==============================] - 17s 49ms/step - loss: 2.0729 - acc: 0.4469 - top5-acc: 0.7613 - val_loss: 2.1917 - val_acc: 0.4280 - val_top5-acc: 0.7338\n",
            "Epoch 32/50\n",
            "352/352 [==============================] - 17s 50ms/step - loss: 2.0672 - acc: 0.4456 - top5-acc: 0.7655 - val_loss: 2.2152 - val_acc: 0.4256 - val_top5-acc: 0.7354\n",
            "Epoch 33/50\n",
            "352/352 [==============================] - 17s 49ms/step - loss: 2.0598 - acc: 0.4497 - top5-acc: 0.7651 - val_loss: 2.1878 - val_acc: 0.4256 - val_top5-acc: 0.7376\n",
            "Epoch 34/50\n",
            "352/352 [==============================] - 17s 49ms/step - loss: 2.0432 - acc: 0.4510 - top5-acc: 0.7677 - val_loss: 2.2222 - val_acc: 0.4156 - val_top5-acc: 0.7356\n",
            "Epoch 35/50\n",
            "352/352 [==============================] - 17s 49ms/step - loss: 2.0369 - acc: 0.4520 - top5-acc: 0.7690 - val_loss: 2.1583 - val_acc: 0.4350 - val_top5-acc: 0.7406\n",
            "Epoch 36/50\n",
            "352/352 [==============================] - 17s 49ms/step - loss: 2.0171 - acc: 0.4612 - top5-acc: 0.7710 - val_loss: 2.1739 - val_acc: 0.4220 - val_top5-acc: 0.7410\n",
            "Epoch 37/50\n",
            "352/352 [==============================] - 17s 49ms/step - loss: 2.0100 - acc: 0.4589 - top5-acc: 0.7739 - val_loss: 2.1812 - val_acc: 0.4294 - val_top5-acc: 0.7396\n",
            "Epoch 38/50\n",
            "352/352 [==============================] - 17s 49ms/step - loss: 2.0002 - acc: 0.4616 - top5-acc: 0.7752 - val_loss: 2.1840 - val_acc: 0.4276 - val_top5-acc: 0.7332\n",
            "Epoch 39/50\n",
            "352/352 [==============================] - 17s 49ms/step - loss: 1.9857 - acc: 0.4643 - top5-acc: 0.7780 - val_loss: 2.1631 - val_acc: 0.4246 - val_top5-acc: 0.7472\n",
            "Epoch 40/50\n",
            "352/352 [==============================] - 17s 50ms/step - loss: 1.9763 - acc: 0.4660 - top5-acc: 0.7797 - val_loss: 2.1282 - val_acc: 0.4404 - val_top5-acc: 0.7470\n",
            "Epoch 41/50\n",
            "352/352 [==============================] - 17s 50ms/step - loss: 1.9640 - acc: 0.4654 - top5-acc: 0.7835 - val_loss: 2.1126 - val_acc: 0.4466 - val_top5-acc: 0.7562\n",
            "Epoch 42/50\n",
            "352/352 [==============================] - 18s 50ms/step - loss: 1.9571 - acc: 0.4686 - top5-acc: 0.7826 - val_loss: 2.1218 - val_acc: 0.4462 - val_top5-acc: 0.7556\n",
            "Epoch 43/50\n",
            "352/352 [==============================] - 18s 50ms/step - loss: 1.9578 - acc: 0.4686 - top5-acc: 0.7858 - val_loss: 2.2114 - val_acc: 0.4266 - val_top5-acc: 0.7304\n",
            "Epoch 44/50\n",
            "352/352 [==============================] - 17s 49ms/step - loss: 1.9397 - acc: 0.4752 - top5-acc: 0.7870 - val_loss: 2.1603 - val_acc: 0.4354 - val_top5-acc: 0.7440\n",
            "Epoch 45/50\n",
            "352/352 [==============================] - 17s 49ms/step - loss: 1.9337 - acc: 0.4737 - top5-acc: 0.7901 - val_loss: 2.1556 - val_acc: 0.4428 - val_top5-acc: 0.7400\n",
            "Epoch 46/50\n",
            "352/352 [==============================] - 17s 49ms/step - loss: 1.9262 - acc: 0.4759 - top5-acc: 0.7918 - val_loss: 2.2131 - val_acc: 0.4228 - val_top5-acc: 0.7320\n",
            "Epoch 47/50\n",
            "352/352 [==============================] - 17s 49ms/step - loss: 1.7966 - acc: 0.5064 - top5-acc: 0.8176 - val_loss: 2.0303 - val_acc: 0.4598 - val_top5-acc: 0.7672\n",
            "Epoch 48/50\n",
            "352/352 [==============================] - 17s 49ms/step - loss: 1.7711 - acc: 0.5147 - top5-acc: 0.8203 - val_loss: 2.0618 - val_acc: 0.4546 - val_top5-acc: 0.7628\n",
            "Epoch 49/50\n",
            "352/352 [==============================] - 17s 50ms/step - loss: 1.7753 - acc: 0.5173 - top5-acc: 0.8195 - val_loss: 2.0441 - val_acc: 0.4632 - val_top5-acc: 0.7648\n",
            "Epoch 50/50\n",
            "352/352 [==============================] - 17s 50ms/step - loss: 1.7755 - acc: 0.5154 - top5-acc: 0.8211 - val_loss: 2.0414 - val_acc: 0.4618 - val_top5-acc: 0.7686\n",
            "313/313 [==============================] - 2s 7ms/step - loss: 2.0071 - acc: 0.4720 - top5-acc: 0.7676\n",
            "Test accuracy: 47.2%\n",
            "Test top 5 accuracy: 76.76%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "shtDUoq2DUfY"
      },
      "source": [
        "# The gMLP model\n",
        "\n",
        "class gMLPLayer(layers.Layer):\n",
        "    def __init__(self, num_patches, embedding_dim, dropout_rate, *args, **kwargs):\n",
        "        super(gMLPLayer, self).__init__(*args, **kwargs)\n",
        "\n",
        "        self.channel_projection1 = keras.Sequential(\n",
        "            [\n",
        "                layers.Dense(units=embedding_dim * 2),\n",
        "                tfa.layers.GELU(),\n",
        "                layers.Dropout(rate=dropout_rate),\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        self.channel_projection2 = layers.Dense(units=embedding_dim)\n",
        "\n",
        "        self.spatial_projection = layers.Dense(\n",
        "            units=num_patches, bias_initializer=\"Ones\"\n",
        "        )\n",
        "\n",
        "        self.normalize1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.normalize2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "    def spatial_gating_unit(self, x):\n",
        "        # Split x along the channel dimensions.\n",
        "        # Tensors u and v will in th shape of [batch_size, num_patchs, embedding_dim].\n",
        "        u, v = tf.split(x, num_or_size_splits=2, axis=2)\n",
        "        # Apply layer normalization.\n",
        "        v = self.normalize2(v)\n",
        "        # Apply spatial projection.\n",
        "        v_channels = tf.linalg.matrix_transpose(v)\n",
        "        v_projected = self.spatial_projection(v_channels)\n",
        "        v_projected = tf.linalg.matrix_transpose(v_projected)\n",
        "        # Apply element-wise multiplication.\n",
        "        return u * v_projected\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # Apply layer normalization.\n",
        "        x = self.normalize1(inputs)\n",
        "        # Apply the first channel projection. x_projected shape: [batch_size, num_patches, embedding_dim * 2].\n",
        "        x_projected = self.channel_projection1(x)\n",
        "        # Apply the spatial gating unit. x_spatial shape: [batch_size, num_patches, embedding_dim].\n",
        "        x_spatial = self.spatial_gating_unit(x_projected)\n",
        "        # Apply the second channel projection. x_projected shape: [batch_size, num_patches, embedding_dim].\n",
        "        x_projected = self.channel_projection2(x_spatial)\n",
        "        # Add skip connection.\n",
        "        return x + x_projected"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uB9hay_ADYkA",
        "outputId": "343673f4-e5ea-41ec-f640-e0812bf29586"
      },
      "source": [
        "gmlp_blocks = keras.Sequential(\n",
        "    [gMLPLayer(num_patches, embedding_dim, dropout_rate) for _ in range(num_blocks)]\n",
        ")\n",
        "learning_rate = 0.003\n",
        "gmlp_classifier = build_classifier(gmlp_blocks)\n",
        "history = run_experiment(gmlp_classifier)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "352/352 [==============================] - 25s 63ms/step - loss: 3.9118 - acc: 0.0998 - top5-acc: 0.2982 - val_loss: 3.4990 - val_acc: 0.1682 - val_top5-acc: 0.4254\n",
            "Epoch 2/50\n",
            "352/352 [==============================] - 22s 61ms/step - loss: 3.4505 - acc: 0.1718 - top5-acc: 0.4342 - val_loss: 3.2347 - val_acc: 0.2120 - val_top5-acc: 0.4942\n",
            "Epoch 3/50\n",
            "352/352 [==============================] - 22s 61ms/step - loss: 3.2311 - acc: 0.2116 - top5-acc: 0.4941 - val_loss: 3.0380 - val_acc: 0.2472 - val_top5-acc: 0.5462\n",
            "Epoch 4/50\n",
            "352/352 [==============================] - 21s 61ms/step - loss: 3.0513 - acc: 0.2404 - top5-acc: 0.5419 - val_loss: 3.0092 - val_acc: 0.2550 - val_top5-acc: 0.5666\n",
            "Epoch 5/50\n",
            "352/352 [==============================] - 21s 61ms/step - loss: 2.9230 - acc: 0.2697 - top5-acc: 0.5736 - val_loss: 2.8942 - val_acc: 0.2842 - val_top5-acc: 0.5878\n",
            "Epoch 6/50\n",
            "352/352 [==============================] - 21s 61ms/step - loss: 2.8184 - acc: 0.2927 - top5-acc: 0.5988 - val_loss: 2.7998 - val_acc: 0.3042 - val_top5-acc: 0.6160\n",
            "Epoch 7/50\n",
            "352/352 [==============================] - 21s 61ms/step - loss: 2.7483 - acc: 0.3010 - top5-acc: 0.6182 - val_loss: 2.6821 - val_acc: 0.3274 - val_top5-acc: 0.6384\n",
            "Epoch 8/50\n",
            "352/352 [==============================] - 22s 61ms/step - loss: 2.6714 - acc: 0.3194 - top5-acc: 0.6348 - val_loss: 2.6561 - val_acc: 0.3370 - val_top5-acc: 0.6414\n",
            "Epoch 9/50\n",
            "352/352 [==============================] - 21s 61ms/step - loss: 2.6050 - acc: 0.3354 - top5-acc: 0.6498 - val_loss: 2.6091 - val_acc: 0.3402 - val_top5-acc: 0.6544\n",
            "Epoch 10/50\n",
            "352/352 [==============================] - 22s 61ms/step - loss: 2.5352 - acc: 0.3489 - top5-acc: 0.6650 - val_loss: 2.5250 - val_acc: 0.3632 - val_top5-acc: 0.6666\n",
            "Epoch 11/50\n",
            "352/352 [==============================] - 21s 61ms/step - loss: 2.4624 - acc: 0.3639 - top5-acc: 0.6798 - val_loss: 2.4558 - val_acc: 0.3750 - val_top5-acc: 0.6854\n",
            "Epoch 12/50\n",
            "352/352 [==============================] - 22s 61ms/step - loss: 2.4109 - acc: 0.3717 - top5-acc: 0.6911 - val_loss: 2.4640 - val_acc: 0.3690 - val_top5-acc: 0.6868\n",
            "Epoch 13/50\n",
            "352/352 [==============================] - 22s 61ms/step - loss: 2.3558 - acc: 0.3859 - top5-acc: 0.7022 - val_loss: 2.3804 - val_acc: 0.3966 - val_top5-acc: 0.7076\n",
            "Epoch 14/50\n",
            "352/352 [==============================] - 22s 61ms/step - loss: 2.3128 - acc: 0.3928 - top5-acc: 0.7100 - val_loss: 2.3811 - val_acc: 0.3906 - val_top5-acc: 0.6998\n",
            "Epoch 15/50\n",
            "352/352 [==============================] - 22s 61ms/step - loss: 2.2884 - acc: 0.3991 - top5-acc: 0.7172 - val_loss: 2.3326 - val_acc: 0.4054 - val_top5-acc: 0.7172\n",
            "Epoch 16/50\n",
            "352/352 [==============================] - 22s 61ms/step - loss: 2.2588 - acc: 0.4075 - top5-acc: 0.7211 - val_loss: 2.3296 - val_acc: 0.3998 - val_top5-acc: 0.7206\n",
            "Epoch 17/50\n",
            "352/352 [==============================] - 21s 61ms/step - loss: 2.2352 - acc: 0.4107 - top5-acc: 0.7277 - val_loss: 2.3511 - val_acc: 0.4020 - val_top5-acc: 0.7098\n",
            "Epoch 18/50\n",
            "352/352 [==============================] - 22s 61ms/step - loss: 2.1975 - acc: 0.4194 - top5-acc: 0.7336 - val_loss: 2.3551 - val_acc: 0.4028 - val_top5-acc: 0.7128\n",
            "Epoch 19/50\n",
            "352/352 [==============================] - 22s 61ms/step - loss: 2.1789 - acc: 0.4230 - top5-acc: 0.7412 - val_loss: 2.3316 - val_acc: 0.4070 - val_top5-acc: 0.7162\n",
            "Epoch 20/50\n",
            "352/352 [==============================] - 22s 61ms/step - loss: 2.1600 - acc: 0.4261 - top5-acc: 0.7466 - val_loss: 2.2683 - val_acc: 0.4230 - val_top5-acc: 0.7264\n",
            "Epoch 21/50\n",
            "352/352 [==============================] - 22s 61ms/step - loss: 2.1272 - acc: 0.4350 - top5-acc: 0.7491 - val_loss: 2.2563 - val_acc: 0.4252 - val_top5-acc: 0.7238\n",
            "Epoch 22/50\n",
            "352/352 [==============================] - 22s 61ms/step - loss: 2.1188 - acc: 0.4343 - top5-acc: 0.7517 - val_loss: 2.3124 - val_acc: 0.4122 - val_top5-acc: 0.7250\n",
            "Epoch 23/50\n",
            "352/352 [==============================] - 21s 61ms/step - loss: 2.0991 - acc: 0.4397 - top5-acc: 0.7560 - val_loss: 2.2271 - val_acc: 0.4282 - val_top5-acc: 0.7330\n",
            "Epoch 24/50\n",
            "352/352 [==============================] - 22s 61ms/step - loss: 2.0789 - acc: 0.4454 - top5-acc: 0.7586 - val_loss: 2.2625 - val_acc: 0.4226 - val_top5-acc: 0.7286\n",
            "Epoch 25/50\n",
            "352/352 [==============================] - 22s 61ms/step - loss: 2.0555 - acc: 0.4490 - top5-acc: 0.7669 - val_loss: 2.2305 - val_acc: 0.4318 - val_top5-acc: 0.7370\n",
            "Epoch 26/50\n",
            "352/352 [==============================] - 21s 61ms/step - loss: 2.0522 - acc: 0.4511 - top5-acc: 0.7671 - val_loss: 2.2291 - val_acc: 0.4326 - val_top5-acc: 0.7386\n",
            "Epoch 27/50\n",
            "352/352 [==============================] - 22s 61ms/step - loss: 2.0324 - acc: 0.4555 - top5-acc: 0.7668 - val_loss: 2.2383 - val_acc: 0.4358 - val_top5-acc: 0.7342\n",
            "Epoch 28/50\n",
            "352/352 [==============================] - 22s 61ms/step - loss: 2.0191 - acc: 0.4575 - top5-acc: 0.7717 - val_loss: 2.2645 - val_acc: 0.4326 - val_top5-acc: 0.7338\n",
            "Epoch 29/50\n",
            "352/352 [==============================] - 22s 61ms/step - loss: 1.8308 - acc: 0.5021 - top5-acc: 0.8052 - val_loss: 2.2205 - val_acc: 0.4478 - val_top5-acc: 0.7458\n",
            "Epoch 30/50\n",
            "352/352 [==============================] - 21s 61ms/step - loss: 1.7860 - acc: 0.5105 - top5-acc: 0.8132 - val_loss: 2.1400 - val_acc: 0.4574 - val_top5-acc: 0.7524\n",
            "Epoch 31/50\n",
            "352/352 [==============================] - 22s 61ms/step - loss: 1.7675 - acc: 0.5146 - top5-acc: 0.8166 - val_loss: 2.1944 - val_acc: 0.4584 - val_top5-acc: 0.7526\n",
            "Epoch 32/50\n",
            "352/352 [==============================] - 22s 61ms/step - loss: 1.7355 - acc: 0.5222 - top5-acc: 0.8225 - val_loss: 2.1829 - val_acc: 0.4622 - val_top5-acc: 0.7446\n",
            "Epoch 33/50\n",
            "352/352 [==============================] - 22s 61ms/step - loss: 1.7253 - acc: 0.5239 - top5-acc: 0.8243 - val_loss: 2.1698 - val_acc: 0.4656 - val_top5-acc: 0.7542\n",
            "Epoch 34/50\n",
            "352/352 [==============================] - 22s 61ms/step - loss: 1.7174 - acc: 0.5235 - top5-acc: 0.8275 - val_loss: 2.2134 - val_acc: 0.4622 - val_top5-acc: 0.7556\n",
            "Epoch 35/50\n",
            "352/352 [==============================] - 22s 61ms/step - loss: 1.7042 - acc: 0.5262 - top5-acc: 0.8292 - val_loss: 2.2381 - val_acc: 0.4546 - val_top5-acc: 0.7510\n",
            "Epoch 36/50\n",
            "352/352 [==============================] - 22s 61ms/step - loss: 1.5676 - acc: 0.5634 - top5-acc: 0.8499 - val_loss: 2.1627 - val_acc: 0.4714 - val_top5-acc: 0.7630\n",
            "Epoch 37/50\n",
            "352/352 [==============================] - 22s 61ms/step - loss: 1.5275 - acc: 0.5720 - top5-acc: 0.8572 - val_loss: 2.1215 - val_acc: 0.4750 - val_top5-acc: 0.7674\n",
            "Epoch 38/50\n",
            "352/352 [==============================] - 22s 61ms/step - loss: 1.5098 - acc: 0.5758 - top5-acc: 0.8598 - val_loss: 2.1462 - val_acc: 0.4722 - val_top5-acc: 0.7670\n",
            "Epoch 39/50\n",
            "352/352 [==============================] - 22s 61ms/step - loss: 1.5013 - acc: 0.5790 - top5-acc: 0.8622 - val_loss: 2.1366 - val_acc: 0.4764 - val_top5-acc: 0.7664\n",
            "Epoch 40/50\n",
            "352/352 [==============================] - 22s 61ms/step - loss: 1.4801 - acc: 0.5822 - top5-acc: 0.8675 - val_loss: 2.1258 - val_acc: 0.4784 - val_top5-acc: 0.7676\n",
            "Epoch 41/50\n",
            "352/352 [==============================] - 22s 61ms/step - loss: 1.4826 - acc: 0.5830 - top5-acc: 0.8651 - val_loss: 2.1460 - val_acc: 0.4816 - val_top5-acc: 0.7676\n",
            "Epoch 42/50\n",
            "352/352 [==============================] - 22s 61ms/step - loss: 1.4605 - acc: 0.5900 - top5-acc: 0.8688 - val_loss: 2.1268 - val_acc: 0.4810 - val_top5-acc: 0.7698\n",
            "Epoch 43/50\n",
            "352/352 [==============================] - 22s 61ms/step - loss: 1.3777 - acc: 0.6076 - top5-acc: 0.8838 - val_loss: 2.1193 - val_acc: 0.4782 - val_top5-acc: 0.7730\n",
            "Epoch 44/50\n",
            "352/352 [==============================] - 22s 61ms/step - loss: 1.3600 - acc: 0.6143 - top5-acc: 0.8855 - val_loss: 2.1102 - val_acc: 0.4872 - val_top5-acc: 0.7732\n",
            "Epoch 45/50\n",
            "352/352 [==============================] - 22s 61ms/step - loss: 1.3613 - acc: 0.6135 - top5-acc: 0.8847 - val_loss: 2.1193 - val_acc: 0.4800 - val_top5-acc: 0.7702\n",
            "Epoch 46/50\n",
            "352/352 [==============================] - 21s 61ms/step - loss: 1.3529 - acc: 0.6154 - top5-acc: 0.8861 - val_loss: 2.1349 - val_acc: 0.4832 - val_top5-acc: 0.7698\n",
            "Epoch 47/50\n",
            "352/352 [==============================] - 22s 61ms/step - loss: 1.3521 - acc: 0.6155 - top5-acc: 0.8875 - val_loss: 2.1028 - val_acc: 0.4848 - val_top5-acc: 0.7790\n",
            "Epoch 48/50\n",
            "352/352 [==============================] - 22s 61ms/step - loss: 1.3422 - acc: 0.6174 - top5-acc: 0.8895 - val_loss: 2.1194 - val_acc: 0.4808 - val_top5-acc: 0.7762\n",
            "Epoch 49/50\n",
            "352/352 [==============================] - 22s 61ms/step - loss: 1.3428 - acc: 0.6193 - top5-acc: 0.8871 - val_loss: 2.1324 - val_acc: 0.4840 - val_top5-acc: 0.7736\n",
            "Epoch 50/50\n",
            "352/352 [==============================] - 22s 61ms/step - loss: 1.3331 - acc: 0.6211 - top5-acc: 0.8899 - val_loss: 2.1216 - val_acc: 0.4820 - val_top5-acc: 0.7754\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 2.0792 - acc: 0.4862 - top5-acc: 0.7748\n",
            "Test accuracy: 48.62%\n",
            "Test top 5 accuracy: 77.48%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PyA6AAtbDZpC"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}