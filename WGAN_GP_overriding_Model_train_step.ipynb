{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "WGAN-GP overriding Model.train_step.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOSF/nVc8i9GCXOb9sQHfK0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Lee-Gunju/AI-paper-code-review-for-personal-project/blob/master/WGAN_GP_overriding_Model_train_step.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-JYoRpn5LhVN"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import matplotlib.pyplot as plt\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CxhkNSETmucc",
        "outputId": "4e5341c5-b5c1-40e1-b739-f1e625c7e521"
      },
      "source": [
        "IMG_SHAPE = (28, 28, 1)\n",
        "BATCH_SIZE = 512\n",
        "\n",
        "# Size of the noise vector\n",
        "noise_dim = 128\n",
        "\n",
        "fashion_mnist = keras.datasets.fashion_mnist\n",
        "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
        "print(f\"Number of examples: {len(train_images)}\")\n",
        "print(f\"Shape of the images in the dataset: {train_images.shape[1:]}\")\n",
        "\n",
        "# Reshape each sample to (28, 28, 1) and normalize the pixel values in the [-1, 1] range\n",
        "train_images = train_images.reshape(train_images.shape[0], *IMG_SHAPE).astype(\"float32\")\n",
        "train_images = (train_images - 127.5) / 127.5"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "32768/29515 [=================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26427392/26421880 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "8192/5148 [===============================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4423680/4422102 [==============================] - 0s 0us/step\n",
            "Number of examples: 60000\n",
            "Shape of the images in the dataset: (28, 28)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pmuuT-W7m8Xi",
        "outputId": "b726ba25-c611-405a-edf8-f628d8e2853b"
      },
      "source": [
        "train_images.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 28, 28, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "id": "1TZVuExqnExb",
        "outputId": "0a29627f-2b0e-4793-c0fe-a4a81b50e0f0"
      },
      "source": [
        "plt.imshow(train_images[59999][:,:,0], cmap='gray')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7ff940063e10>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SXbIFK-VnPFy"
      },
      "source": [
        "def conv_block(x, filters, activation, kernel_size= (3,3), strides = (1,1), padding = 'same', use_bias=True, use_bn = False, use_dropout = False, drop_value = 0.5):\n",
        "  x = layers.Conv2D(filters, kernel_size, strides=strides, padding=padding, use_bias=use_bias)(x)\n",
        "  if use_bn:\n",
        "    x = layers.BatchNormalization()(x)\n",
        "  x = activation(x)\n",
        "  if use_dropout:\n",
        "    x = layers.Dropout(drop_value)(x)\n",
        "  return x \n",
        "\n",
        "def get_discriminator_model():\n",
        "  img_input = layers.Input(shape = IMG_SHAPE)\n",
        "  # Zero pad the input to make the input images size to (32, 32, 1).\n",
        "  x = layers.ZeroPadding2D((2,2))(img_input)\n",
        "  x = conv_block(x, 64, kernel_size = (5,5), strides = (2,2), use_bn = False, use_bias = True, activation=layers.LeakyReLU(0.2), use_dropout=False, drop_value=0.3)\n",
        "  x = conv_block(\n",
        "        x,\n",
        "        128,\n",
        "        kernel_size=(5, 5),\n",
        "        strides=(2, 2),\n",
        "        use_bn=False,\n",
        "        activation=layers.LeakyReLU(0.2),\n",
        "        use_bias=True,\n",
        "        use_dropout=True,\n",
        "        drop_value=0.3,\n",
        "    )\n",
        "  x = conv_block(\n",
        "      x,\n",
        "      256,\n",
        "      kernel_size=(5, 5),\n",
        "      strides=(2, 2),\n",
        "      use_bn=False,\n",
        "      activation=layers.LeakyReLU(0.2),\n",
        "      use_bias=True,\n",
        "      use_dropout=True,\n",
        "      drop_value=0.3,\n",
        "  )\n",
        "  x = conv_block(\n",
        "      x,\n",
        "      512,\n",
        "      kernel_size=(5, 5),\n",
        "      strides=(2, 2),\n",
        "      use_bn=False,\n",
        "      activation=layers.LeakyReLU(0.2),\n",
        "      use_bias=True,\n",
        "      use_dropout=False,\n",
        "      drop_value=0.3,\n",
        "  )\n",
        "\n",
        "  x = layers.Flatten()(x)\n",
        "  x = layers.Dropout(0.2)(x)\n",
        "  x = layers.Dense(1)(x)\n",
        "\n",
        "  d_model = keras.models.Model(img_input, x, name = 'discriminator')\n",
        "  return d_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MB4dM4PcrOkO",
        "outputId": "44a994de-f15d-4756-9f2c-fa3af559e9d7"
      },
      "source": [
        "d_model = get_discriminator_model()\n",
        "d_model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"discriminator\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_3 (InputLayer)         [(None, 28, 28, 1)]       0         \n",
            "_________________________________________________________________\n",
            "zero_padding2d_2 (ZeroPaddin (None, 32, 32, 1)         0         \n",
            "_________________________________________________________________\n",
            "conv2d (Conv2D)              (None, 16, 16, 64)        1664      \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_2 (LeakyReLU)    (None, 16, 16, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 8, 8, 128)         204928    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_3 (LeakyReLU)    (None, 8, 8, 128)         0         \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 8, 8, 128)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 4, 4, 256)         819456    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_4 (LeakyReLU)    (None, 4, 4, 256)         0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 4, 4, 256)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 2, 2, 512)         3277312   \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_5 (LeakyReLU)    (None, 2, 2, 512)         0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 2048)              0         \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 2048)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 1)                 2049      \n",
            "=================================================================\n",
            "Total params: 4,305,409\n",
            "Trainable params: 4,305,409\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tyLtEu2BrVeU",
        "outputId": "0e7e9a9c-9336-4f4f-d9a1-aa0e200abb7c"
      },
      "source": [
        "def upsample_block(x, filters, activation, kernel_size = (3,3), strides = (1,1), up_size = (2,2), padding ='same', use_bn = False, use_bias = True, use_dropout = False, drop_value = 0.3):\n",
        "  x = layers.UpSampling2D(up_size)(x)\n",
        "  x = layers.Conv2D(filters, kernel_size, strides=strides, padding = padding, use_bias = use_bias)(x)\n",
        "  if use_bn:\n",
        "    x = layers.BatchNormalization()(x)\n",
        "  if activation:\n",
        "    x = activation(x)\n",
        "  if use_dropout:\n",
        "    x = layers.Dropout(drop_value)(x)\n",
        "  return x \n",
        "\n",
        "\n",
        "def get_generator_model():\n",
        "  noise = layers.Input(shape = (noise_dim,))\n",
        "  x = layers.Dense(4 * 4 * 256, use_bias=False)(noise)\n",
        "  x = layers.BatchNormalization()(x)\n",
        "  x = layers.LeakyReLU(0.2)(x)\n",
        "\n",
        "  x = layers.Reshape((4, 4, 256))(x)\n",
        "  x = upsample_block(\n",
        "        x,\n",
        "        128,\n",
        "        layers.LeakyReLU(0.2),\n",
        "        strides=(1, 1),\n",
        "        use_bias=False,\n",
        "        use_bn=True,\n",
        "        padding=\"same\",\n",
        "        use_dropout=False,\n",
        "    )\n",
        "  x = upsample_block(\n",
        "      x,\n",
        "      64,\n",
        "      layers.LeakyReLU(0.2),\n",
        "      strides=(1, 1),\n",
        "      use_bias=False,\n",
        "      use_bn=True,\n",
        "      padding=\"same\",\n",
        "      use_dropout=False,\n",
        "  )\n",
        "  x = upsample_block(\n",
        "      x, 1, layers.Activation(\"tanh\"), strides=(1, 1), use_bias=False, use_bn=True\n",
        "  )\n",
        "  # At this point, we have an output which has the same shape as the input, (32, 32, 1).\n",
        "  # We will use a Cropping2D layer to make it (28, 28, 1).\n",
        "  x = layers.Cropping2D((2, 2))(x)\n",
        "\n",
        "  g_model = keras.models.Model(noise, x, name=\"generator\")\n",
        "  return g_model\n",
        "\n",
        "\n",
        "g_model = get_generator_model()\n",
        "g_model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"generator\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_5 (InputLayer)         [(None, 128)]             0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 4096)              524288    \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 4096)              16384     \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_8 (LeakyReLU)    (None, 4096)              0         \n",
            "_________________________________________________________________\n",
            "reshape_1 (Reshape)          (None, 4, 4, 256)         0         \n",
            "_________________________________________________________________\n",
            "up_sampling2d_1 (UpSampling2 (None, 8, 8, 256)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, 8, 8, 128)         294912    \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 8, 8, 128)         512       \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_9 (LeakyReLU)    (None, 8, 8, 128)         0         \n",
            "_________________________________________________________________\n",
            "up_sampling2d_2 (UpSampling2 (None, 16, 16, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_6 (Conv2D)            (None, 16, 16, 64)        73728     \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 16, 16, 64)        256       \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_10 (LeakyReLU)   (None, 16, 16, 64)        0         \n",
            "_________________________________________________________________\n",
            "up_sampling2d_3 (UpSampling2 (None, 32, 32, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_7 (Conv2D)            (None, 32, 32, 1)         576       \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 32, 32, 1)         4         \n",
            "_________________________________________________________________\n",
            "activation (Activation)      (None, 32, 32, 1)         0         \n",
            "_________________________________________________________________\n",
            "cropping2d (Cropping2D)      (None, 28, 28, 1)         0         \n",
            "=================================================================\n",
            "Total params: 910,660\n",
            "Trainable params: 902,082\n",
            "Non-trainable params: 8,578\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fBhwYK_htbRf"
      },
      "source": [
        "class WGAN(keras.Model):\n",
        "  def __init__(self, discriminator, generator, latent_dim, discriminator_extra_steps = 3, gp_weight = 10.0):\n",
        "    super(WGAN, self).__init__()\n",
        "    self.discriminator = discriminator\n",
        "    self.generator = generator \n",
        "    self.latent_dim = latent_dim\n",
        "    self.d_steps = discriminator_extra_steps\n",
        "    self.gp_weight = gp_weight\n",
        "\n",
        "  def compile(self, d_optimizer, g_optimizer, d_loss_fn, g_loss_fn):\n",
        "    super(WGAN, self).compile()\n",
        "    self.d_optimizer = d_optimizer \n",
        "    self.g_optimizer = g_optimizer \n",
        "    self.d_loss_fn = d_loss_fn\n",
        "    self.g_loss_fn = g_loss_fn \n",
        "\n",
        "  def gradient_penalty(self, batch_size, real_images, fake_images):\n",
        "    \"\"\" Calculates the gradient penalty.\n",
        "\n",
        "    This loss is calculated on an interpolated image\n",
        "    and added to the discriminator loss.\n",
        "    \"\"\"\n",
        "\n",
        "    # Get the interpolated image\n",
        "    alpha = tf.random.normal(shape = [batch_size, 1, ,1 ,1], 0.0, 1.0)\n",
        "    diff = fake_image - real_image \n",
        "    interpolated = real_image + alpha * diff \n",
        "\n",
        "    with tf.GradientTape() as gp_tape:\n",
        "      gp_tape.watch(interpolated)\n",
        "      # 1. Get the discriminator output for this interpolated image.\n",
        "      pred = self.discriminator(interpolated, training = True)\n",
        "    \n",
        "    # 2. Calculate the gradients w.r.t to this interpolated image.\n",
        "    grads = gp_tape.gradient(pred, [interploated])[0]\n",
        "    # 3. Calculate the norm of the gradients.\n",
        "    norm = tf.sqrt(tf.reduce_sum(tf.square(grads), axis = [1,2,3]))\n",
        "    gp = tf.reduce_mean((norm - 1.0) **2)\n",
        "    return gp \n",
        "\n",
        "  def train_step(self, real_images):\n",
        "    if isinstance(real_images, tuple):\n",
        "      real_images = real_images[0]\n",
        "\n",
        "    batch_size = tf.shape(real_images)[0]\n",
        "    # For each batch, we are going to perform the\n",
        "    # following steps as laid out in the original paper:\n",
        "    # 1. Train the generator and get the generator loss\n",
        "    # 2. Train the discriminator and get the discriminator loss\n",
        "    # 3. Calculate the gradient penalty\n",
        "    # 4. Multiply this gradient penalty with a constant weight factor\n",
        "    # 5. Add the gradient penalty to the discriminator loss\n",
        "    # 6. Return the generator and discriminator losses as a loss dictionary\n",
        "\n",
        "    # Train the discriminator first. The original paper recommends training\n",
        "    # the discriminator for `x` more steps (typically 5) as compared to\n",
        "    # one step of the generator. Here we will train it\n",
        "\n",
        "    for i in range(self.d_steps):\n",
        "      random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
        "\n",
        "      with tf.GradientTape() as tape:\n",
        "        fake_images = self.generator(random_latent_vectors, training = True)\n",
        "\n",
        "        fake_logits = self.discriminator(fake_images, training = True)\n",
        "\n",
        "        real_logits = self.discriminator(real_images, training = True)\n",
        "\n",
        "        d_cost = self.d_loss_fn(real_img = real_logits, fake_img = fake_logits)\n",
        "\n",
        "        gp = self.gradient_penalty(batch_size, real_images, fake_images)\n",
        "\n",
        "        d_loss = d_cost + gp * self.gp_weight \n",
        "\n",
        "      d_gradient = tape.gradient(d_loss, self.discriminator.trainable_variables)\n",
        "      self.d_optimizer.apply_gradients(zip(d_gradient, self.discriminator.trainable_variables))\n",
        "\n",
        "\n",
        "    # Train the generator\n",
        "    random_latent_vectors = tf.random.normal(shape= (batch_size, self.latent_dim))\n",
        "    with tf\n",
        "\n",
        "\n",
        "\n",
        "  \n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}