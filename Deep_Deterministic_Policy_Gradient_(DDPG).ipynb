{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Deep Deterministic Policy Gradient (DDPG).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyM4wK+5REHmzFF7C8/onI0k",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Lee-Gunju/AI-paper-code-review-for-personal-project/blob/master/Deep_Deterministic_Policy_Gradient_(DDPG).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ACZQ4jcuELGc"
      },
      "source": [
        "import gym\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h0y91SOjFfV_"
      },
      "source": [
        "problem = \"Pendulum-v0\"\n",
        "env = gym.make(problem)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c8gHcTIIFqwc",
        "outputId": "10177455-378b-4d7b-8caa-444396eb8cc8"
      },
      "source": [
        "problem = \"Pendulum-v0\"\n",
        "env = gym.make(problem)\n",
        "\n",
        "num_states = env.observation_space.shape[0]\n",
        "print(\"Size of State Space ->  {}\".format(num_states))\n",
        "num_actions = env.action_space.shape[0]\n",
        "print(\"Size of Action Space ->  {}\".format(num_actions))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Size of State Space ->  3\n",
            "Size of Action Space ->  1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R8VLpGbbFtp8",
        "outputId": "bf399cd9-26dd-487e-fc6f-0eec0ab20693"
      },
      "source": [
        "upper_bound = env.action_space.high[0]\n",
        "lower_bound = env.action_space.low[0]\n",
        "\n",
        "print(\"Max Value of Action ->  {}\".format(upper_bound))\n",
        "print(\"Min Value of Action ->  {}\".format(lower_bound))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Max Value of Action ->  2.0\n",
            "Min Value of Action ->  -2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zCv9n1UmGAQE"
      },
      "source": [
        "class OUActionNoise:\n",
        "  def __init__(self, mean, std_deviation, theta = 0.15, dt = 1e-2, x_initial = None):\n",
        "    self.theta = theta\n",
        "    self.mean = mean\n",
        "    self.std_dev = std_deviation\n",
        "    self.dt = dt\n",
        "    self.x_initial = x_initial\n",
        "    self.reset()\n",
        "\n",
        "  def __call__(self):\n",
        "    x = (self.x_prev + self.theta * (self.mean - self.x_prev) * self.dt + self.std_dev * np.sqrt(self.dt) * np.random.normal(size = self.mean.shape))\n",
        "    self.x_prev = x \n",
        "    return x \n",
        "\n",
        "  def reset(self):\n",
        "        if self.x_initial is not None:\n",
        "            self.x_prev = self.x_initial\n",
        "        else:\n",
        "            self.x_prev = np.zeros_like(self.mean)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kP6vTQOMHoy2"
      },
      "source": [
        "class Buffer:\n",
        "    def __init__(self, buffer_capacity=100000, batch_size=64):\n",
        "        # Number of \"experiences\" to store at max\n",
        "        self.buffer_capacity = buffer_capacity\n",
        "        # Num of tuples to train on.\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "        # Its tells us num of times record() was called.\n",
        "        self.buffer_counter = 0\n",
        "\n",
        "        # Instead of list of tuples as the exp.replay concept go\n",
        "        # We use different np.arrays for each tuple element\n",
        "        self.state_buffer = np.zeros((self.buffer_capacity, num_states))\n",
        "        self.action_buffer = np.zeros((self.buffer_capacity, num_actions))\n",
        "        self.reward_buffer = np.zeros((self.buffer_capacity, 1))\n",
        "        self.next_state_buffer = np.zeros((self.buffer_capacity, num_states))\n",
        "\n",
        "    # Takes (s,a,r,s') obervation tuple as input\n",
        "    def record(self, obs_tuple):\n",
        "        # Set index to zero if buffer_capacity is exceeded,\n",
        "        # replacing old records\n",
        "        index = self.buffer_counter % self.buffer_capacity\n",
        "\n",
        "        self.state_buffer[index] = obs_tuple[0]\n",
        "        self.action_buffer[index] = obs_tuple[1]\n",
        "        self.reward_buffer[index] = obs_tuple[2]\n",
        "        self.next_state_buffer[index] = obs_tuple[3]\n",
        "\n",
        "        self.buffer_counter += 1\n",
        "\n",
        "    # Eager execution is turned on by default in TensorFlow 2. Decorating with tf.function allows\n",
        "    # TensorFlow to build a static graph out of the logic and computations in our function.\n",
        "    # This provides a large speed up for blocks of code that contain many small TensorFlow operations such as this one.\n",
        "    @tf.function\n",
        "    def update(\n",
        "        self, state_batch, action_batch, reward_batch, next_state_batch,\n",
        "    ):\n",
        "        # Training and updating Actor & Critic networks.\n",
        "        # See Pseudo Code.\n",
        "        with tf.GradientTape() as tape:\n",
        "            target_actions = target_actor(next_state_batch, training=True)\n",
        "            y = reward_batch + gamma * target_critic(\n",
        "                [next_state_batch, target_actions], training=True\n",
        "            )\n",
        "            critic_value = critic_model([state_batch, action_batch], training=True)\n",
        "            critic_loss = tf.math.reduce_mean(tf.math.square(y - critic_value))\n",
        "\n",
        "        critic_grad = tape.gradient(critic_loss, critic_model.trainable_variables)\n",
        "        critic_optimizer.apply_gradients(\n",
        "            zip(critic_grad, critic_model.trainable_variables)\n",
        "        )\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            actions = actor_model(state_batch, training=True)\n",
        "            critic_value = critic_model([state_batch, actions], training=True)\n",
        "            # Used `-value` as we want to maximize the value given\n",
        "            # by the critic for our actions\n",
        "            actor_loss = -tf.math.reduce_mean(critic_value)\n",
        "\n",
        "        actor_grad = tape.gradient(actor_loss, actor_model.trainable_variables)\n",
        "        actor_optimizer.apply_gradients(\n",
        "            zip(actor_grad, actor_model.trainable_variables)\n",
        "        )\n",
        "\n",
        "    # We compute the loss and update parameters\n",
        "    def learn(self):\n",
        "        # Get sampling range\n",
        "        record_range = min(self.buffer_counter, self.buffer_capacity)\n",
        "        # Randomly sample indices\n",
        "        batch_indices = np.random.choice(record_range, self.batch_size)\n",
        "\n",
        "        # Convert to tensors\n",
        "        state_batch = tf.convert_to_tensor(self.state_buffer[batch_indices])\n",
        "        action_batch = tf.convert_to_tensor(self.action_buffer[batch_indices])\n",
        "        reward_batch = tf.convert_to_tensor(self.reward_buffer[batch_indices])\n",
        "        reward_batch = tf.cast(reward_batch, dtype=tf.float32)\n",
        "        next_state_batch = tf.convert_to_tensor(self.next_state_buffer[batch_indices])\n",
        "\n",
        "        self.update(state_batch, action_batch, reward_batch, next_state_batch)\n",
        "\n",
        "\n",
        "# This update target parameters slowly\n",
        "# Based on rate `tau`, which is much less than one.\n",
        "@tf.function\n",
        "def update_target(target_weights, weights, tau):\n",
        "    for (a, b) in zip(target_weights, weights):\n",
        "        a.assign(b * tau + a * (1 - tau))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HYGP6L2KLOcD"
      },
      "source": [
        "def get_actor():\n",
        "    # Initialize weights between -3e-3 and 3-e3\n",
        "    last_init = tf.random_uniform_initializer(minval=-0.003, maxval=0.003)\n",
        "\n",
        "    inputs = layers.Input(shape=(num_states,))\n",
        "    out = layers.Dense(256, activation=\"relu\")(inputs)\n",
        "    out = layers.Dense(256, activation=\"relu\")(out)\n",
        "    outputs = layers.Dense(1, activation=\"tanh\", kernel_initializer=last_init)(out)\n",
        "\n",
        "    # Our upper bound is 2.0 for Pendulum.\n",
        "    outputs = outputs * upper_bound\n",
        "    model = tf.keras.Model(inputs, outputs)\n",
        "    return model\n",
        "\n",
        "\n",
        "def get_critic():\n",
        "    # State as input\n",
        "    state_input = layers.Input(shape=(num_states))\n",
        "    state_out = layers.Dense(16, activation=\"relu\")(state_input)\n",
        "    state_out = layers.Dense(32, activation=\"relu\")(state_out)\n",
        "\n",
        "    # Action as input\n",
        "    action_input = layers.Input(shape=(num_actions))\n",
        "    action_out = layers.Dense(32, activation=\"relu\")(action_input)\n",
        "\n",
        "    # Both are passed through seperate layer before concatenating\n",
        "    concat = layers.Concatenate()([state_out, action_out])\n",
        "\n",
        "    out = layers.Dense(256, activation=\"relu\")(concat)\n",
        "    out = layers.Dense(256, activation=\"relu\")(out)\n",
        "    outputs = layers.Dense(1)(out)\n",
        "\n",
        "    # Outputs single value for give state-action\n",
        "    model = tf.keras.Model([state_input, action_input], outputs)\n",
        "\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K6ZktxFMMf_T"
      },
      "source": [
        "def policy(state, noise_object):\n",
        "    sampled_actions = tf.squeeze(actor_model(state))\n",
        "    noise = noise_object()\n",
        "    # Adding noise to action\n",
        "    sampled_actions = sampled_actions.numpy() + noise\n",
        "\n",
        "    # We make sure action is within bounds\n",
        "    legal_action = np.clip(sampled_actions, lower_bound, upper_bound)\n",
        "\n",
        "    return [np.squeeze(legal_action)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "97_Bo_h9Mr6L"
      },
      "source": [
        "std_dev = 0.2\n",
        "\n",
        "ou_noise = OUActionNoise(mean=np.zeros(1), std_deviation=float(std_dev) * np.ones(1))\n",
        "\n",
        "actor_model = get_actor()\n",
        "critic_model = get_critic()\n",
        "\n",
        "target_actor = get_actor()\n",
        "target_critic = get_critic()\n",
        "\n",
        "# Making the weights equal initially\n",
        "target_actor.set_weights(actor_model.get_weights())\n",
        "target_critic.set_weights(critic_model.get_weights())\n",
        "\n",
        "# Learning rate for actor-critic models\n",
        "critic_lr = 0.002\n",
        "actor_lr = 0.001\n",
        "\n",
        "critic_optimizer = tf.keras.optimizers.Adam(critic_lr)\n",
        "actor_optimizer = tf.keras.optimizers.Adam(actor_lr)\n",
        "\n",
        "total_episodes = 100\n",
        "# Discount factor for future rewards\n",
        "gamma = 0.99\n",
        "# Used to update target networks\n",
        "tau = 0.005\n",
        "\n",
        "buffer = Buffer(50000, 64)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "DNTo7UUxMt6k",
        "outputId": "806018e6-5302-4eab-fd55-3d6abb085bf4"
      },
      "source": [
        "# To store reward history of each episode\n",
        "ep_reward_list = []\n",
        "# To store average reward history of last few episodes\n",
        "avg_reward_list = []\n",
        "\n",
        "# Takes about 4 min to train\n",
        "for ep in range(total_episodes):\n",
        "\n",
        "    prev_state = env.reset()\n",
        "    episodic_reward = 0\n",
        "\n",
        "    while True:\n",
        "        # Uncomment this to see the Actor in action\n",
        "        # But not in a python notebook.\n",
        "        # env.render()\n",
        "\n",
        "        tf_prev_state = tf.expand_dims(tf.convert_to_tensor(prev_state), 0)\n",
        "\n",
        "        action = policy(tf_prev_state, ou_noise)\n",
        "        # Recieve state and reward from environment.\n",
        "        state, reward, done, info = env.step(action)\n",
        "\n",
        "        buffer.record((prev_state, action, reward, state))\n",
        "        episodic_reward += reward\n",
        "\n",
        "        buffer.learn()\n",
        "        update_target(target_actor.variables, actor_model.variables, tau)\n",
        "        update_target(target_critic.variables, critic_model.variables, tau)\n",
        "\n",
        "        # End this episode when `done` is True\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "        prev_state = state\n",
        "\n",
        "    ep_reward_list.append(episodic_reward)\n",
        "\n",
        "    # Mean of last 40 episodes\n",
        "    avg_reward = np.mean(ep_reward_list[-40:])\n",
        "    print(\"Episode * {} * Avg Reward is ==> {}\".format(ep, avg_reward))\n",
        "    avg_reward_list.append(avg_reward)\n",
        "\n",
        "# Plotting graph\n",
        "# Episodes versus Avg. Rewards\n",
        "plt.plot(avg_reward_list)\n",
        "plt.xlabel(\"Episode\")\n",
        "plt.ylabel(\"Avg. Epsiodic Reward\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Episode * 0 * Avg Reward is ==> -1187.7877002125397\n",
            "Episode * 1 * Avg Reward is ==> -1244.4097293781938\n",
            "Episode * 2 * Avg Reward is ==> -1205.1630832913088\n",
            "Episode * 3 * Avg Reward is ==> -1216.289885294449\n",
            "Episode * 4 * Avg Reward is ==> -1295.9880925243524\n",
            "Episode * 5 * Avg Reward is ==> -1317.8407524204467\n",
            "Episode * 6 * Avg Reward is ==> -1337.4200277912678\n",
            "Episode * 7 * Avg Reward is ==> -1365.9456007384488\n",
            "Episode * 8 * Avg Reward is ==> -1383.224120152453\n",
            "Episode * 9 * Avg Reward is ==> -1360.5179950163533\n",
            "Episode * 10 * Avg Reward is ==> -1375.8551452520642\n",
            "Episode * 11 * Avg Reward is ==> -1349.066015920567\n",
            "Episode * 12 * Avg Reward is ==> -1329.5839770577136\n",
            "Episode * 13 * Avg Reward is ==> -1320.6249542117287\n",
            "Episode * 14 * Avg Reward is ==> -1325.6897572515488\n",
            "Episode * 15 * Avg Reward is ==> -1250.9168786810812\n",
            "Episode * 16 * Avg Reward is ==> -1266.4148363953548\n",
            "Episode * 17 * Avg Reward is ==> -1213.5077669598586\n",
            "Episode * 18 * Avg Reward is ==> -1163.3700035404422\n",
            "Episode * 19 * Avg Reward is ==> -1117.5524323385346\n",
            "Episode * 20 * Avg Reward is ==> -1096.4758772283174\n",
            "Episode * 21 * Avg Reward is ==> -1046.7071349221026\n",
            "Episode * 22 * Avg Reward is ==> -1066.1881863926656\n",
            "Episode * 23 * Avg Reward is ==> -1021.7921955648031\n",
            "Episode * 24 * Avg Reward is ==> -980.9406239366731\n",
            "Episode * 25 * Avg Reward is ==> -948.0155292793071\n",
            "Episode * 26 * Avg Reward is ==> -922.533699687331\n",
            "Episode * 27 * Avg Reward is ==> -894.0371221555281\n",
            "Episode * 28 * Avg Reward is ==> -871.9949005460529\n",
            "Episode * 29 * Avg Reward is ==> -846.9152941171249\n",
            "Episode * 30 * Avg Reward is ==> -849.6142794988534\n",
            "Episode * 31 * Avg Reward is ==> -826.9005819103738\n",
            "Episode * 32 * Avg Reward is ==> -805.4489965551381\n",
            "Episode * 33 * Avg Reward is ==> -785.4376341371309\n",
            "Episode * 34 * Avg Reward is ==> -772.0832510831393\n",
            "Episode * 35 * Avg Reward is ==> -754.1488127465582\n",
            "Episode * 36 * Avg Reward is ==> -737.1370011363031\n",
            "Episode * 37 * Avg Reward is ==> -720.9333617995954\n",
            "Episode * 38 * Avg Reward is ==> -705.5803875382945\n",
            "Episode * 39 * Avg Reward is ==> -687.9593296549372\n",
            "Episode * 40 * Avg Reward is ==> -661.2967226869861\n",
            "Episode * 41 * Avg Reward is ==> -628.9111338536218\n",
            "Episode * 42 * Avg Reward is ==> -609.7651584638276\n",
            "Episode * 43 * Avg Reward is ==> -587.256037601437\n",
            "Episode * 44 * Avg Reward is ==> -549.974523516674\n",
            "Episode * 45 * Avg Reward is ==> -520.1268411096631\n",
            "Episode * 46 * Avg Reward is ==> -490.11546481651686\n",
            "Episode * 47 * Avg Reward is ==> -457.2204697061334\n",
            "Episode * 48 * Avg Reward is ==> -428.45154811992523\n",
            "Episode * 49 * Avg Reward is ==> -408.71500617074264\n",
            "Episode * 50 * Avg Reward is ==> -382.83667180400903\n",
            "Episode * 51 * Avg Reward is ==> -365.79204841260037\n",
            "Episode * 52 * Avg Reward is ==> -341.37015763516683\n",
            "Episode * 53 * Avg Reward is ==> -314.374334548883\n",
            "Episode * 54 * Avg Reward is ==> -285.36971233209596\n",
            "Episode * 55 * Avg Reward is ==> -285.22653354400825\n",
            "Episode * 56 * Avg Reward is ==> -250.5576783405292\n",
            "Episode * 57 * Avg Reward is ==> -245.68753390542184\n",
            "Episode * 58 * Avg Reward is ==> -245.09262934668217\n",
            "Episode * 59 * Avg Reward is ==> -241.83720674581713\n",
            "Episode * 60 * Avg Reward is ==> -233.48324055806452\n",
            "Episode * 61 * Avg Reward is ==> -242.22738091404085\n",
            "Episode * 62 * Avg Reward is ==> -208.00406007458332\n",
            "Episode * 63 * Avg Reward is ==> -214.033661860336\n",
            "Episode * 64 * Avg Reward is ==> -216.94795788693017\n",
            "Episode * 65 * Avg Reward is ==> -219.51583460169613\n",
            "Episode * 66 * Avg Reward is ==> -216.21382549523173\n",
            "Episode * 67 * Avg Reward is ==> -219.31621165203842\n",
            "Episode * 68 * Avg Reward is ==> -216.09423164709838\n",
            "Episode * 69 * Avg Reward is ==> -216.2182673708266\n",
            "Episode * 70 * Avg Reward is ==> -195.85109880827378\n",
            "Episode * 71 * Avg Reward is ==> -199.90689250650706\n",
            "Episode * 72 * Avg Reward is ==> -200.04006285396824\n",
            "Episode * 73 * Avg Reward is ==> -202.59071439258605\n",
            "Episode * 74 * Avg Reward is ==> -203.92227075379895\n",
            "Episode * 75 * Avg Reward is ==> -200.77087982450362\n",
            "Episode * 76 * Avg Reward is ==> -203.70650759571063\n",
            "Episode * 77 * Avg Reward is ==> -203.68154710251514\n",
            "Episode * 78 * Avg Reward is ==> -203.5717419258542\n",
            "Episode * 79 * Avg Reward is ==> -206.53215100143439\n",
            "Episode * 80 * Avg Reward is ==> -206.72990382790425\n",
            "Episode * 81 * Avg Reward is ==> -209.74941476717223\n",
            "Episode * 82 * Avg Reward is ==> -203.69906894851488\n",
            "Episode * 83 * Avg Reward is ==> -194.99743861145703\n",
            "Episode * 84 * Avg Reward is ==> -194.78734315983618\n",
            "Episode * 85 * Avg Reward is ==> -192.15362020111172\n",
            "Episode * 86 * Avg Reward is ==> -188.88542020221766\n",
            "Episode * 87 * Avg Reward is ==> -182.6654968436725\n",
            "Episode * 88 * Avg Reward is ==> -179.32637113367258\n",
            "Episode * 89 * Avg Reward is ==> -173.13552181942399\n",
            "Episode * 90 * Avg Reward is ==> -163.88074347158224\n",
            "Episode * 91 * Avg Reward is ==> -157.8155455655898\n",
            "Episode * 92 * Avg Reward is ==> -157.7499951623982\n",
            "Episode * 93 * Avg Reward is ==> -160.5321868537937\n",
            "Episode * 94 * Avg Reward is ==> -160.9809953782313\n",
            "Episode * 95 * Avg Reward is ==> -160.90571439430738\n",
            "Episode * 96 * Avg Reward is ==> -160.8139271184519\n",
            "Episode * 97 * Avg Reward is ==> -163.3986210173503\n",
            "Episode * 98 * Avg Reward is ==> -163.45901346453633\n",
            "Episode * 99 * Avg Reward is ==> -178.50290520282115\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zumIqzqOM7Tk"
      },
      "source": [
        "# Save the weights\n",
        "actor_model.save_weights(\"pendulum_actor.h5\")\n",
        "critic_model.save_weights(\"pendulum_critic.h5\")\n",
        "\n",
        "target_actor.save_weights(\"pendulum_target_actor.h5\")\n",
        "target_critic.save_weights(\"pendulum_target_critic.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oOxy4OO9NF-c"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}