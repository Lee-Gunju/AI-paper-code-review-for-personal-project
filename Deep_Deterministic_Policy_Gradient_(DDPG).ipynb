{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Deep Deterministic Policy Gradient (DDPG).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN35TzI69mQwH4rG5RXB3/F",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Lee-Gunju/AI-paper-code-review-for-personal-project/blob/master/Deep_Deterministic_Policy_Gradient_(DDPG).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ryap7ozlJgIy"
      },
      "source": [
        "import gym\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4c6MbfPtKP7y",
        "outputId": "009235c8-dc22-47e9-885d-96124bd226a6"
      },
      "source": [
        "problem = \"Pendulum-v0\"\n",
        "env = gym.make(problem)\n",
        "\n",
        "num_states = env.observation_space.shape[0]\n",
        "print(\"Size of State Space ->  {}\".format(num_states))\n",
        "num_actions = env.action_space.shape[0]\n",
        "print(\"Size of Action Space ->  {}\".format(num_actions))\n",
        "\n",
        "upper_bound = env.action_space.high[0]\n",
        "lower_bound = env.action_space.low[0]\n",
        "\n",
        "print(\"Max Value of Action ->  {}\".format(upper_bound))\n",
        "print(\"Min Value of Action ->  {}\".format(lower_bound))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Size of State Space ->  3\n",
            "Size of Action Space ->  1\n",
            "Max Value of Action ->  2.0\n",
            "Min Value of Action ->  -2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "23wkFiIuKUhX",
        "outputId": "2044195d-de80-41dc-a2fd-76398b79983b"
      },
      "source": [
        "env.observation_space"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Box(-8.0, 8.0, (3,), float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "66zxAeF6KaYO",
        "outputId": "252a200c-2ab9-4ef2-b775-cec3083641c2"
      },
      "source": [
        "env.action_space"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Box(-2.0, 2.0, (1,), float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yzY3jWhwKna1"
      },
      "source": [
        "class OUActionNoise:\n",
        "  def __init__(self, mean, std_deviation, theta = 0.15, dt=1e-2, x_initial = None):\n",
        "    self.theta = theta\n",
        "    self.mean = mean\n",
        "    self.std_dev = std_deviation\n",
        "    self.dt = dt\n",
        "    self.x_initial = x_initial\n",
        "    self.reset() \n",
        "\n",
        "  def __call__(self):\n",
        "    x = (self.x_prev + self.theta * (self.mean - self.x_prev) * self.dt \n",
        "         + self.std_dev * np.sqrt(self.dt) * np.random.normal(size = self.mean.shape))\n",
        "\n",
        "    self.x_prev = x \n",
        "    return x \n",
        "\n",
        "  def reset(self):\n",
        "    if self.x_initial is not None:\n",
        "      self.x_prev = self.x_initial\n",
        "\n",
        "    else:\n",
        "      self.x_prev = np.zeros_like(self.mean)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zYcexiacPEnF"
      },
      "source": [
        "class Buffer:\n",
        "    def __init__(self, buffer_capacity=100000, batch_size=64):\n",
        "        # Number of \"experiences\" to store at max\n",
        "        self.buffer_capacity = buffer_capacity\n",
        "        # Num of tuples to train on.\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "        # Its tells us num of times record() was called.\n",
        "        self.buffer_counter = 0\n",
        "\n",
        "        # Instead of list of tuples as the exp.replay concept go\n",
        "        # We use different np.arrays for each tuple element\n",
        "        self.state_buffer = np.zeros((self.buffer_capacity, num_states))\n",
        "        self.action_buffer = np.zeros((self.buffer_capacity, num_actions))\n",
        "        self.reward_buffer = np.zeros((self.buffer_capacity, 1))\n",
        "        self.next_state_buffer = np.zeros((self.buffer_capacity, num_states))\n",
        "\n",
        "    # Takes (s,a,r,s') obervation tuple as input\n",
        "    def record(self, obs_tuple):\n",
        "        # Set index to zero if buffer_capacity is exceeded,\n",
        "        # replacing old records\n",
        "        index = self.buffer_counter % self.buffer_capacity\n",
        "\n",
        "        self.state_buffer[index] = obs_tuple[0]\n",
        "        self.action_buffer[index] = obs_tuple[1]\n",
        "        self.reward_buffer[index] = obs_tuple[2]\n",
        "        self.next_state_buffer[index] = obs_tuple[3]\n",
        "\n",
        "        self.buffer_counter += 1\n",
        "\n",
        "    # Eager execution is turned on by default in TensorFlow 2. Decorating with tf.function allows\n",
        "    # TensorFlow to build a static graph out of the logic and computations in our function.\n",
        "    # This provides a large speed up for blocks of code that contain many small TensorFlow operations such as this one.\n",
        "    @tf.function\n",
        "    def update(\n",
        "        self, state_batch, action_batch, reward_batch, next_state_batch,\n",
        "    ):\n",
        "        # Training and updating Actor & Critic networks.\n",
        "        # See Pseudo Code.\n",
        "        with tf.GradientTape() as tape:\n",
        "            target_actions = target_actor(next_state_batch, training=True)\n",
        "            y = reward_batch + gamma * target_critic(\n",
        "                [next_state_batch, target_actions], training=True\n",
        "            )\n",
        "            critic_value = critic_model([state_batch, action_batch], training=True)\n",
        "            critic_loss = tf.math.reduce_mean(tf.math.square(y - critic_value))\n",
        "\n",
        "        critic_grad = tape.gradient(critic_loss, critic_model.trainable_variables)\n",
        "        critic_optimizer.apply_gradients(\n",
        "            zip(critic_grad, critic_model.trainable_variables)\n",
        "        )\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            actions = actor_model(state_batch, training=True)\n",
        "            critic_value = critic_model([state_batch, actions], training=True)\n",
        "            # Used `-value` as we want to maximize the value given\n",
        "            # by the critic for our actions\n",
        "            actor_loss = -tf.math.reduce_mean(critic_value)\n",
        "\n",
        "        actor_grad = tape.gradient(actor_loss, actor_model.trainable_variables)\n",
        "        actor_optimizer.apply_gradients(\n",
        "            zip(actor_grad, actor_model.trainable_variables)\n",
        "        )\n",
        "\n",
        "    # We compute the loss and update parameters\n",
        "    def learn(self):\n",
        "        # Get sampling range\n",
        "        record_range = min(self.buffer_counter, self.buffer_capacity)\n",
        "        # Randomly sample indices\n",
        "        batch_indices = np.random.choice(record_range, self.batch_size)\n",
        "\n",
        "        # Convert to tensors\n",
        "        state_batch = tf.convert_to_tensor(self.state_buffer[batch_indices])\n",
        "        action_batch = tf.convert_to_tensor(self.action_buffer[batch_indices])\n",
        "        reward_batch = tf.convert_to_tensor(self.reward_buffer[batch_indices])\n",
        "        reward_batch = tf.cast(reward_batch, dtype=tf.float32)\n",
        "        next_state_batch = tf.convert_to_tensor(self.next_state_buffer[batch_indices])\n",
        "\n",
        "        self.update(state_batch, action_batch, reward_batch, next_state_batch)\n",
        "\n",
        "\n",
        "# This update target parameters slowly\n",
        "# Based on rate `tau`, which is much less than one.\n",
        "@tf.function\n",
        "def update_target(target_weights, weights, tau):\n",
        "    for (a, b) in zip(target_weights, weights):\n",
        "        a.assign(b * tau + a * (1 - tau))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ILAtrNOiSdxG"
      },
      "source": [
        "def get_actor():\n",
        "    # Initialize weights between -3e-3 and 3-e3\n",
        "    last_init = tf.random_uniform_initializer(minval=-0.003, maxval=0.003)\n",
        "\n",
        "    inputs = layers.Input(shape=(num_states,))\n",
        "    out = layers.Dense(256, activation=\"relu\")(inputs)\n",
        "    out = layers.Dense(256, activation=\"relu\")(out)\n",
        "    outputs = layers.Dense(1, activation=\"tanh\", kernel_initializer=last_init)(out)\n",
        "\n",
        "    # Our upper bound is 2.0 for Pendulum.\n",
        "    outputs = outputs * upper_bound\n",
        "    model = tf.keras.Model(inputs, outputs)\n",
        "    return model\n",
        "\n",
        "\n",
        "def get_critic():\n",
        "    # State as input\n",
        "    state_input = layers.Input(shape=(num_states))\n",
        "    state_out = layers.Dense(16, activation=\"relu\")(state_input)\n",
        "    state_out = layers.Dense(32, activation=\"relu\")(state_out)\n",
        "\n",
        "    # Action as input\n",
        "    action_input = layers.Input(shape=(num_actions))\n",
        "    action_out = layers.Dense(32, activation=\"relu\")(action_input)\n",
        "\n",
        "    # Both are passed through seperate layer before concatenating\n",
        "    concat = layers.Concatenate()([state_out, action_out])\n",
        "\n",
        "    out = layers.Dense(256, activation=\"relu\")(concat)\n",
        "    out = layers.Dense(256, activation=\"relu\")(out)\n",
        "    outputs = layers.Dense(1)(out)\n",
        "\n",
        "    # Outputs single value for give state-action\n",
        "    model = tf.keras.Model([state_input, action_input], outputs)\n",
        "\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0WdX7dUzS1ob"
      },
      "source": [
        "def policy(state, noise_object):\n",
        "    sampled_actions = tf.squeeze(actor_model(state))\n",
        "    noise = noise_object()\n",
        "    # Adding noise to action\n",
        "    sampled_actions = sampled_actions.numpy() + noise\n",
        "\n",
        "    # We make sure action is within bounds\n",
        "    legal_action = np.clip(sampled_actions, lower_bound, upper_bound)\n",
        "\n",
        "    return [np.squeeze(legal_action)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yC2lPXaATqMz"
      },
      "source": [
        "std_dev = 0.2\n",
        "ou_noise = OUActionNoise(mean=np.zeros(1), std_deviation=float(std_dev) * np.ones(1))\n",
        "\n",
        "actor_model = get_actor()\n",
        "critic_model = get_critic()\n",
        "\n",
        "target_actor = get_actor()\n",
        "target_critic = get_critic()\n",
        "\n",
        "# Making the weights equal initially\n",
        "target_actor.set_weights(actor_model.get_weights())\n",
        "target_critic.set_weights(critic_model.get_weights())\n",
        "\n",
        "# Learning rate for actor-critic models\n",
        "critic_lr = 0.002\n",
        "actor_lr = 0.001\n",
        "\n",
        "critic_optimizer = tf.keras.optimizers.Adam(critic_lr)\n",
        "actor_optimizer = tf.keras.optimizers.Adam(actor_lr)\n",
        "\n",
        "total_episodes = 100\n",
        "# Discount factor for future rewards\n",
        "gamma = 0.99\n",
        "# Used to update target networks\n",
        "tau = 0.005\n",
        "\n",
        "buffer = Buffer(50000, 64)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "3qHN4zZtUYTS",
        "outputId": "5f87b499-33b1-4f3e-8eaa-2a34c99af2aa"
      },
      "source": [
        "# To store reward history of each episode\n",
        "ep_reward_list = []\n",
        "# To store average reward history of last few episodes\n",
        "avg_reward_list = []\n",
        "\n",
        "# Takes about 4 min to train\n",
        "for ep in range(total_episodes):\n",
        "\n",
        "    prev_state = env.reset()\n",
        "    episodic_reward = 0\n",
        "\n",
        "    while True:\n",
        "        # Uncomment this to see the Actor in action\n",
        "        # But not in a python notebook.\n",
        "        # env.render()\n",
        "\n",
        "        tf_prev_state = tf.expand_dims(tf.convert_to_tensor(prev_state), 0)\n",
        "\n",
        "        action = policy(tf_prev_state, ou_noise)\n",
        "        # Recieve state and reward from environment.\n",
        "        state, reward, done, info = env.step(action)\n",
        "\n",
        "        buffer.record((prev_state, action, reward, state))\n",
        "        episodic_reward += reward\n",
        "\n",
        "        buffer.learn()\n",
        "        update_target(target_actor.variables, actor_model.variables, tau)\n",
        "        update_target(target_critic.variables, critic_model.variables, tau)\n",
        "\n",
        "        # End this episode when `done` is True\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "        prev_state = state\n",
        "\n",
        "    ep_reward_list.append(episodic_reward)\n",
        "\n",
        "    # Mean of last 40 episodes\n",
        "    avg_reward = np.mean(ep_reward_list[-40:])\n",
        "    print(\"Episode * {} * Avg Reward is ==> {}\".format(ep, avg_reward))\n",
        "    avg_reward_list.append(avg_reward)\n",
        "\n",
        "# Plotting graph\n",
        "# Episodes versus Avg. Rewards\n",
        "plt.plot(avg_reward_list)\n",
        "plt.xlabel(\"Episode\")\n",
        "plt.ylabel(\"Avg. Epsiodic Reward\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Episode * 0 * Avg Reward is ==> -1030.2774320314104\n",
            "Episode * 1 * Avg Reward is ==> -1337.8695046166322\n",
            "Episode * 2 * Avg Reward is ==> -1232.0194096661508\n",
            "Episode * 3 * Avg Reward is ==> -1354.334802135346\n",
            "Episode * 4 * Avg Reward is ==> -1355.3198182319768\n",
            "Episode * 5 * Avg Reward is ==> -1385.3122535051878\n",
            "Episode * 6 * Avg Reward is ==> -1400.535706009603\n",
            "Episode * 7 * Avg Reward is ==> -1397.0667298710914\n",
            "Episode * 8 * Avg Reward is ==> -1364.4205752635849\n",
            "Episode * 9 * Avg Reward is ==> -1316.4190997844285\n",
            "Episode * 10 * Avg Reward is ==> -1335.108116532667\n",
            "Episode * 11 * Avg Reward is ==> -1293.0897003823836\n",
            "Episode * 12 * Avg Reward is ==> -1247.6258203376851\n",
            "Episode * 13 * Avg Reward is ==> -1215.5779898125986\n",
            "Episode * 14 * Avg Reward is ==> -1169.5098696872399\n",
            "Episode * 15 * Avg Reward is ==> -1128.5251602006879\n",
            "Episode * 16 * Avg Reward is ==> -1092.2790436295306\n",
            "Episode * 17 * Avg Reward is ==> -1075.0267593009858\n",
            "Episode * 18 * Avg Reward is ==> -1038.4977170482568\n",
            "Episode * 19 * Avg Reward is ==> -993.0900132436676\n",
            "Episode * 20 * Avg Reward is ==> -1017.0869481361921\n",
            "Episode * 21 * Avg Reward is ==> -976.6377625972243\n",
            "Episode * 22 * Avg Reward is ==> -949.7268462386556\n",
            "Episode * 23 * Avg Reward is ==> -915.6130793974249\n",
            "Episode * 24 * Avg Reward is ==> -879.0302703370049\n",
            "Episode * 25 * Avg Reward is ==> -854.4479408586947\n",
            "Episode * 26 * Avg Reward is ==> -830.9798969727009\n",
            "Episode * 27 * Avg Reward is ==> -809.8852595911858\n",
            "Episode * 28 * Avg Reward is ==> -786.2840644740174\n",
            "Episode * 29 * Avg Reward is ==> -768.2854814182081\n",
            "Episode * 30 * Avg Reward is ==> -755.7152855084611\n",
            "Episode * 31 * Avg Reward is ==> -743.756644817972\n",
            "Episode * 32 * Avg Reward is ==> -728.2608033856122\n",
            "Episode * 33 * Avg Reward is ==> -710.5298906216905\n",
            "Episode * 34 * Avg Reward is ==> -700.9527949431401\n",
            "Episode * 35 * Avg Reward is ==> -684.8210612700622\n",
            "Episode * 36 * Avg Reward is ==> -672.713956232103\n",
            "Episode * 37 * Avg Reward is ==> -655.0707627240254\n",
            "Episode * 38 * Avg Reward is ==> -644.0803848012703\n",
            "Episode * 39 * Avg Reward is ==> -631.1318764300033\n",
            "Episode * 40 * Avg Reward is ==> -611.650757734631\n",
            "Episode * 41 * Avg Reward is ==> -579.0975329258979\n",
            "Episode * 42 * Avg Reward is ==> -556.8045653088159\n",
            "Episode * 43 * Avg Reward is ==> -516.80057265769\n",
            "Episode * 44 * Avg Reward is ==> -485.9416732059607\n",
            "Episode * 45 * Avg Reward is ==> -450.55591940682405\n",
            "Episode * 46 * Avg Reward is ==> -413.3556786004841\n",
            "Episode * 47 * Avg Reward is ==> -382.2541364570223\n",
            "Episode * 48 * Avg Reward is ==> -360.8479407262114\n",
            "Episode * 49 * Avg Reward is ==> -341.8883192157452\n",
            "Episode * 50 * Avg Reward is ==> -306.78430961307566\n",
            "Episode * 51 * Avg Reward is ==> -288.9140524363588\n",
            "Episode * 52 * Avg Reward is ==> -274.5592462932706\n",
            "Episode * 53 * Avg Reward is ==> -262.946718941589\n",
            "Episode * 54 * Avg Reward is ==> -253.10303599015114\n",
            "Episode * 55 * Avg Reward is ==> -243.677455708034\n",
            "Episode * 56 * Avg Reward is ==> -239.64529188429842\n",
            "Episode * 57 * Avg Reward is ==> -234.98623740877377\n",
            "Episode * 58 * Avg Reward is ==> -237.95850370230443\n",
            "Episode * 59 * Avg Reward is ==> -237.88145045351402\n",
            "Episode * 60 * Avg Reward is ==> -208.5841081369399\n",
            "Episode * 61 * Avg Reward is ==> -208.5857985447106\n",
            "Episode * 62 * Avg Reward is ==> -202.84539167006295\n",
            "Episode * 63 * Avg Reward is ==> -208.936343503231\n",
            "Episode * 64 * Avg Reward is ==> -221.0040042746963\n",
            "Episode * 65 * Avg Reward is ==> -224.37846188852527\n",
            "Episode * 66 * Avg Reward is ==> -224.9690425628418\n",
            "Episode * 67 * Avg Reward is ==> -222.08280161554484\n",
            "Episode * 68 * Avg Reward is ==> -219.05471129235065\n",
            "Episode * 69 * Avg Reward is ==> -219.1415528801921\n",
            "Episode * 70 * Avg Reward is ==> -212.77387612505635\n",
            "Episode * 71 * Avg Reward is ==> -206.4272185759625\n",
            "Episode * 72 * Avg Reward is ==> -206.5904940424547\n",
            "Episode * 73 * Avg Reward is ==> -206.68951889068975\n",
            "Episode * 74 * Avg Reward is ==> -200.5399078634812\n",
            "Episode * 75 * Avg Reward is ==> -203.73108956444582\n",
            "Episode * 76 * Avg Reward is ==> -207.267971919127\n",
            "Episode * 77 * Avg Reward is ==> -217.96904695176573\n",
            "Episode * 78 * Avg Reward is ==> -228.65109588744727\n",
            "Episode * 79 * Avg Reward is ==> -240.87186075635555\n",
            "Episode * 80 * Avg Reward is ==> -249.51402048978358\n",
            "Episode * 81 * Avg Reward is ==> -250.7302668079511\n",
            "Episode * 82 * Avg Reward is ==> -250.89358946770807\n",
            "Episode * 83 * Avg Reward is ==> -251.0607717550134\n",
            "Episode * 84 * Avg Reward is ==> -250.94676250957795\n",
            "Episode * 85 * Avg Reward is ==> -254.06858333509018\n",
            "Episode * 86 * Avg Reward is ==> -257.10370452998404\n",
            "Episode * 87 * Avg Reward is ==> -256.892468556141\n",
            "Episode * 88 * Avg Reward is ==> -253.8351525507832\n",
            "Episode * 89 * Avg Reward is ==> -257.1191702979998\n",
            "Episode * 90 * Avg Reward is ==> -262.6896300815176\n",
            "Episode * 91 * Avg Reward is ==> -266.04080209442884\n",
            "Episode * 92 * Avg Reward is ==> -266.0969641632979\n",
            "Episode * 93 * Avg Reward is ==> -258.0676260814861\n",
            "Episode * 94 * Avg Reward is ==> -258.1831565598495\n",
            "Episode * 95 * Avg Reward is ==> -260.99732841206855\n",
            "Episode * 96 * Avg Reward is ==> -258.0297356122525\n",
            "Episode * 97 * Avg Reward is ==> -246.40190019382234\n",
            "Episode * 98 * Avg Reward is ==> -234.22007246598446\n",
            "Episode * 99 * Avg Reward is ==> -234.3448998565356\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Emx5gTZAUjfS"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}