{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text generation with a miniature GPT.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPlZgjOaznKGNv+/TrU3ELs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Lee-Gunju/AI-paper-code-review-for-personal-project/blob/master/Text_generation_with_a_miniature_GPT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3CJhSflvAZZ8"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
        "import numpy as np\n",
        "import os\n",
        "import re\n",
        "import string\n",
        "import random"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yzqxxzOmAmT7"
      },
      "source": [
        "a = [1,2,3,4,5,6]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vleu3rGgHiR2"
      },
      "source": [
        "b = np.shape(a)[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2xusEbKNHiq1"
      },
      "source": [
        "i = tf.range(b)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "55S2jwfHHtJl"
      },
      "source": [
        "c = [5,4,3,2,1,7]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-vTibKQwH6tl"
      },
      "source": [
        "d = np.shape(c)[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eliv8-UpH8TF",
        "outputId": "9e16b7c7-b3a9-43a8-b7de-d941dacb0f58"
      },
      "source": [
        "j = tf.range(d)[:, None]\n",
        "j"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(6, 1), dtype=int32, numpy=\n",
              "array([[0],\n",
              "       [1],\n",
              "       [2],\n",
              "       [3],\n",
              "       [4],\n",
              "       [5]], dtype=int32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WcirNr6SIBD9",
        "outputId": "7f35642d-7e4c-493a-9959-b10031ead524"
      },
      "source": [
        "j - i"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(6, 6), dtype=int32, numpy=\n",
              "array([[ 0, -1, -2, -3, -4, -5],\n",
              "       [ 1,  0, -1, -2, -3, -4],\n",
              "       [ 2,  1,  0, -1, -2, -3],\n",
              "       [ 3,  2,  1,  0, -1, -2],\n",
              "       [ 4,  3,  2,  1,  0, -1],\n",
              "       [ 5,  4,  3,  2,  1,  0]], dtype=int32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t0HRfjGhIBl1"
      },
      "source": [
        "m = i >=j - abc +bbb"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pvtstg1EIeoU",
        "outputId": "2a62b19c-6c16-424f-dd67-058755263305"
      },
      "source": [
        "m"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(6, 6), dtype=bool, numpy=\n",
              "array([[False, False,  True,  True,  True,  True],\n",
              "       [False, False, False,  True,  True,  True],\n",
              "       [False, False, False, False,  True,  True],\n",
              "       [False, False, False, False, False,  True],\n",
              "       [False, False, False, False, False, False],\n",
              "       [False, False, False, False, False, False]])>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ZwfRZGxIfJW"
      },
      "source": [
        "abc = 3\n",
        "bbb = 5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xoV1is06Il8d",
        "outputId": "51886022-2c47-4796-f572-2bd14869c654"
      },
      "source": [
        "i + abc - bbb"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(6,), dtype=int32, numpy=array([-2, -1,  0,  1,  2,  3], dtype=int32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yhpdey1qIxYa"
      },
      "source": [
        "def causal_attention_mask(batch_size, n_dest, n_src, dtype):\n",
        "  i = tf.range(n_dest)[:, None]\n",
        "  j = tf.range(n_src)\n",
        "  m = i >= j -n_src + n_dest\n",
        "  mask = tf.cast(m, dtype)\n",
        "  mask = tf.reshape(mask, [1, n_dest, n_src])\n",
        "  mult = tf.concat([tf.expand_dims(batch_size, -1), tf.constant([1,1], dtype = tf.int32)], 0)\n",
        "  return tf.tile(mask, mult)\n",
        "\n",
        "\n",
        "class TransformerBlock(layers.Layer):\n",
        "  def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
        "    super(TransformerBlock, self).__init__()\n",
        "    self.att = layers.MultiHeadAttention(num_heads, embed_dim)\n",
        "    self.ffn = keras.Sequential([\n",
        "                                 layers.Dense(ff_dim, activation='relu'),\n",
        "                                 layers.Dense(embed_dim)\n",
        "    ])\n",
        "    self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "    self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "    self.dropout1 = layers.Dropout(rate)\n",
        "    self.dropout2 = layers.Dropout(rate)\n",
        "\n",
        "  def call(self, inputs):\n",
        "    input_shape = tf.shape(inputs)\n",
        "    batch_size = input_shape[0]\n",
        "    seq_len = input_shape[1]\n",
        "    causal_mask = causal_attention_mask(batch_size, seq_len , seq_len , tf.bool)\n",
        "    attention_output = self.att(inputs, inputs, attention_mask = causal_mask)\n",
        "    attention_output = self.dropout1(attention_output)\n",
        "    out1 = self.layernorm1(inputs + attention_output)\n",
        "    ffn_output = self.ffn(out1)\n",
        "    ffn_output = self.dropout2(ffn_output)\n",
        "    return self.layernorm2(out1 + ffn_output)\n",
        "\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hy4uWGn8KpSW"
      },
      "source": [
        "class TokenAndPositionEmbedding(layers.Layer):\n",
        "  def __init__(self, maxlen, vocab_size, embed_dim):\n",
        "    super(TokenAndPositionEmbedding, self).__init__()\n",
        "    self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
        "    self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
        "\n",
        "  def call(self, x):\n",
        "    maxlen = tf.shape(x)[-1]\n",
        "    positions = tf.range(start=0, limit=maxlen, delta=1)\n",
        "    positions = self.pos_emb(positions)\n",
        "    x = self.token_emb(x)\n",
        "    return x + positions\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jm1xmY5_QZQk"
      },
      "source": [
        "vocab_size = 20000  # Only consider the top 20k words\n",
        "maxlen = 80  # Max sequence size\n",
        "embed_dim = 256  # Embedding size for each token\n",
        "num_heads = 2  # Number of attention heads\n",
        "feed_forward_dim = 256  # Hidden layer size in feed forward network inside transformer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_cCYIwWtQbAK"
      },
      "source": [
        "def create_model():\n",
        "  inputs = layers.Input(shape=(maxlen,), dtype = tf.int32)\n",
        "  embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)\n",
        "  x = embedding_layer(inputs)\n",
        "  transformer_block = TransformerBlock(embed_dim, num_heads, feed_forward_dim)\n",
        "  x = transformer_block(x)\n",
        "  outputs = layers.Dense(vocab_size)(x)\n",
        "  model = keras.Model(inputs = inputs, outputs = [outputs, x])\n",
        "  loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "  model.compile('adam', loss=[loss_fn, None])\n",
        "  return model "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EE4d5XqvSJKe",
        "outputId": "f97551f3-e467-45a7-97a2-7e9eb10c2ab7"
      },
      "source": [
        "model = create_model()\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_2 (InputLayer)         [(None, 80)]              0         \n",
            "_________________________________________________________________\n",
            "token_and_position_embedding (None, 80, 256)           5140480   \n",
            "_________________________________________________________________\n",
            "transformer_block_1 (Transfo (None, 80, 256)           658688    \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 80, 20000)         5140000   \n",
            "=================================================================\n",
            "Total params: 10,939,168\n",
            "Trainable params: 10,939,168\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fvqYB3y2SL5T",
        "outputId": "e1e7bf81-94df-4ddd-d3d9-f3bb4a09a7a2"
      },
      "source": [
        "!curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "!tar -xf aclImdb_v1.tar.gz"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 80.2M  100 80.2M    0     0  49.0M      0  0:00:01  0:00:01 --:--:-- 49.0M\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y-GdWLk8SYN7",
        "outputId": "adb447e7-b407-4757-cd6d-8ad5f608dd12"
      },
      "source": [
        "batch_size = 128\n",
        "\n",
        "# The dataset contains each review in a separate text file\n",
        "# The text files are present in four different folders\n",
        "# Create a list all files\n",
        "filenames = []\n",
        "directories = [\n",
        "    \"aclImdb/train/pos\",\n",
        "    \"aclImdb/train/neg\",\n",
        "    \"aclImdb/test/pos\",\n",
        "    \"aclImdb/test/neg\",\n",
        "]\n",
        "\n",
        "for dir in directories:\n",
        "  for f in os.listdir(dir):\n",
        "    filenames.append(os.path.join(dir, f))\n",
        "\n",
        "print(f\"{len(filenames)} files\")\n",
        "\n",
        "# Create a dataset from text files\n",
        "random.shuffle(filenames)\n",
        "text_ds = tf.data.TextLineDataset(filenames)\n",
        "text_ds = text_ds.shuffle(buffer_size = 256)\n",
        "text_ds = text_ds.batch(batch_size)\n",
        "\n",
        "\n",
        "\n",
        "def custom_standardization(input_string):\n",
        "  \"\"\" Remove html line-break tags and handle punctuation \"\"\"\n",
        "  lowercased = tf.strings.lower(input_string)\n",
        "  stripped_html = tf.strings.regex_replace(lowercased, \"<br />\", \" \")\n",
        "  return tf.strings.regex_replace(stripped_html, f\"([{string.punctuation}])\", r\" \\1\")\n",
        "\n",
        "\n",
        "# Create a vectorization layer and adapt it to the text\n",
        "vectorize_layer = TextVectorization(standardize=custom_standardization, max_tokens=vocab_size -1, output_mode='int', output_sequence_length= maxlen+1)\n",
        "vectorize_layer.adapt(text_ds)\n",
        "vocab = vectorize_layer.get_vocabulary() # To get words back from token indices\n",
        "\n",
        "\n",
        "def prepare_lm_inputs_labels(text):\n",
        "  \"\"\"\n",
        "  Shift word sequences by 1 position so that the target for position (i) is\n",
        "  word at position (i+1). The model will use all words up till position (i)\n",
        "  to predict the next word.\n",
        "  \"\"\"\n",
        "  text = tf.expand_dims(text, -1)\n",
        "  tokenized_sentences = vectorize_layer(text)\n",
        "  x = tokenized_sentences[:, :-1]\n",
        "  y = tokenized_sentences[:, 1:]\n",
        "\n",
        "  return x, y \n",
        "\n",
        "\n",
        "text_ds = text_ds.map(prepare_lm_inputs_labels)\n",
        "text_ds = text_ds.prefetch(tf.data.experimental.AUTOTUNE)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "50000 files\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "us95ZHv2V2ma"
      },
      "source": [
        "a, b = next(iter(text_ds))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s_rCoYumV8aO",
        "outputId": "6c9ebc7b-f108-4f6b-dde6-e2a35f3ac110"
      },
      "source": [
        "a[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(80,), dtype=int64, numpy=\n",
              "array([    2,  1286,     7,  1629,    52, 12010,     2,  2642,     7,\n",
              "        2540,  4773,     4,  1294,     1,     4,  2918,  7530,   534,\n",
              "           3,    17,  8699,     7,     2,   563,    15,     3,    95,\n",
              "           2,  1475,    37,  7724,  2055,    77,    43,  1601,     4,\n",
              "          38,   743,    26,    84, 12010,     8,     2,  2642,    38,\n",
              "        9382,  1751,     3,     5,    22,    44,     2,  1086,   211,\n",
              "         161,   315,    44,     5, 15687,   429,     7,  1343,  1143,\n",
              "           1,     4,    21,    10,    16,     5,     1,   594,    60,\n",
              "          10,    16,   624,  1554,     8,   747,  3666,   879])>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5zvQzeNCWB61",
        "outputId": "83a53bf0-886a-4fed-acd2-e14af9cae353"
      },
      "source": [
        "b[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(80,), dtype=int64, numpy=\n",
              "array([ 1286,     7,  1629,    52, 12010,     2,  2642,     7,  2540,\n",
              "        4773,     4,  1294,     1,     4,  2918,  7530,   534,     3,\n",
              "          17,  8699,     7,     2,   563,    15,     3,    95,     2,\n",
              "        1475,    37,  7724,  2055,    77,    43,  1601,     4,    38,\n",
              "         743,    26,    84, 12010,     8,     2,  2642,    38,  9382,\n",
              "        1751,     3,     5,    22,    44,     2,  1086,   211,   161,\n",
              "         315,    44,     5, 15687,   429,     7,  1343,  1143,     1,\n",
              "           4,    21,    10,    16,     5,     1,   594,    60,    10,\n",
              "          16,   624,  1554,     8,   747,  3666,   879,     1])>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qvrM9etuWCQ9"
      },
      "source": [
        "class TextGenerator(keras.callbacks.Callback):\n",
        "  \"\"\"A callback to generate text from a trained model.\n",
        "    1. Feed some starting prompt to the model\n",
        "    2. Predict probabilities for the next token\n",
        "    3. Sample the next token and add it to the next input\n",
        "\n",
        "  Arguments:\n",
        "      max_tokens: Integer, the number of tokens to be generated after prompt.\n",
        "      start_tokens: List of integers, the token indices for the starting prompt.\n",
        "      index_to_word: List of strings, obtained from the TextVectorization layer.\n",
        "      top_k: Integer, sample from the `top_k` token predictions.\n",
        "      print_every: Integer, print after this many epochs.\n",
        "  \"\"\"\n",
        "  def __init__(self, max_tokens, start_tokens, index_to_word, top_k = 10, print_every = 1):\n",
        "    self.max_tokens = max_tokens\n",
        "    self.start_tokens = start_tokens\n",
        "    self.index_to_word = index_to_word \n",
        "    self.print_every = print_every \n",
        "    self.k = top_k \n",
        "\n",
        "  def sample_from(self, logits):\n",
        "    logits, indices = tf.math.top_k(logits, k = self.k, sorted=True)\n",
        "    indices = np.asarray(indices).astype('int32')\n",
        "    preds = keras.activations.softmax(tf.expand_dims(logits, 0))[0]\n",
        "    preds = np.asarray(preds).astype('float32')\n",
        "    return np.random.choice(indices, p = preds)\n",
        "\n",
        "  def detokenize(self, number):\n",
        "    return self.index_to_word[number]\n",
        "\n",
        "  def on_epoch_end(self, epoch, logs = None):\n",
        "    start_tokens = [_ for _ in self.start_tokens]\n",
        "    if (epoch + 1) % self.print_every != 0:\n",
        "      return \n",
        "    num_tokens_generated = 0\n",
        "    tokens_generated = []\n",
        "    while num_tokens_generated <= self.max_tokens:\n",
        "      pad_len = maxlen - len(start_tokens)\n",
        "      sample_index = len(start_tokens) - 1 \n",
        "      if pad_len < 0:\n",
        "        x = start_tokens[:maxlen]\n",
        "        sample_index = maxlen - 1 \n",
        "      elif pad_len > 0:\n",
        "        x = start_tokens + [0] * pad_len\n",
        "      else:\n",
        "        x = start_tokens\n",
        "\n",
        "      x = np.array([x])\n",
        "      y, _ = self.model.predict(x)\n",
        "      sample_token = self.sample_from(y[0][sample_index])\n",
        "      tokens_generated.append(sample_token)\n",
        "      start_tokens.append(sample_token)\n",
        "      num_tokens_generated = len(tokens_generated)\n",
        "\n",
        "    txt = \" \".join([self.detokenize(_) for _ in self.start_tokens + tokens_generated])\n",
        "    print(f\"genered text: \\n{txt}\\n\")\n",
        "\n",
        "\n",
        "# Tokenize starting prompt\n",
        "word_to_index = {}\n",
        "for index, word in enumerate(vocab):\n",
        "    word_to_index[word] = index\n",
        "\n",
        "start_prompt = \"this movie is\"\n",
        "start_tokens = [word_to_index.get(_, 1) for _ in start_prompt.split()]\n",
        "num_tokens_generated = 40\n",
        "text_gen_callback = TextGenerator(num_tokens_generated, start_tokens, vocab)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fn1MDh27iGZP",
        "outputId": "e68e1b40-5200-44f5-8394-7f3c4c8b4773"
      },
      "source": [
        "model = create_model()\n",
        "\n",
        "model.fit(text_ds, verbose=2, epochs=25, callbacks=[text_gen_callback])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "391/391 - 78s - loss: 5.5652 - dense_11_loss: 5.5652\n",
            "genered text: \n",
            "this movie is the worst of the worst movies i 've ever seen . i watched a long time , and i had been disappointed with this movie . there 's not only one of some of the most of the film . i\n",
            "\n",
            "Epoch 2/25\n",
            "391/391 - 77s - loss: 4.7074 - dense_11_loss: 4.7074\n",
            "genered text: \n",
            "this movie is very funny , well acted by the best .                                \n",
            "\n",
            "Epoch 3/25\n",
            "391/391 - 77s - loss: 4.4581 - dense_11_loss: 4.4581\n",
            "genered text: \n",
            "this movie is a very well acted by far one of my favourite films , i think i 've ever heard about this movie . it 's not the fact that a film was made by far more like this on a sunday [UNK]\n",
            "\n",
            "Epoch 4/25\n",
            "391/391 - 77s - loss: 4.2997 - dense_11_loss: 4.2997\n",
            "genered text: \n",
            "this movie is a very entertaining movie . i remember it from beginning to watch it . it was a little slow and i thought that it wasn 't funny how to be funny . i was very disappointed that it was so awful\n",
            "\n",
            "Epoch 5/25\n",
            "391/391 - 76s - loss: 4.1788 - dense_11_loss: 4.1788\n",
            "genered text: \n",
            "this movie is the only good thing . the only thing that is the only reason i have to say is the least [UNK] , and that i 'm sorry for that i have ever watched . my friend was in my opinion ,\n",
            "\n",
            "Epoch 6/25\n",
            "391/391 - 77s - loss: 4.0790 - dense_11_loss: 4.0790\n",
            "genered text: \n",
            "this movie is an excellent movie , not it 's a very good movie . it also contains a number of times and the acting is great - especially the [UNK] of the [UNK] . it has been done by a great actress ,\n",
            "\n",
            "Epoch 7/25\n",
            "391/391 - 77s - loss: 3.9937 - dense_11_loss: 3.9937\n",
            "genered text: \n",
            "this movie is very good ! the first part 3 stars in the series and the [UNK] of the series ' . i 've read a lot of fans of it . but , it just seems like a good show , it 's\n",
            "\n",
            "Epoch 8/25\n",
            "391/391 - 77s - loss: 3.9194 - dense_11_loss: 3.9194\n",
            "genered text: \n",
            "this movie is about two sisters , the best , i 've been in my opinion , but when i saw this last night , i was very interested in seeing it again . i remember the whole movie , i remember seeing it\n",
            "\n",
            "Epoch 9/25\n",
            "391/391 - 77s - loss: 3.8526 - dense_11_loss: 3.8526\n",
            "genered text: \n",
            "this movie is a good film . [UNK] , and i 'm sure the acting was ok , i 've been looking forward to this . it 's a good film to the fact that this could be the only one i 've ever\n",
            "\n",
            "Epoch 10/25\n",
            "391/391 - 77s - loss: 3.7930 - dense_11_loss: 3.7930\n",
            "genered text: \n",
            "this movie is one bad , it 's hard to find that a movie is bad in it , and the acting is awful , the directing and the acting is a bad , but i 'm not sure whether it was the story\n",
            "\n",
            "Epoch 11/25\n",
            "391/391 - 77s - loss: 3.7394 - dense_11_loss: 3.7394\n",
            "genered text: \n",
            "this movie is one of the worst movies i have ever seen . i am a huge fan of the acting , especially by joseph smith , and the story was a good choice , but not a good movie i had ever seen\n",
            "\n",
            "Epoch 12/25\n",
            "391/391 - 77s - loss: 3.6906 - dense_11_loss: 3.6906\n",
            "genered text: \n",
            "this movie is a very good movie and a lot of the people in it . i can 't really say how it made it a movie , and it is really a good movie . the first i didn 't expect it to\n",
            "\n",
            "Epoch 13/25\n",
            "391/391 - 76s - loss: 3.6453 - dense_11_loss: 3.6453\n",
            "genered text: \n",
            "this movie is a good example of how the director got a director who wrote [UNK] his own play . the movie has been cast as good as the director of [UNK] and co -writer [UNK] , director , director , co -writer jeff\n",
            "\n",
            "Epoch 14/25\n",
            "391/391 - 76s - loss: 3.6044 - dense_11_loss: 3.6044\n",
            "genered text: \n",
            "this movie is a bad movie . it was a bad movie . but it is not a good movie but the script is a very bad movie . the direction is bad and i 'm sorry to say . it 's so bad\n",
            "\n",
            "Epoch 15/25\n",
            "391/391 - 77s - loss: 3.5662 - dense_11_loss: 3.5662\n",
            "genered text: \n",
            "this movie is one good movie and it 's just not funny , it 's not just a good one . i liked it a lot . it doesn 't matter the fact that it 's really not as good , it seems to\n",
            "\n",
            "Epoch 16/25\n",
            "391/391 - 77s - loss: 3.5313 - dense_11_loss: 3.5313\n",
            "genered text: \n",
            "this movie is a waste of time than the plot was terrible , the characters , the story was terrible . the acting was terrible , poor direction , the acting and writing was just bad , the directing and lighting was terrible ,\n",
            "\n",
            "Epoch 17/25\n",
            "391/391 - 77s - loss: 3.4991 - dense_11_loss: 3.4991\n",
            "genered text: \n",
            "this movie is a great movie . the storyline is about a man who lives to get involved in the movie of it . it 's not that bad . the story has no plot . . . . it is bad , the\n",
            "\n",
            "Epoch 18/25\n",
            "391/391 - 77s - loss: 3.4688 - dense_11_loss: 3.4688\n",
            "genered text: \n",
            "this movie is about a young man who is a girl and starts out in a house in his apartment where he meets the kid , with a man . he meets a girl named [UNK] . he meets a prostitute named danny .\n",
            "\n",
            "Epoch 19/25\n",
            "391/391 - 77s - loss: 3.4403 - dense_11_loss: 3.4403\n",
            "genered text: \n",
            "this movie is a very good film . in it is a very good one and the worst films i 've ever seen . the script is terrible , the camera work , direction , production quality , editing , lighting , camera work\n",
            "\n",
            "Epoch 20/25\n",
            "391/391 - 77s - loss: 3.4146 - dense_11_loss: 3.4146\n",
            "genered text: \n",
            "this movie is a very bad movie , and i don 't really like this . there are many good reviews about it . and the actors are really bad , the dialogue is so bad , i don 't know how bad it\n",
            "\n",
            "Epoch 21/25\n",
            "391/391 - 77s - loss: 3.3897 - dense_11_loss: 3.3897\n",
            "genered text: \n",
            "this movie is not an example of a film maker named juliette lewis and a perfectly fine writer . he is a great director . [UNK] [UNK] 's character and he 's a good actor . unfortunately , the film has no problems leading\n",
            "\n",
            "Epoch 22/25\n",
            "391/391 - 77s - loss: 3.3675 - dense_11_loss: 3.3675\n",
            "genered text: \n",
            "this movie is an amazing movie for the first time . the plot of this movie was a very poor excuse for a film , and the acting is terrible , the acting was terrible and it was the story . the story was\n",
            "\n",
            "Epoch 23/25\n",
            "391/391 - 77s - loss: 3.3451 - dense_11_loss: 3.3451\n",
            "genered text: \n",
            "this movie is a very funny movie . it is very funny and you have a great cast that weaves and issues , and effortless . the other [UNK] in (over [UNK] to a long time ) and try to become rich and clever\n",
            "\n",
            "Epoch 24/25\n",
            "391/391 - 77s - loss: 3.3253 - dense_11_loss: 3.3253\n",
            "genered text: \n",
            "this movie is a good story , but the characters are very engaging , with great cast of all time . it seems like a great play and the story lines of a little girl , and her love for all ages ! it\n",
            "\n",
            "Epoch 25/25\n",
            "391/391 - 77s - loss: 3.3063 - dense_11_loss: 3.3063\n",
            "genered text: \n",
            "this movie is about a man who lives in an abandoned college [UNK] shop and ends up being stalked by the death of a killer . meanwhile and wesley snipes is a good film . unfortunately , this movie is an outstanding performance by\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f7794f79490>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 107
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CNWFP41ondsP"
      },
      "source": [
        "x = \"i want\"\n",
        "x = [word_to_index.get(_, 1) for _ in x.split()]\n",
        "x = [_ for _ in x]\n",
        "\n",
        "x = x + [0] * 78\n",
        "x = np.array([x])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f0NAV_Z6lzJ_"
      },
      "source": [
        "y, _ = model.predict(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WsrRSiYhnqcP",
        "outputId": "40dcd149-e7ca-455a-cc88-910003e732bc"
      },
      "source": [
        "x"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 12, 191,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 96
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ofpYIlToGRw",
        "outputId": "0e406ceb-63de-4a68-8ec3-f12b0919fb4e"
      },
      "source": [
        "y[0][10]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(20000,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 105
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "69lIgoT0o0uJ",
        "outputId": "f9b39216-5e88-4da8-a202-eb4f678a46e4"
      },
      "source": [
        "vocab[20]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'for'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 106
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OYelgLrTp0OD"
      },
      "source": [
        "from collections import deque\n",
        "\n",
        "row = 3\n",
        "col = 3 \n",
        "d = 0 \n",
        "\n",
        "q = deque([[row, col, d]])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z6cdNlqP_Muu",
        "outputId": "870d94ec-c27a-4c2e-b4f3-67cd29f55594"
      },
      "source": [
        "q.popleft()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[3, 3, 0]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 114
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zg5VLcMw_M8F",
        "outputId": "a854fa3b-d97c-42f7-d26a-c30c59fe6d27"
      },
      "source": [
        "q"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "deque([])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 115
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GEpZgYvy_Qo1"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}